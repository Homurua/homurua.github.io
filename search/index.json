[{"content":"　Lua 是一门小巧精炼的语言，虽然功能不太强大，但是该有的倒也都有，关键是代码量很少，只有不到两万行，非常适合通过阅读代码来学习带虚拟机的脚本语言的实现。本篇以 Lua-5.4.3 的源码为准，参考官方手册。\n库函数 　Lua 的库函数的源码文件分布基本是按照官方手册索引页给出的分类进行划分的，有了索引页的结构做参照，代码分布就会比较清晰。\n 基础库\n基础库就是索引中 base 下面的方法，提供的是最基础的功能，比如加载文件，打印输出这种功能。这部分的代码实现是在 lbaselib.c 中。 协程库\n索引中 coroutine 下面的部分是 Lua 所有的协程接口，协程是 Lua 中非常重要的组成部分，它们的实现在 lcorolib.c 中。 调试库\n索引中 debug 下面的部分是 Lua 所有的调试接口，这些接口有时在查 debug 的时候有很好的效果，不过有一些函数运行速度不太好，应该尽量避免在业务逻辑中使用调试库中的函数。调试库的实现是在 ldblia.c 中。 IO 库\nio 下面的函数是 Lua 提供的处理文件输入输出的接口，有两套风格的接口，一套使用隐式的文件句柄，另一套使用显式的文件句柄。IO 库的实现在 liolib.c 中。 数学库\nmath 库提供了一些基础的数学方法，Lua 提供的数学库不算很完善，倒也基本上够用，这部分的代码在 lmathlib.c 中实现。 os 库\n系统库中封装了提供了一些系统接口的封装，平时开发中用的最多的就是时间和日期相关的接口了，系统库的函数在 loslib.c 中实现。 package 库\npackage 库提供的是 Lua 的模块加载功能，其中 require 是直接导出到了全局变量中，其它的函数都在 package 表中。package 表中的函数在开发中不怎么常用，实现是在 loadlib.c 中。 字符串库\nstring 库中包含了所有针对字符串操作的函数，库中的函数不仅可以直接调用，同时也被设为了字符串元表的 __index 域，所以也可以通过面向对象的方式调用。这部分的实现在 lstrlib.c 中。 table 库\ntable 库中是所有针对 table 的操作，提供的操作其实比较少，不太够用，一般需要自己扩展一下。这部分的实现在 ltablib.c 中。 utf8 库\nutf8 库提供的是对 Unicode 编码数据的支持，除了特殊情况，开发中应该很少用到。这部分的实现在 lutf8lib.c 中。 C API\n由于 Lua 胶水语言的定位，所以提供了很多 C API 供 C/C++ 直接调用。在官方手册中，Index 下所有 C API 下的函数的实现都在 lapi.c 中实现。 辅助库\n辅助库也就是索引中 auxiliary library 下面的方法，文档中说辅助库是为用户提供一些编写 C 方法时常用的便利函数，相较于 C API 提供的函数来说，辅助库中的函数是 \u0026ldquo;higher-level functions\u0026rdquo;，这些函数都是在 lauxlib.c 中实现的。  数据类型 　需要有自定义结构的数据类型，它们的结构定义都在 lobject.h 中，还有大量的与数据类型相关的宏方法也在其中。\n 字符串\n字符串的结构定义在 lobject.h 中，此外还有 lstring.h 和 lstring.c 两个文件，实现了对字符串的创建等一系列底层操作。 函数\nLua 中的函数全部以闭包的形式存在，结构定义在 lobject.h 中，在 lfunc.h 和 lfunc.c 中还有一些针对闭包和函数原型的辅助函数。 表\ntable 除了结构定义在 lobject.h 中以外，其余的逻辑在 ltable.h 和 ltable.c 中，实现了包括 table 创建，修改等一些列操作。  虚拟机 　虚拟机的 opcode 部分在 lopcodes.h 和 lopcodes.c 里面定义和实现，VM 中的操作在 lvm.h 和 lvm.c 里面定义和实现，语法解析在 lparser.h 和 lparser.c 里面定义和实现，词法解析在 llex.h 和 llex.c 中定义和实现。\n每个进程中有一个 global_state 和很多个 lua_State 变量，这两个的定义和实现在 lstate.h 和 lstate.c 中。\n其它  保存 chunk 把脚本编译成字节码保存在二进制文件里的实现在 ldump.c 中。 初始化库 库的加载方法在 linit.c 中实现。 内存分配 内存分配的接口在 lmem.h 和 lmem.c 中。 GC GC 方法的定义和实现在 lgc.h 和 lgc.c 中。 元方法 元方法相关的内容在 ltm.h 和 ltm.c 中。 字符类型 在 lctype.h 和 lctype.c 中实现了一套通过打表快速判断字符类型的方法，比如判断字符是否是空格，是否是数字。这个用在词法解析阶段。 ","date":"2022-03-24T02:18:47+08:00","permalink":"https://wmf.im/p/lua%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0%E4%B8%80%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84/","title":"Lua源码笔记（一）代码结构"},{"content":"　程序员基本上离不开科学上网了，clash 是我感觉体验最好的科学上网软件了，就是相比之下有点吃性能，在路由器上跑的话很多 ARM 架构的路由器都跑不利索，不过 x86 的 CPU 跑到带宽上限一般还是没问题的。\nGUI客户端 　不管 Windows 还是 Linux 下，都有 clash_for_windows_pkg 可以用。Windows 版本支持非常全面，Bug 也很少，不过 Linux 就一般了，Bug 不少，而且修复的比较慢，只能说是凑合可以用了。\nAndroid 平台下可以使用 ClashForAndroid，很稳定，功能也比较齐全。\n命令行客户端 部署服务 　在 clash 下载适合自己使用的 clash 执行程序。新式 CPU 可以下载 linux-amd64-v3 版本，旧式的 CPU 下载 linux-amd64 版。v3 版本可以使用更多的指令集。\n在 maxmind-geoip 下载用于判断主机地址位置的 Country.mmdb 文件。从自己的服务提供商那里下载 config.yaml 文件。执行下面的命令将文件放在对应的位置上。\n1 2 3 4  sudo mkdir /etc/clash sudo cp clash /usr/local/bin sudo cp config.yaml /etc/clash/ sudo cp Country.mmdb /etc/clash/   　创建 systemd 的配置文件 /etc/systemd/system/clash.service :\n1 2 3 4 5 6 7 8 9 10 11  [Unit] Description=Clash daemon, A rule-based proxy in Go. After=network.target [Service] Type=simple Restart=always ExecStart=/usr/local/bin/clash -d /etc/clash [Install] WantedBy=multi-user.target   　执行下面的命令让服务启动：\n1 2 3  sudo systemctl enable clash sudo systemctl start clash sudo systemctl status clash   部署网页控制台 　可以使用网页控制台来控制 clash 服务的配置。有很多可以直接使用的控制台程序，比如由 clash 的作者开发的 clash-dashboard 和广受好评的 yacd 都可以提供比较完善的功能。\n这些网页控制台可以直接访问作者部署好的地址来使用，但是一般速度堪忧，比如 yacd，所以还是自己部署比较方便。因为有了 docker 所以部署变得非常简单，以 yacd 为例，安装好 docker 以后只需要一行命令即可在本地部署完成。\n1  docker run -p 1234:80 -d --rm haishanh/yacd   使用代理 　由于 clash 是自带分流的，不需要在使用的地方进行二次地址过滤，所以使用起来非常简单，直接修改系统代理即可。\nWindows 和 Linux 直接修改系统代理指向目标地址和端口即可，移动设备也可以在 WIFI 设置里填写代理的地址和端口。\n如果不希望修改系统代理，则可以修改浏览器的代理，比如 Firefox 可以直接在 “设置” 的 “网络设置” 里填写代理的设置。Chrome 系的浏览器可能不支持，需要使用 Proxy SwitchyOmega 之类的软件设置代理。\n","date":"2022-03-19T03:43:08+08:00","permalink":"https://wmf.im/p/clash%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/","title":"Clash使用笔记"},{"content":"　Git 是当之无愧的最流行版本控制软件，虽然相比 SVN 有一些复杂，但是也有很多优点。本篇用于记录在使用 Git 中的心得。\n设置代理 　在 github 设置部署代码的时候，会非常非常慢，成功率完全随缘。Git 支持针对指定网站的代理设置，非常方便。\n不管在 Windows 还是 Linux 下，都可以通过在用户目录的根目录下，创建一个 .gitconfig 的文件来配置 Git 的代理。\n1 2 3  [http] [http \u0026#34;https://github.com\u0026#34;] proxy = http://x.x.x.x:xxx   设置用户 　Git 可以在配置中指定当前使用的用户名和电子邮箱地址，设置过以后就不需要每次操作的时候都输入了，设置的位置同样是在用户根目录下的 .gitconfig 中。\n1 2 3  [user] name = xxx email = xxx@xxx.com   Github 配置密钥 　通过在 setting-\u0026gt;SSH and GPG keys 中增加一个 ssh 的公钥即可实现密钥的配置。如果 ssh 本身的密钥配置没有问题的话，在设置过 Git 的用户以后，即可顺利实现对 Github 仓库的无密码访问。\n默认分支 　由于国外轰轰烈烈的 Black Lives Matter 运动，导致了 Github 将自己默认仓库的分支从 master 改为了 main，但是 Git 本身继续使用 master 作为默认分支。\n这就导致了一个问题，如果是在本地通过 git init 创建的分支，又关联了 Github 上创建好的一个远程仓库，就会发现这两个仓库的默认分支不一样，需要把本地的仓库默认分支从 master 改为 main 来适应 Github 的修改。\nTortoiseGit 配置 ssh 工具 　Git 的 GUI 工具里，Win 平台下最好用的就是小乌龟了。不过小乌龟有个问题，它是用的自带的 ssh 工具，这样没办法使用配置好的 ssh 密钥，必须再使用它自带的工具生成一个 ppk 格式的密钥来专门给它自己用，比较麻烦。\n可以通过修改小乌龟的设置，让它使用自带的 ssh 软件，这样可以直接用上命令行配置好的密钥。通过修改 setting-\u0026gt;network-\u0026gt;ssh client 的软件指向已经安装的 git-\u0026gt;usr-\u0026gt;bin-\u0026gt;ssh.exe 即可完成设置。\n","date":"2022-03-19T03:38:47+08:00","permalink":"https://wmf.im/p/git%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/","title":"Git使用笔记"},{"content":"　Lua 脚本的热更新是快速开发和不停服修复 bug 的重要功能，基本上每个基于 Lua 的框架都会提供。但是因为 skynet 开启了众多的 LuaState 的特殊性，所以它的脚本热更新会跟别的框架有很大区别。本文会尝试讲解 skynet 中热更新的操作。\nclearcache 　通过 debug 后台的 clearcache 指令可以清空进程的 CodeCache 缓存。更新本地文件，并且清空 CodeCache 的操作有时会被用来作为 skynet 热更新的方法。但是由于 CodeCache 清空时并不会修改旧的引用，所以旧的服务并不能用到更新，只有新启动的服务可以用到新的内容。\n因为这样的特性，导致了这种方法使用范围比较窄。如果服务本身就是不断创建和销毁的，那就可以直接用，旧的服务会按照以前的逻辑执行完毕，新的服务会直接用新的逻辑。比如像一些副本之类的功能可能就会这样设计，每个副本都对应了一个为它提供支持的服务，副本开始时创建服务，结束时销毁服务。\n还有一类服务也可以用这种方法热更新，就是无状态的服务。虽然旧的服务没办法被更新，但是因为是无状态服务，所以可以直接启动等量的新服务，然后让别的服务都把消息发送到新服务，旧的服务销毁即可。但是 skynet 并不提供相关的支持，要自己实现这类功能。\nsharetable 　sharetable 是 skynet 提供的一种可以在不同 LuaState 中共享只读数据的操作，一般在项目中用于共享策划配置表数据。\nsharetable 中的数据可以进行热更新，针对同一份文件再次执行 loadfile 即可重新加载该文件，然后要在所有需要更新数据的服务里执行 sharetable.update 实现数据更新。\ninject 　inject 是 skynet 提供的可以用于热更新的后台指令，它的特点比较鲜明，用起来比较麻烦，但是理论上可以覆盖所有情况。\ninject 会把该服务的 skynet.dispatch_message 和 skynet.register_protocol 这两个函数所有的 upvalue 作为 env 传给通过 load 加载的注入脚本，_U 是这两个函数的 upvalue，_P 是按协议类型分类的分发函数的 upvalue，想要替换任何函数或者是 upvalue 都需要自己在脚本中处理。\n这里给出一个尽量简单的例子，用来修改 example 里的 simpledb 服务，将 GET 改为每获取一次为原 value 后面追加一个 \u0026ldquo;A\u0026rdquo; 字母。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  local function getupvalues(func, target) if not func then return end local i = 1 while true do local name, value = debug.getupvalue(func, i) if name == target then return value end i = i + 1 end end local command = _P.lua.command local db = getupvalues(command.GET, \u0026#34;db\u0026#34;) command.GET = function (key) db[key] = db[key] .. \u0026#34;A\u0026#34; return db[key] end print \u0026#34;inject success!!\u0026#34;   　首先使用任意 telnet 程序连上 skynet 的 debug 后台，这里要注意一下如果在启动 debug_console 时不指定监听地址的话，debug_console 默认监听的地址是 127.0.0.1，这样的话不能从远端连接该端口。\n输入 list，让 skynet 列出当前进程所有服务的地址。然后使用 inject 命令，指定目标服务的地址和要注入的脚本地址即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  $ nc 127.0.0.1 8000 Welcome to skynet console list :01000004 snlua cmaster :01000005 snlua cslave :01000007 snlua datacenterd :01000008 snlua service_mgr :0100000a snlua protoloader :0100000b snlua console :0100000c snlua debug_console 8000 :0100000d snlua simpledb :0100000e snlua watchdog :0100000f snlua gate \u0026lt;CMD OK\u0026gt; inject :0100000d examples/injectsimpledb.lua inject success!! \u0026lt;CMD OK\u0026gt;   　成功注入以后，启动客户端，向 simpledb 发送 GET 命令。可以看到成功返回，并且每次返回的值都在原来的值后面追加了一个 \u0026ldquo;A\u0026rdquo; 字母。\n1 2 3 4 5 6 7 8  hello result worldA hello result worldAA hello result worldAAA hello result worldAAAA   ","date":"2022-03-14T23:12:36+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E5%8D%81%E5%85%AB%E7%83%AD%E6%9B%B4%E6%96%B0%E6%93%8D%E4%BD%9C/","title":"Skynet笔记（十八）热更新操作"},{"content":"　因为 skynet 中的每一个 snlua 服务都运行在一个独立的 LuaState 中，所以就存在了数据共享的问题。因为 Lua 本身是不支持跨 LuaState 分享数据的，所以 skynet 通过修改 Lua 实现了这一功能，不过有一些局限，只能共享只读数据。本篇会尝试分析这一功能的实现。\n服务 　使用 sharetable 的时候会启动一个进程唯一的服务 sharetable，它的内容在 sharetable_service 这个函数中，提供了对共享数据加载，查询，关闭的功能。\n加载 　ShareTable 可以通过三种方式加载数据，文件，table 和字符串，不管是什么途径，都要传入一个 filename 来作为文件的标识。因为一般 ShareTable 都是用来存策划配置表的，所以用文件加载的时候最多。\nskynet 中把加载过的共享文件的结构叫做 matrix，在 Lua 层有两个 table 用来管理 matrix 的映射，table matrix 保存的是指针地址到文件被引用信息的映射，table files 保存的是文件名字到 matrix 的映射。\ncore.matrix 是加载共享数据的最终入口函数，执行的时候会为每一份数据创建一个新的 LuaState 来加载和保存文件。文件加载用的是 Lua 自带的 luaL_loadfilex_ 进行，只是有一个关键的步骤，会调用 skynet 在 Lua 中增加的 mark_shared 函数，给加载后的 table 打上 G_SHARED 的标志，并且会递归的给 table 中所有需要 gc 的值都打上 G_SHARED 标志。所有共享的数据，在 gc 过程中都不会被标记和回收。\ncore.matrix 返回的是一个 userdata，也就是 matrix 了，结构是 state_ud，可以看到 state_ud 中只有一个 lua_State 指针，指向的就是为加载共享数据创建的那个 LuaState 变量。filename 到 matrix 的映射关系会被保存在 table files 中。\n1 2 3  struct state_ud { lua_State *L; };   　返回值会被设置一些元表项，close 用来关闭 LuaState 停止分享，getptr 用来获取 matrix 的指针，size 用来获取 LuaState 的内存使用量。\n查询 　查询通过的是 sharetable.query 进行，需要指定要查询的 filename，首先在 table files 中查找对应的 matrix，如果没有则说明没有加载过，直接返回即可。\n如果有记录的话，则尝试获取文件的被引用信息，累加文件的被引用计数，并且把引用的源服务地址加入到 table clients 中，然后通过 matrix 的 getptr 拿到其指针，将它返回给源服务，源服务拿到指针以后，会调用 clone 方法将共享的 table 的引用复制到本 LuaState 中。\n更新 　要更新 sharetable 首先要重新 load 一次新文件，在 load 同一个文件时，会检查之前是否有过加载的记录，如果有的话，会清掉之前的记录，然后再加载。\n在执行过新的加载以后，还要在使用这张表的服务里调用 sharetable.update 才行。update 会首先重新 query 一次，拿到新的共享表数据。然后对新数据和旧数据进行一次递归对比，找出所有不一样的数据，然后遍历整个 LuaState 中的数据，替换掉全部的旧值。\n字符串处理 　这样实现会有一个问题，就是字符串的比较问题。在新版的 Lua 中，短字符串还是内化的，也就是两个相同内容的短字符串是指向的同一份数据。根据这个实现又对字符串的比较做了一个优化，就是只比较地址即可。\n但是因为 sharetable 是在别的 LuaState 里面的，所以就会出现本 LuaState 中的某个字符串与 sharetable 中某个相同字面量的字符串不等的问题。\n为了解决这个问题，skynet 对 Lua 进行了一些改动，在 string 的类型 TString 中加了一个 id 来辅助比较。普通字符串默认都是 0，每个被共享化的字符串都会被赋予一个正整数 id。\n加上 id 以后的字符串比较方式变为了，首先比较地址，如果不同的话，比较 id，如果 id 不为 0 且相等，也可以直接判定字符串相等。如果 id 的比较也不相等的话，则需要老老实实调用 memcmp 进行一次完整比较，如果完整比较相等的话，会把较小的 id 改为较大的 id，这样下一次比较的时候就可以通过 id 直接判断出相等了。\n","date":"2022-03-12T05:42:49+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E5%8D%81%E4%B8%83sharetable%E7%9A%84%E5%AE%9E%E7%8E%B0/","title":"Skynet笔记（十七）ShareTable的实现"},{"content":"　skynet 在默认情况下使用了 jemalloc 进行内存分配，这种情况下 skynet 提供了自定义的内存管理函数，本篇会尝试讲解这部分功能的实现。\n接口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  #define skynet_malloc malloc #define skynet_calloc calloc #define skynet_realloc realloc #define skynet_free free #define skynet_memalign memalign #define skynet_aligned_alloc aligned_alloc #define skynet_posix_memalign posix_memalign  void * skynet_malloc(size_t sz); void * skynet_calloc(size_t nmemb,size_t size); void * skynet_realloc(void *ptr, size_t size); void skynet_free(void *ptr); char * skynet_strdup(const char *str); void * skynet_lalloc(void *ptr, size_t osize, size_t nsize); void * skynet_memalign(size_t alignment, size_t size); void * skynet_aligned_alloc(size_t alignment, size_t size); int skynet_posix_memalign(void **memptr, size_t alignment, size_t size);   　这一套接口跟标准库的接口命名完全一样，只是在名字前面加了 skynet_ 的前缀，同时参数和功能也是完全一样的。\n这里有一个小技巧，除了声明了这些函数以外，每个接口还有一个同名的 define 定义，如果加了编译选项 NOUSE_JEMALLOC 的话，则这些函数声明都不会有实现，这时候调用这些函数实际就是使用的同名的 define 定义。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  static ATOM_SIZET _used_memory = 0; static ATOM_SIZET _memory_block = 0; struct mem_data { uint32_t handle; ssize_t allocated; }; struct mem_cookie { uint32_t handle; #ifdef MEMORY_CHECK  uint32_t dogtag; #endif }; #define SLOT_SIZE 0x10000 static struct mem_data mem_stats[SLOT_SIZE];   　上面是内存数据相关的记录，_used_memory 是本进程内通过 skynet_malloc 分配的所有内存数量，_memory_block 是本进程内通过 skynet_malloc 分配的内存块数，跟分配次数相等。\nmem_data 是针对服务的，每个服务都会有一个 mem_data 结构的变量在 mem_stats 里面保存。handle 用来保存服务的 handle，allocated 用来保存该服务已分配的内存字节数。这里有个小问题，就是 SLOT_SIZE 是小于 handle 的增长上限的，而且数组长度也不会增长，所以并不是所有服务都可以统计分配的内存数量。\nmem_cookie 是每次分配内存的时候都会在分配出的内存块后面加上这样一块后缀数据。如果不开 MEMORY_CHECK 的话，也会记录，不过只会有 handle 的记录。\n实现 分配 　分配的接口 skynet_malloc 实现很简单，只有三步操作。首先需要调用 je_malloc 分配一块需求长度 + 后缀长度的内存。然后检查分配结果，如果分配失败则触发 OOM 处理输出错误信息，执行 abort 结束进程。如果分配成功，则为内存块填充后缀，填充完毕以后修改上面提到的各种数据计数。\n如果要使用 skynet_realloc 进行内存的重分配的话，也不会太麻烦，相比 skynet_malloc 只是多了一步要先清除就的后缀的操作。\n释放 　调用 skynet_free 可以释放内存。释放内存的主要额外步骤就是要清除后缀了。清除后缀其实并不是真的为了清除后缀，因为后缀不用清除，直接释放即可。\n清除后缀真正做的事情是为了修改分配的是计数，这里要先通过后缀拿到服务的 handle 才行。拿到以后即可将对应的内存分配数据减少。\n检测 　skynet 提供了一个简单的内存检测功能，可以通过定义 MEMORY_CHECK 开启。提供了 double free 和 out of bound 的检测。\n检测的原理主要是通过检测 mem_cookie 的 dogtag 的值来判断的。在内存分配的时候，如果开启 MEMORY_CHECK 的情况下，dogtag 会被赋值为 MEMORY_ALLOCTAG，其余并无额外的操作。\n在内存释放的时候，如果 dogtag 不是 MEMORY_ALLOCTAG，则说明之前的 dogtag 被覆盖掉了，则可以看作是内存越界写入了。如果是 MEMORY_ALLOCTAG 则将 dogtag 设为 MEMORY_FREETAG 来标记它已经被释放了。同时在一开始也会检查 dogtag 是否是 MEMORY_FREETAG，如果是的话，则可以说明存在二次释放的问题。\n","date":"2022-03-02T19:24:25+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E5%8D%81%E5%85%AD%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","title":"Skynet笔记（十六）内存管理"},{"content":"　CodeCache 应该算是 skynet 的一个特色功能了，是用来优化不同 LuaState 加载同一份代码的速度。因为 skynet 大量启动了 LuaState 来作为 Acotr 使用，所以才在重复加载代码上需要这样的优化。本篇会尝试分析 CodeCache 的实现。\n作用 　首先要了解 CodeCache 解决的是什么问题。简单来说，是在文件第一次加载的时候，将其从磁盘中读取出来，解析完毕以后，把函数原型缓存在一个独立的 LuaState 中。之后其它 LuaState 有加载需求时首先会尝试从缓存中加载，并且函数原型 Proto 可以直接共享缓存中的数据。\n缓存的加入可以达到减少文件 IO，减少代码解析步骤，减少函数原型的内存占用的目的。云风大佬有一篇博客很详细的写了构思过程和要达到的目的。\n实现 结构 　用来从磁盘加载代码并且缓存加载结果的结构是 codecache，它的结构很简单，有一个 lua_State 用来加载和保存结果，还有一个自旋锁，因为加载很可能是并行的，所以修改的时候需要加锁。\n1 2 3 4 5 6  struct codecache { struct spinlock lock; lua_State *L; }; static struct codecache CC;   保存 　Lua 加载文件的最终入口是 luaL_loadfilex，skynet 特供版的 Lua 就是通过修改了这个函数来实现的 CodeCache 功能。\n为了让加载的过程实现线程安全，每次需要加载的时候，skynet 都会创建一个新的 LuaState 进行文件加载。加载完毕以后的 Lua 函数原型 Proto 通过 lua_topointer 拿到地址，并且通过函数 save 把它的指针保存在 CC.L 里面。这个为了加载文件创建出的临时 LuaState 并不会被关闭，在整个进程的生命周期中都存在。\nsave 会首先检查 CC 有没有被初始化，如果没有的话会调用 init 来初始化 CC 中的 L 变量。保存的时候会用文件名作为 key 来保存对应的 proto 变量的指针，存在了 CC.L 的 LUA_REGISTRYINDEX 表中。\n加载 　修改以后的 luaL_loadfilex 会在一开始的时候首先执行 load 尝试从 CC.L 中加载出之前保存过的文件名对应的 proto 变量。\nload 的执行过程比较简单，先锁住 CC 的锁 lock，然后在 LUA_REGISTRYINDEX 表中按文件名尝试读取 proto 结构体。\n如果读取到了，则可以通过 lua_clonefunction 在源 LuaState 中根据已有的 proto 创建一个 closure 出来。在新创建的 LClosure 结构体变量中，共享的部分就是结构体中 proto 的部分，这部分会直接指向在缓存 LuaState 中读取到的指针，从而可以节约掉这部分数据本来需要占用的内存。\n共享模式 　CodeCache 一共有三种共享模式，分别是 OFF/EXIST/ON 模式。共享模式是每个 LuaState 独立的，可以根据本服务的特性来设置。\nOFF 模式会关闭共享，不管什么情况，都自己加载自己使用，既不复用之前的缓存，也不新增缓存。EXIST 模式只会加载已经存在的缓存，如果不存在，则自己加载自己使用，不会新增缓存。ON 模式是默认模式，会先尝试加载之前的缓存，如果不存在缓存的，会从文件中读取，并且新增缓存。\n可以通过 Lua 层的接口 codecache.mode 设置当前 LuaState 的共享模式。C 层接口cache_level 可以用来获取 LuaState 的共享模式。\n清空缓存 　CodeCache 加载过的缓存会在本进程的生命时间里一直存在。不过 skynet 提供了一个可以清空所有缓存的接口，一般用在特殊场景的热更新操作中。\nLua 层的接口 codecache.clear 可以清空当前的所有缓存，同时在 debug 后台里也有一个命令 clearcache 支持调用 codecache.clear 清空缓存。\n清空缓存的实现很简单，因为缓存都在 CC.L 里面放着，所以锁上 CC 的锁以后，直接调用 lua_close 关闭掉 CC.L，之后再给 CC.L 创建一个新的 LuaState 即可完成清空。\n可以看到清空缓存其实清掉的只是函数原型缓存的指针而已，并没有释放掉原来的缓存，只是当下一次加载一个文件的时候，会重新生成缓存而已，之前的服务还是引用的旧的函数原型，既不会更新也不会失效。\n","date":"2022-03-02T19:22:28+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E5%8D%81%E4%BA%94codecache%E7%9A%84%E5%AE%9E%E7%8E%B0/","title":"Skynet笔记（十五）CodeCache的实现"},{"content":"　skynet 中有不少关键功能都是依赖 Lua 协程实现的，skynet 本身也对 Lua 协程有一些管理。本篇会尝试分析 skynet 中对 Lua 协程的使用方法。\n协程池 　一般基于 Lua 的框架都会提供协程的复用，复用可以减少不断创建和销毁协程的开销。协程池是大部分框架的选择，在 skynet 中也是通过协程池实现的协程复用。\n协程池的结构就是一个 table 而已，被当作队列来使用，需要协程时通过 table.remove 从队首取出，用完以后放在队尾。协程池一开始是空的，并不会初始化一些协程备用。\n创建协程 　在 skynet 中创建协程的接口是 co_create(func)，它会首先尝试从协程池中拿取一个协程，如果拿不到，会自己创建一个协程出来。\n因为协程在用完以后要放回协程池中，所以在创建的时候包裹中的函数除了要执行参数 func 以外，还需要一个无限循环。循环中可以接受新的函数，并且再次执行。\nskynet 通过使用 yield 和 resume 配合，实现了给一个创建好的协程传入要执行的函数。在协程的循环中，协程 yield 了两次，第一次 yield 等待目标函数的传入，第二次 yield 是等待第一次传入的函数的参数。co_create 的调用会把目标函数当作参数对协程调用 resume 完成第一步，并且返回协程，等待第二次调用。\n1 2  f = coroutine_yield \u0026#34;SUSPEND\u0026#34; f(coroutine_yield())   协程与消息 　协程在 skynet 中的大部分使用场景都是用来处理消息的。skyent 中有几个 table 用来记录协程和消息的关系，其中 session_id_coroutine 是 session 到 协程的映射，session_coroutine_id 是协程到 session 的映射，session_coroutine_address 是协程到消息的源服务地址的映射。\n在处理接收到的消息时，会感觉消息类型获取其 dispatch 函数，并且为 dispatch 函数创建一个协程。创建以后会在 session_coroutine_id 和 session_coroutine_address 增加数据。当协程被 resume 执行完消息处理函数之后，协程的循环中会删除掉 session_coroutine_id 和 session_coroutine_address 的记录，并且把协程重新投入协程池中去。\n在发送消息时，如果是通过 skynet.send 发消息的话，不需要做记录，如果是通过 skynet.call 发消息的话，需要向 session_id_coroutine 中新增记录，保存 session 到协程的映射。有了这个映射关系，在处理回复的消息的时候，才能根据 session 拿到在等待它返回的协程，然后清除掉 session_id_coroutine 中的记录，并且把返回的消息交给等待的协程处理。\nsession_coroutine_id 还有一个作用，可以判断是不是忘记回复消息的源服务了。如果在处理消息的时候使用 skynet.ret 回复了源服务的话，则会清掉 session_coroutine_id 中的数据。在协程的循环中，当消息的处理函数调用结束以后，会检查 session_coroutine_id 中本协程对应的 session，如果还没有清掉并且不为 0 的话，则说明这是一条需要回复但是并没有进行回复的消息。\nfork 　skynet.fork 可以创建一个协程来执行一个函数，一般用在逻辑中需要调用阻塞接口，但是又不想阻塞在这里的情况，比如需要连续多次的 call 调用的情况就可以为每个 call 调用创建一个协程。另外在通过循环 sleep 实现连续的定时调用时也可以用 fork 来包裹需要不断调用的函数。\n它的实现就是用 co_create 创建了一个协程，包裹住目标函数。然后将其投入到了一个叫做 fork_queue 的队列中去，等待被调用。\nfork_queue 的调用时机其实很快，就在处理完本条消息以后，skynet.dispatch_message 就会为 fork_queue 中所有等待的协程执行 resume 来处理它。\n云风说可以把 skynet.fork 当作是更加高效的 skynet.timeout(0) 来看待，通过实现也可以看出，fork 确实比 timeout 要省略了很多步骤。\n睡眠和唤醒 　skynet.wait 和 skynet.sleep 都可以让出当前协程，都是需要传入一个 token 来标识挂起的协程。区别是 skynet.sleep 需要指定一个唤醒的定时，如果在到期之前还没有被 skynet.wake 唤醒的话，则会由 timer 线程发送消息来唤醒。skynet.yield 也可以让出当前协程，不过它只是对 skynet.sleep(0) 的简单封装，这里不再赘述。\nwait 和 sleep 的实现没太大区别，最主要的区别是 session 的获取。wait 是直接调用 session 分配接口获得的，sleep 则是注册定时事件，通过定时事件返回 session 获得的。它们都通过同一个接口 suspend_sleep 将自己阻塞，并且把 session 和协程的映射关系保存在了 session_id_coroutine 中，把 token 和 session 的映射关系保存在了 sleep_session 中。\nskynet.wakeup 可以唤醒这两种睡眠的协程，包括还未到期的通过 sleep 接口睡眠的协程。但是逻辑并不在这个函数中，它负责的只是把要唤醒的协程的 token 插入到了 wakeup_queue 中。\nwakeup_queue 中等待唤醒的协程会在消息的处理协程挂起以后，通过 dispatch_wakeup 根据 wakeup 的调用顺序被依次唤醒。\n","date":"2022-02-26T18:12:59+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E5%8D%81%E5%9B%9B%E5%8D%8F%E7%A8%8B%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"Skynet笔记（十四）协程的使用"},{"content":"　skynet 的定时器采用了时间轮设计，可以高效的处理定时调用，本篇会尝试理清楚定时器的设计思路和实现方法。\n时间轮算法 　时间轮是实现高效率定时器的常见算法。虽然经常被形容的很高大上，但是其实时间轮非常好理解，因为它就是取自生活中的例子。\n一个简单的例子，如果要在 2022 年 2 月 22 日 22 点 22 分 22 秒做一件事情，那么首先会看年份是不是对应，年份对应了再看月份，月份对应了再看几号，一级一级的向下看，直到秒也可以对应的上，这时候就是要做这件事情了。\n可以看到如果是参考现实计时法的时间轮的话，那实现的其实就是一个类似闹钟的功能。当有需要定时执行的事件时，将它加入到目标时间对应的事件队列中。然后每秒执行一次时间累计，不断累加时间。每次累加时间以后，检查当前时间对应的事件队列，如果不为空，则依次执行。\n与另一种常见的定时器实现算法最小堆比较，时间轮算法最大的好处是其加入一个新定时事件时的复杂度是 O(1) 的，删除一个事件虽然不是 O(1) 但是大概率也比最小堆要快一些。\n用法 　skynet 只提供了创建一个定时事件的接口 skynet.timeout，并没有取消这个事件的接口。通过调用 skynet.timeout(ti, func) 即可启动一个定时调用。\n因为没办法取消定时，但是有时候又确实需要取消，所以 skynet 的文档中给了一个通过修改闭包函数变量把 func 置空的操作。虽然不是真正的从时间轮中移除，但是也算是一份折中的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  function cancelable_timeout(ti, func) local function cb() if func then func() end end local function cancel() func = nil end skynet.timeout(ti, cb) return cancel end local cancel = cancelable_timeout(ti, dosomething) cancel() -- canel dosomething   实现 时间轮结构 　虽然上面举例使用的是现实中钟表的例子，但是实际上时间轮的实现基本不会是类似钟表的设计，因为在代码中，使用准确的日期其实并不便于计算。\nskynet 的实现是一个分层时间轮，一共分了五层。最接近要执行的时间点在 near 里，其余的分层在 t 里，有 4 层。每一层用链表实现，事件执行时从头部取，增加事件时从尾部增加。\n因为存在并发的情况，所以时间轮在修改的时候是需要加锁的。这里还是采用了 skynet 中常见的自旋锁来处理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  struct link_list { struct timer_node head; // 头节点  struct timer_node *tail; // 尾指针 }; struct timer { struct link_list near[TIME_NEAR]; // TIME_NEAR 256，工作队列  struct link_list t[4][TIME_LEVEL]; // TIME_LEVEL 64，分层等待队列，最后一层为溢出队列  struct spinlock lock; // 时间轮的自旋锁  uint32_t time; // 时间轮转动次数，会一直累加到 uint32_t 的上限，然后变成 0 以后继续累加  uint32_t starttime; // 开始时间  uint64_t current; // 从 starttime 开始到现在运行了多久  uint64_t current_point; // 以操作系统启动时间为基准的时间戳，单位为厘秒 };   增加定时事件 　timer_node 结构体用来保存单个定时事件，它是每个定时队列的组成部分。timer_event 用来保存发起定时事件的源服务 handle 和与定时事件绑定的 session 这两部分数据。\n1 2 3 4 5 6 7 8 9  struct timer_event { uint32_t handle; // 定时的服务 handle  int session; // 定时事件的 session }; struct timer_node { struct timer_node *next; // 链表的下个节点  uint32_t expire; // 过期时间，单位为厘秒 };   　当调用 skynet.timeout 的时候，会向时间轮中加入一个定时节点。处理过程中会首先创建一个 timer_event 结构的变量，然后调用 timer_add 创建一个 timer_node 结构变量，并且把 timer_event 变量的内存直接追加到了 timer_node 变量的内存后面。timer_node 的 expire 会设为 timer.time + 定时的时间。\n　增加定时节点时，首先要做的是找到合适的定时队列。skynet 一共为队列分了五层，以到期时间距离当前的时间偏移量计算，分别时如下五档。\n   过期时间 分层 差异位     0 - 255 near 0-8 bit   256 - 16383 0 9-14 bit   16384 - 1048575 1 15-20 bit   1048576 - 67108863 2 21-26 bit   67108864 - 正无穷 3 27-32 bit    　查找适合队列的时候是用 (A | mask) == (B | mask) 的方式计算的，比计算两数相减差值比较上下限要快一点。确定一个时间应该在哪个队列中的方法是，从 0-8 bit 开始比较目标时间和当前时间，如果差异在 0-8 bit 内，则说明距离当前时间不超过 255 秒，则应该放在 near 层中。如果不在当前层，则将掩码左移 6 位继续比较两个值。如果前四层都确定不了的话，则把剩下的全部加到最后一层中。\n找到了合适的队列以后，直接把 timer_node 链接到 link_list 的最后即可。\n更新时间 　timer 线程会每隔 2.5 毫秒更新一次时间。由于 skynet 定时器的最小精度是厘秒，也就是 10 毫秒，所以并不是每次调用的时候都会真正触发时间更新。\n当可以触发时间更新时，会计算当前时间和上次时间的偏移量，然后为每个偏移量调用 timer_update 更新定时器。虽然理论上来说不太可能连续多次触发，但是为了防止特殊情况造成的定时被忽略，所以要对每个偏移量执行 timer_update 操作。\n更新时间的时候，需要检查是否要移动分层队列中节点的层级。这个并不是每次都要检查，而是每当时间轮的计时累加了 256 次以后才会检查一次。相当于是产生了一次进位，进位以后，当前时间在下一层对应的队列中的节点要被移动到 near 里。\n为了便于理解，这里举例说明一下进位操作。比如当前的 T-\u0026gt;time 为 511，当前在 near 队列中的数据为 [256, 511]，此时再次累加，T-\u0026gt;time 变为 512，这时一个 near 单位也就是 256 被处理完毕了，这个时候就要把 [512, 767] 的数据从分层队列中移除，加入到 near 队列中。这部分数据就在第 0 层的 2 节点中，通过将 (time \u0026raquo; 8) \u0026amp; 63 即可计算得出。\n执行事件 　执行事件的时候通过 time \u0026amp; 255 即可拿到当前时间对应的事件链表，如果事件链表不为空，则从头到尾依次对事件调用 dispatch_list 即可。在 dispatch_list 中，timer 线程会拿到一开始加入定时事件时保存的源服务的 handle 和关联的消息 session，向源服务发送一条有对应 session 的消息即可，定时事件的回调函数会在后面被 worker 线程调用。\n执行事件本身并没有什么复杂的地方，但是看源码会发现，skynet 在一次 timer_update 里面执行了两次 timer_execute 方法，第一次是在 timer_shift 之前，第二次是在 timer_shift 之后。第二次调用很正常，没什么疑问，但是第一次的调用可能会有点疑惑的地方。\n之所以要在 timer_shift 之前也调用一次，是为了处理在上一次 timer_update 之后，到这次 timer_update 之间，加入的 timeout 为 0 的事件，如果没有在 timer_shift 之前的 timer_execute 调用，则这部分事件会被漏掉。\n但是，其实如果是通过 skynet 提供的接口创建的定时事件的话，应该是用不到前面的那一次 timer_execute 的，因为 skynet 提供的接口的入口是 skynet_timeout，在这个里面当 time \u0026lt;= 0 时，已经过滤掉了这部分的调用，没有经过定时器，而是直接向源服务发送了触发的消息。那么为什么还要保留这个前置调用呢，这里有一个 issue 说了这个原因，timer 不需要关注 skynet 的用法实现。\n","date":"2022-02-26T18:11:47+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E5%8D%81%E4%B8%89%E8%AE%A1%E6%97%B6%E5%99%A8%E5%AE%9E%E7%8E%B0/","title":"Skynet笔记（十三）计时器实现"},{"content":"　skynet 中支持 harbor 和 cluster 两种方式实现集群，本篇主要讨论 cluster，原因是我个人比较喜欢 cluster 这种方式，XD\n配置 　集群功能需要各个远程主机的配置，这个配置可以通过在 config 文件中指定 cluster 变量为一个配置文件的路径来实现，也可以通过 cluster.reload 来加载。\n不管是通过文件还是 reload 加载配置，配置的格式都是一样的，是一个 Lua 的 table 格式。\n1  {NodeName = \u0026#34;[xxx.xxx.xxx.xxx]:xxxx\u0026#34;}   服务 　实现 cluster 时使用了很多服务，整个逻辑个人感觉有一点绕。这里先简单讲一下使用到的各个服务大概做了什么事情。\nclusterd 是一个通过 skynet.uniqueservice 创建出来的进程唯一服务。它在 cluster 第一次被 require 的时候就会启动。clusterd 提供集群的管理服务，常用的集群接口 cluster.xxx 大部分都是在向 clusterd 发送消息。\n网关服务 gate 集群相关的部分每个进程一个，由 clusterd 创建，主要负责网络相关的集群节点的端口监听，数据发送，数据接收等操作。\nclustersender 由 clusterd 创建，每个对外的连接都有一个，在发起对外连接的时候创建，主要是负责向本服务关联的连接发送数据。\nclusteragent 由 clusterd 创建，每次 accept 的时候会为新的连接创建一个，用于处理该连接的后续数据。\nlisten 　集群中的每个进程都要有一个对外的端口，cluster.open 可以打开端口监听，可以传端口，也可以传配置中有的节点名字。\n实际上处理监听的逻辑在 clusterd 中，当调用 listen 的时候，会启动一个新的网关服务 gate 来处理网络相关操作。启动好 gate 以后，通过 skynet.call 让 gate 服务执行 open 操作，开启端口的监听。\naccept 　clusterd 作为了 gate 的 watchdog 提供服务。当有外部连接时，watchdog 的 socket 命令中的 open 子命令会被执行。\nclusterd 会为每个新连接创建一个 clusteragent 服务来处理本连接的数据。clusteragent 会注册 client 的消息类型，然后对 gate 服务调用 forward 操作，把本服务设为连接的 agent 来接管后续的数据。\nconnect 　集群节点之间对外连接是长连接，连接上以后不会主动断开。在节点第一次对外发送消息的时候会向对方发起连接。\nnode_channel 中保存了全部节点的连接，它的元方法 __index 会调用 open_channel 开启一个对外连接。对外连接是通过另一个服务 clustersender 实现的。\nclustersender 是每个对外连接都会有的服务，它负责通过 socket_channel 发起对外连接，并且负责后续针对这个节点的所有发送消息的操作。\n对外服务名字 　服务可以注册一个对外的名字，一定要注册名字以后才能接收远程的消息。通过 cluster.register 可以注册名字。\ncluster.register 会发消息给 clusterd 来保存名字和地址的对应关系，同时 clusterd 还提供了根据名字查询服务地址的功能。\n发送数据 　与同节点的不同服务间发送消息的接口类似，cluster.send 可以向指定节点的指定服务发送数据。cluster.call 向指定节点的指定服务执行一个调用。\n消息通过在连接时启动的 clustersender 服务进行处理。发送之前需要进行数据打包，都是通过 lua-cluster.c 中的 packrequest 进行打包的。打包好的数据通过之前服务启动时建立的 socket 连接发送出去。\n接收数据 　接收到新数据的时候，gate 会把数据转发给连接的 agent 也就是上面提到的 clusteragent 服务来进行处理。\nclusteragent 使用 lua-cluster.c 中的 lunpackrequest 进行数据解包。解包以后的数据交给消息分发函数进行处理。\n在消息分发函数中，第一次向某个服务发消息之前会去 clusterd 中根据名字查询服务地址，查到以后会保存下来，以后不会再次查询。当名字有变更的时候，clusterd 会发送 namechange 消息给 clusteragent 来变更名字。因为数据已经打包过了，所以这一步的发送不再需要打包数据了，发送消息使用的是 rawsend 和 rawcall 即可。使用对应方法直接把消息转给通过服务名字查询到的服务地址。\n","date":"2022-02-26T18:11:13+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E5%8D%81%E4%BA%8C%E9%9B%86%E7%BE%A4/","title":"Skynet笔记（十二）集群"},{"content":"　netpack 提供的是网络基础数据包的打包和解包功能，是网络输出传输绕不开的一个点。本篇会尝试分析一下 netpack 的实现。\n概述 　netpack 中一共提供了 5 个 Lua 接口。pack 用来打包数据，tostring 用来将一个二进制数据转化为字符串，filter 用来将数据按照类型整理，clear 用来清空待处理数据的队列，pop 用来从待处理队列中弹出一个网络包。\npack 　pack 可以把一个二进制数据或者是一个字符串打包成一个二进制数据包。\n包头占两个字节，是数据的长度，因为只有两个字节，所以单个数据包的长度需要小于 65536 个字节。值得注意的是，数据的字节序固定采用了大端方式，第一个字节是长度的高 8 位，第二个字节是长度的低 8 位。之所以这样写而不是直接保存长度，主要是担心夸字节序主机之间的交流。包头后面紧跟了数据内容。\ntostring 　tostring 负责将一个二进制数据转化为字符串格式。接受一个 userdata 和一个长度参数，返回转化后的字符串。实现上全都是通过 Lua 的 api 实现的，比较简单。\n有一点需要注意的是，netpack.tostring 会释放掉本来的 userdata 的内存。\nfilter 　lfilter 在处理大部分类型的命令时都只是简单的传递了一下参数而已，此处不再赘述。需要重点关注的点是，当处理 SKYNET_SOCKET_TYPE_DATA 类型的网络数据的时候，会执行的对数据进行分包的操作。\n结构 　每个服务有一个网络数据队列，它包含了两部分，一部分是每个套接字的未完成部分 hash，它是一个 uncomplete 结构体的哈希桶，存放着每一个连接的一份未完成数据。另一个部分是已经完成分包的数据包队列，它是一个动态长度的环形数组，可以动态扩容，每次扩容 QUEUESIZE 个数据长度，存放着每一个连接的全部待处理网络包，等待消息分发函数调用 pop 来拿取网络包。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  #define QUEUESIZE 1024 #define HASHSIZE 4096  struct netpack { int id; // 套接字 id  int size; // 长度  void *buffer; // 数据 }; struct uncomplete { struct netpack pack; // 未完成数据对应的网络包结构  struct uncomplete *next; // 哈希桶的下一个  int read; // 已读取的长度  int header; // 当只有 1 字节的数据读取到时，用这个记录一下 }; struct queue { int cap; // queue 的长度  int head; // queue 的头节点索引  int tail; // queue 的尾节点索引  struct uncomplete *hash[HASHSIZE]; // 未完成分包的数据  struct netpack queue[QUEUESIZE]; // 已经完成分包的网络包队列 };   分包 　分包的最终处理函数是 filter_data_，这里就会用上上面提到的结构体，根据不同的情况，分别填充不同的数据，最后返回给 Lua 层的调用。\n函数会首先尝试通过 find_uncomplete 拿到 socket id 对应的 uncomplate 结构体。这里因为保存的数据结构是个哈希桶，所以找到对应哈希节点以后要遍历链表进行查找。\n如果存在对应的 uncomplate 结构体，则说明上次还有未完全读取的数据包。首先判断已读取长度 read 的大小，如果 read 小于 0，说明上次只读取到了本数据包的一个字节数据，这时候会读第二个字节的数据来得到完成的数据包长度。通过 uncompalte 结构体中的数据包总长度 pack.size 和已读取长度 read 可以计算出还需要的数据长度 need 来。\n通过比较还需要的长度和本次处理的数据长度，可以知道本次处理的情况。如果本次的长度还是不够拼出全部的数据，则把本次的数据加到 uncomplate 中即可。如果本次的数据正好可以完成当前的数据包，则直接把数据加到 uncomplate 中并且返回它即可。如果本次的数据会超出当前需要的数据，则首先把本数据包的内容加入到 queue.queue 中去，然后递归调用 push_more 处理剩下的内容，直到数据长度不再够组成一个完整数据包。\n一开始不存在 uncomplate 结构体的处理方法基本一样，只是要额外处理一下本次读取的数据长度为 1 的情况，这种情况不仅要保存 uncompalte 结构体，还要把 read 置为 -1 来作为这种特殊情况的标识。\npop 　用来从已完成组装的队列中拿取最早的那一个数据包。上面说了分包的时候如果数据一次读取的比较多，超出了一条消息的范围，则会依次处理并且把每条消息都 push 到 queue 的 queue 数组中去。要处理消息的时候，就调用 netpack.pop 从队列中拿取一条消息出来。\nclear 　用来清空整个队列的方法，在 gateserver 中被放在了 CMD 的 __gc 元方法中。会遍历清空 queue 中的 hash 和 queue 这两个结构，释放每一个数据的内存。\n","date":"2022-02-26T18:08:38+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E5%8D%81%E4%B8%80netpack%E5%88%86%E6%9E%90/","title":"Skynet笔记（十一）netpack分析"},{"content":"　网络相关的部分是我感觉 skynet 中最复杂的部分了，本篇中会尝试尽量完整的分析到网络相关的大部分功能的实现原理。\n线程模型 　skynet 使用的是多线程 reactor 模型，有一条 socket 线程用来处理 epoll 事件分发，多条 worker 线程来执行事件。\n与其它使用类似模型的框架相比，skynet 最大的区别应该就是还使用了 Actor 的并发模型。所以在 socket 线程处理 epoll 事件的时候，并不是直接把事件交给了 worker 线程来执行，而是把事件和相关的数据一起转化为一条 Actor 之间通用的消息，放到了 Actor 的消息队列中，等待 worker 线程处理 Actor 的消息队列时来处理这个网络事件。\nsocket 管理器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  #define MAX_INFO 128 // MAX_SOCKET will be 2^MAX_SOCKET_P #define MAX_SOCKET_P 16 #define MAX_SOCKET (1\u0026lt;\u0026lt;MAX_SOCKET_P) #define MAX_EVENT 64 #define MAX_UDP_PACKAGE 65535  struct socket_server { volatile uint64_t time; // 当前时间，由 timer 线程更新，socket 线程直接读这个值  int recvctrl_fd; // 接收命令的管道套接字  int sendctrl_fd; // 发送命令的管道套接字  int checkctrl; // 用来标记是否要检查控制台命令的标志  poll_fd event_fd; // 全局的 epoll 套接字  ATOM_INT alloc_id; // 已分配的id，不断累加  int event_n; // 本次调用 epoll 得到的就绪的 fd 个数  int event_index; // 目前处理到的 fd 序号  struct socket_object_interface soi; // userobject 接口  struct event ev[MAX_EVENT]; // 捕获的事件数组  struct socket slot[MAX_SOCKET]; // 全部的 socket 哈希，key 为经过哈希算法计算以后的 id  char buffer[MAX_INFO]; // 临时缓冲区  uint8_t udpbuffer[MAX_UDP_PACKAGE]; // udp 数据缓冲区  fd_set rfds; // 要监听的读描述符集合，用于命令的 select };   　每个 skynet 进程都有一个全局的 socket 管理器，它会在 skynet 进程启动的时候被初始化。从其中的变量大概可以看出一些实现的端倪。比较重要的部分是 epoll 相关的部分和命令相关的部分。\n在其初始化函数 socket_server_create 中，主要的操作是\n 创建了用于 ctrl 命令的管道套接字 创建了 epoll 套接字 调用 pipe 创建管道并且把其中一端加入 epoll 套接字的管理中 初始化 slot 数组中的全部数据，运行中不会扩容，新的 socket 会被复制到指定的位置上  socket 结构分析 结构概览 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  struct socket { uintptr_t opaque; // 本结构关联的服务 handle  struct wb_list high; // 高优先级队列  struct wb_list low; // 低优先级队列  int64_t wb_size; // 等待写入的字节长度  struct socket_stat stat; // 统计数据  ATOM_ULONG sending; // 是否正在发送数据，是一个引用计数，会累加  int fd; // 套接字  int id; // 分配的 id  ATOM_INT type; // 当前连接状态  uint8_t protocol; // 连接协议  bool reading; // fd 的 read 监听标记  bool writing; // fd 的 write 监听标记  bool closing; // fd 的 close 标记  ATOM_INT udpconnecting; // udp 正在连接  int64_t warn_size; // 报警阈值  union { int size; uint8_t udp_address[UDP_ADDRESS_SIZE]; } p; // 如果是 tcp 连接则用 size 表示每次读取的字节数，如果是 udp 则用 udp_address 表示地址  struct spinlock dw_lock; // 自旋锁  int dw_offset; // 已经写入的大小  const void *dw_buffer; // 数据指针  size_t dw_size; // 总大小 };   　socket 连接是个很复杂的结构，因为内存对齐的缘故，变量的先后顺序排列并不是按照关联性分布的，后面会按照功能相关性分开讨论其中的变量。\nsocket 的基础数据  int fd\nfd 是系统分配的 socket 套接字，是网络连接的基础。不同的网络操作会使用不同的参数来使用系统调用 socket 创建自己的网络套接字。 int id\nid 是由 skynet 分配的用来标识 socket 结构体变量的唯一标识符。之所以在有了 fd 以后还需要 id 是因为内核可能会重用 fd，并不能用 fd 来做唯一标识。 ATOM_INT type\n虽然名字叫类型，但是其实是 socket 当前的状态，这个变量会随着 socket 的状态改变而被修改，目前一共有十种状态。 1 2 3 4 5 6 7 8 9 10  #define SOCKET_TYPE_INVALID 0 // 初始状态，表示未使用 #define SOCKET_TYPE_RESERVE 1 // 保留状态 #define SOCKET_TYPE_PLISTEN 2 // 监听前状态 #define SOCKET_TYPE_LISTEN 3 // 监听中状态 #define SOCKET_TYPE_CONNECTING 4 // 连接中状态 #define SOCKET_TYPE_CONNECTED 5 // 已连接状态 #define SOCKET_TYPE_HALFCLOSE_READ 6 // 半关闭剩下读 #define SOCKET_TYPE_HALFCLOSE_WRITE 7 // 半关闭剩下写 #define SOCKET_TYPE_PACCEPT 8 // 已 accept 但是还未加入 epoll #define SOCKET_TYPE_BIND 9 // 绑定状态    uint8_t protocol\nprotocol 表示的是 socket 关联的协议类型，在处理网络事件时，需要知道它的协议类型来执行不同的消息分发函数。目前一共有 4 中类型，其中 PROTOCOL_UNKNOWN 是一开始的默认类型。 1 2 3 4  #define PROTOCOL_TCP 0 #define PROTOCOL_UDP 1 #define PROTOCOL_UDPv6 2 #define PROTOCOL_UNKNOWN 255    uintptr_t opaque\nopaque 是创建本 socket 的源服务的 handle，因为前文中提到的 skynet 并不会直接处理网络事件，而是会把每个网络事件都转换为消息发送给源服务，所以 socket 要记录下来源服务的 handle 来接受网络事件消息。 union {int size; uint8_t udp_address[UDP_ADDRESS_SIZE];} p\n这里为了省内存使用了一个 union 做了两个作用，当 socket 是 tcp 协议的时候，使用 size 表示本连接每次从 fd 中读取的字节数，这个 size 会被初始化 64，并且会根据每次从 fd 中读取数据的情况增加或者减少，这个参数的存在是为了优化读取的效率。如果是 udp 协议的话，使用 udp_address 用来保存对端的地址。 bool reading, writing\n用来表示套接字当前在 epoll 中的状态，主要被用在修改 epoll 的监听事件时，比如当之前已经设置过 EPOLLIN，又要加上 EPOLLOUT 的时候，如果没有 reading 记录的话，就比较麻烦。 bool closing\n标记 socket 的已关闭状态。 struct socket_stat stat\nstat 用来记录当前 socket 的读写数据状态。 1 2 3 4 5 6  struct socket_stat { uint64_t rtime; // 最近一次读取时间  uint64_t wtime; // 最近一次写入时间  uint64_t read; // 已读取的总数据长度  uint64_t write; // 已写入的总数据长度 };    ATOM_INT udpconnecting\n正在连接中的 udp 数量，发起连接时累加，连接成功后递减。  写入队列相关  struct wb_list high, low\nwb_list 即 write buffer list，发送队列是一个单向链表。每个 socket 连接有两个发送队列，一个高优先级队列和一个低优先级队列，高优先级队列中的数据会优先发送出去。发送数据时，需要指定数据的优先级。 1 2 3 4 5 6 7 8 9 10 11 12  struct write_buffer { struct write_buffer *next; const void *buffer; char *ptr; size_t sz; bool userobject; uint8_t udp_address[UDP_ADDRESS_SIZE]; }; struct wb_list { struct write_buffer *head; struct write_buffer *tail; };    int64_t wb_size\nwb_size 即 write buffer size，是 socket 连接中全部等待写入的数据长度，包含两个写入队列的待写入长度总和。 int64_t warn_size\nsocket 等待发送数据的警告阈值，如果超过了这个值，会输出一条警告信息。阈值每次触发以后会变为原来的两倍。  direct write 相关  struct spinlock dw_lock\n直接写入部分的锁，需要修改 dw 相关的数据时要先加锁。 int dw_offset\n通过 dw 已经发出去数据偏移量，在 socket 线程处理发送的时候需要根据这个值计算出还未发送出去的数据。 const void *dw_buffer\n如果 dw 阶段发送出去的数据不完整的话，调用 clone_buffer 使 dw_buffer 指向待发送数据，等待 socket 线程后续再次发送这个数据。 size_t dw_size\n本次 dw 数据的总大小，也就是 dw_buffer 指向数据的大小。  分配 ID 　skynet 在分配 socket 的 ID 时，也会碰到空洞位置的问题，因为关闭的 socket 连接会被再次设为可用状态，这就导致了分配 ID 不能简单累加，而是从 alloc_id 开始，遍历完整个 slot 数组，计算哈希的时候直接针对 MAX_SOCKET 取模即可。这里有个小问题，alloc_id 是个原子变量，可能会让这个分配 ID 的函数的效率雪上加霜，最坏情况下在分配一次 ID 的过程中，alloc_id 要被 atomic_fetch_add 累加几万次。\nepoll 相关 　skynet 在创建 epoll 套接字的时候，为 epoll_create 传入了参数 1024，这里仅为兼容旧版本的内核，新版内核已经不再需要这个用来提示的参数。\n关于 epoll 的触发模式，skynet 使用的是 epoll 默认的水平触发模式。\nself-pipe 　从 socket_server 的初始化函数中可以看到，recvctrl_fd 和 sendctrl_fd 分别是管道 pipe 套接字的两端，并且 recvctrl_fd 被加入到了 epoll 监听中。\n这用到了一个叫做 self-pipe 的技术，《Linux 系统编程手册》65.5.2 有介绍这个技术。它在 skynet 中的应用主要是为了解决单网络线程阻塞在 epoll 的 wait 上这种情况。如果不使用这个技术，则当需要做一些修改，比如修改 epoll 的监听套接字的时候，这时候线程阻塞在了 wait 上，要处理修改只能让 wait 等待一个指定时间以后返回，处理修改，然后再次进入 wait 中。这带来一个问题就是等待时间的选取是比较麻烦的，太久了则修改要等待很久才能生效，太短了则会让 wait 不断返回进入，比较浪费 CPU 资源。\n通过 self-pipe 技术就可以解决这个问题，创建一对管道套接字用于网络命令处理，然后把接收端加入 epoll 管理中，所有对网络操作的修改都发送给管道的发送端。这样就能达到一旦有命令过来，epoll 的 wait 立刻会被唤醒，不需要指定返回时间了。\nsocket 线程的主循环 skynet_socket_poll 　socket 线程会无限循环调用函数 skynet_socket_poll，在这个函数中，通过调用 socket_server_poll 来获取网络事件和处理的数据 result，针对不同的网络事件，通过调用 forward_message 发送不同格式的消息给与触发事件的套接字绑定的服务。\n函数会创建一个 socket_message 的结构体变量 result，把 result 传给了 socket_server_poll，其处理完网络事件以后，会把结果写入到 result 中，然后 result 会再次给到 forward_message，它会将 result 根据需要处理成一条 skynet 消息，然后 push 到源服务的消息队列中。\n1 2 3 4 5 6  struct socket_message { int id; // socket id  uintptr_t opaque; // 目标服务的 handle 句柄  int ud;\t// accept 事件中 ud 是新连接的 id，别的时候是 data 的长度  char * data; //数据指针 };   socket_server_poll 　本函数是 skynet 网络部分处理网络事件的最终循环。该函数大部分情况下都阻塞在 sp_wait 上，等待 socket 事件触发或者是网络命令过来。\n从 epoll_wait 中唤醒以后，开始了一轮处理。在每轮的处理中，首先会把检查控制命令的变量 checkctrl 置为 1，然后开始依次处理本次触发的网络事件。\n在网络事件的处理中，如果碰到的是命令事件，则直接 continue 回到循环的最上面去处理网络命令。否则则会去读取 socket 的状态 type，根据不同的 type 执行不同的操作，向 result 中填充相关的数据。\n每次从 epoll 中取到的就绪套接字个数放在 event_n 中，用一个变量 event_index 保存当前处理到了第几个套接字。当 event_index == event_n 的时候，则说明本轮的处理已经结束了，线程会再次调用 epoll_wait 获取下一轮要处理的就绪套接字。\n本函数会填充 result 参数，并且返回一个处理结果类型给 skynet_socket_poll，返回的结果一共包含了八种类型。\n1 2 3 4 5 6 7 8  #define SOCKET_DATA 0 // 读取数据 #define SOCKET_CLOSE 1 // socket 关闭 #define SOCKET_OPEN 2 // socket 连接成功 #define SOCKET_ACCEPT 3 // accept 成功 #define SOCKET_ERR 4 // socket 错误 #define SOCKET_EXIT 5 // 退出 socket 线程 #define SOCKET_UDP 6 // 接收 udp 数据 #define SOCKET_WARNING 7 // socket 警告   网络指令处理 　每当 epoll_wait 返回时，新的一轮网络事件的处理就会开始，指令的检查标记 checkctrl 也会被设为 1，来开启指令检查。每轮只会处理一次指令，会一直连续处理指令直到全部处理完。\n通过 has_cmd 来检查管道中有没有还没处理的命令数据。这一步是使用系统调用 select 来实现的，使用 select 来检查 recvctrl_fd 是否可读。虽说 recvctrl_fd 被加到了 epoll 中，但是 epoll_wait 唤醒以后，如果是指令数据唤醒的，不会原地处理，而是等下一个循环处理，所以这里还要再检查一次是否可读，并没有以来 epoll 做标记之类的，可能是为了处理简单一些。\n如果 recvctrl_fd 中有指令等待读取，则调用 ctrl_cmd 读取并执行指令。其中用了两次 block_readpipe 来读取管道中的数据，第一次读取了数据头，包括了命令类型和数据长度，第二次用第一次读取到的数据长度读取了数据。\nblock_readpipe 是用来从管道中读取数据的函数，可以看到一个有意思的地方，read 的返回值只处理了小于 0 的情况，并没有处理 n \u0026gt; 0 \u0026amp;\u0026amp; n \u0026lt; size 的情况。这是因为对管道套接字执行 read 操作是一个原子操作，不会被别的情况比如信号之类的打断，所以只有两种可能，错误和全部读取完成。关于 pipe 套接字的读写后面可以考虑开一篇文章写一下。\n读取到了命令以后，根据命令类型，把数据交给不同的处理函数来处理即可。\n网络请求 概述 　skynet 中涉及网络的操作，除了 direct write 以外，都是通过网络请求来实现的。worker 线程根据自己想要做的操作的类型，创建不同结构的请求数据，通过 send_request 发送到命令管道的发送端 sendctrl_fd 中去，等待主循环中接受处理请求。\n请求包的结构 　因为 C 中没有面向对象的功能，所以通过 union 来实现了请求包的结构，每一种类型的请求对应了一类的请求结构体，所有的请求都会转化成一个 request_package 结构体，发送到管道中来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  struct request_package { uint8_t header[8]; // 6 bytes dummy, 第 7 个字节表示类型，第 8 个字节表示长度  union { char buffer[256]; struct request_open open; struct request_send send; struct request_send_udp send_udp; struct request_close close; struct request_listen listen; struct request_bind bind; struct request_resumepause resumepause; struct request_setopt setopt; struct request_udp udp; struct request_setudp set_udp; } u; uint8_t dummy[256]; // 预留了 256 个字节 };   处理请求包 　socket 线程的主循环中，会不断读取管道中的数据，每条命令要执行两次 read 读取，第一次要读到 header 中的内容，然后根据 header[1] 的长度数据，读取剩余的数据。然后根据 hander[0] 中的类型数据，进行不同的处理。目前一共有 13 种网络操作类型。\n S Start socket\nB Bind socket\nL Listen socket\nK Close socket\nO Connect to (Open)\nX Exit\nD Send package (high)\nP Send package (low)\nA Send UDP package\nT Set opt\nU Create UDP socket\nC set udp address\nQ query info\n 网络接口分析 概述 　因为所有的网络操作都是通过发送命令来进行的，所以 skynet 的网络接口都是非阻塞的，不同的接口会完成基本的操作，然后把需要的参数打包成一条上面提到过的 request_package 结构数据发送给命令的接收端。\nconnect 　通过调用 socketdriver.connect 可以发起一个对外连接。lconnect 中会从目标地址的字符串中分离出 host 和 port 这两个参数，再获取一个 socket id，然后打包成一个网络请求，给命令接收套接字发送了一个 \u0026lsquo;O\u0026rsquo; 类型的命令。\n在 socekt 线程中的部分里，对应命令的处理方法是 open_socket，首先它会通过系统调用 getaddrinfo 拿到目标主机的全部地址，然后依此对每个地址尝试去执行系统调用 socket 创建一个套接字并且把它设为 keep_alive 和 non_blocking 的，然后执行 connect 系统调用。\n如果套接字创建成功了，则创建 socket 结构体。检查 connect 的调用返回，如果成功了，则直接把 socket 的状态设为 SOCKET_TYPE_CONNECTED，返回 SOCKET_OPEN，连接成功。\n如果连接失败了，且 errno 是 EINPROGRESS，也就是说无法马上连接的状态，则把 socket 的状态设为 SOCKET_TYPE_CONNECTING，并且将套接字加入到 epoll 中，打开写入事件。当 epoll 触发了套接字的 write 事件时，则说明之前的连接已经建立成功了。将 socket 的状态设为 SOCKET_TYPE_CONNECTED，返回 SOCKET_OPEN 表示连接成功。\nSOCKET_OPEN 返回以后，会创建一个 SKYNET_SOCKET_TYPE_CONNECT 类型的消息给发起连接的源服务，但是源服务并不需要处理连接成功的消息，所以 netpack 在进行解包的时候，直接忽略了 SKYNET_SOCKET_TYPE_CONNECT 类型的消息。\nlisten 　从 gateserver 的 open 命令来分析一下 skynet 套接字的监听步骤。socketdriver.listen 是 skynet 的监听接口，可以开启一个监听套接字，函数返回值是监听套接字的唯一标识符，也就是上面提到的 id 这个变量。监听的接口执行可以分为两个阶段，第一个是在 worker 线程中执行的部分，第二个是在 socket 线程中执行的部分。\n监听操作在第一阶段的执行中，主要逻辑在 socket_server_listen 函数中，其中依次调用了 socket/bind/listen 等系统调用，完成了网络套接字的创建，绑定和监听，但是并未将套接字加入到 epoll 的管理中去。然后通过 reserve_id 分配了一个 socket 结构的唯一 id，但是这里也不会创建 socket 结构体。创建一个 request_package 的结构体，将上面拿到的参数填入其 request_listen 结构体中，然后将其发送给命令的接收套接字 sendctrl_fd 即可。\n监听操作在第二阶段的执行中，主要逻辑在 listen_socket 函数中。这里逻辑就比较简单了，做的事情就是上段中点明了剩下的两个部分，把监听套接字加入到 epoll 管理中，并且创建了 socket 结构变量。socket 中的 type 会被修改为 SOCKET_TYPE_PLISTEN，只是 pre listen 还没有完全完成监听。还有一点需要注意的是，因为上一阶段已经拿到了 socket 的唯一 id，所以这里是直接修改了之前那个 id 在数组 socket_server.slot 中对应的 socket 结构体。\n到这里 socketdriver.listen 的工作就全部结束了，但是明显可以发现这个时候的 listen 还未完全完成。因为此时还未设置 accept 以后的回调，而且 socket 中的状态也还是未完成的监听状态。我们现在有一个 socket id 是监听套接字的句柄，需要做的是使用这个 id 调用 socketdriver.start 来执行后续的步骤。\nsocketdriver.start 中主要做的事情是给网络命令接收套接字发送了一个 \u0026lsquo;R\u0026rsquo; 请求，这个请求会把 id 对应的套接字加入到 epoll 管理中并且打开读取监听。不过由于 listen 的前期创建 socket 的时候已经把套接字加入到了 epoll 并且默认是打开读取的，所以这里并不会做什么操作。这里最主要修改的是上面提到的 socket 的状态，会把状态改为 SOCKET_TYPE_LISTEN，以让循环中可以正确处理监听，然后还把监听 socket 的源服务改为了调用 start 的服务，也就是说可以实现在某个服务中 listen 创建一个 socket id，然后把它传给另一个服务，由另一个服务调用 start 来接收后续的消息。\naccept 　accept 由 socket 线程的 epoll 循环触发，如果触发网络事件的套接字是 SOCKET_TYPE_LISTEN 状态的话，则说明触发了 accept 事件。\nreport_accept 是处理 accept 事件的函数，首先通过系统调用 accept 拿到网络套接字，然后拿到 socket 结构要用的唯一 id，client 的 fd 会被设置 keep_live 和 no_blocking，创建 socket 结构，把 socket 的状态设为 SOCKET_TYPE_PACCEPT 类型。\n上述处理结束以后，返回到 skynet_socket_poll 的类型为 SOCKET_ACCEPT，skynet_socket_poll 会调用 forward_message 给 socket 的源服务发消息，socket 的源服务现在是上一步中执行 socketdriver.start 的服务。发送的消息中把消息结构体中的 type 设为了 SKYNET_SOCKET_TYPE_ACCEPT 来标识消息的类型。\n消息会交给 gate 服务处理，它注册了 \u0026ldquo;socket\u0026rdquo; 类型的协议，负责解包的是 netpack.filter 函数。lfilter 在处理 SKYNET_SOCKET_TYPE_ACCEPT 时很简单，只是整理了一下参数而已，压入了操作类型对应的字符串 \u0026ldquo;open\u0026rdquo; 供 dispatch 方法调用。\n在 dispatch 中，\u0026ldquo;open\u0026rdquo; 操作对应了 MSG.open 方法，其中跟网络层有关的就是调用了 handle.connect，在 handle.connect 中，历经千难万苦，如果是用 examples 中提供的示例的话，就是经过了 watchdog 和 agent 的操作，最终调用了 socketdriver.start 来激活客户端的 socket 结构。与 listen 的步骤类似，在 start 中会把 client 的 socket 类型从 SOCKET_TYPE_PACCEPT 改为 SOCKET_TYPE_CONNECTED，socket 关联的服务 handle 会改为调用 socketdriver.start 的服务。\nwrite 　发送数据有两种方法，常规的是通过 epoll 的写事件触发。为了减少一些开销，skynet 还做了一个叫做 direct write 的操作，也就是直接写入，不通过 socket 线程，而是在 worker 线程直接尝试把数据发出去。\n首先来看 direct write 的部分，发送数据的接口为 socket_server_send，这个函数首先会检查当前 socket 能不能直接发送，如果在当前 socket 的高或低优先级队列中有数据等待发送的，则不能直接发送。如果可以直接发送的话，则会直接在 worker 线程调用 write 往套接字中写入数据。\n直接发送会有三种结果。如果发送失败了，则忽略这个错误，当作写入了 0 长度的数据。如果完整发送了全部的数据，则可以直接返回，不需要再走后面的步骤了，本次发送已经完成了。如果只发送了部分数据，包括前面发送错误产生的结果，都会设置 dw_buffer/dw_size/dw_offset 这三个变量，等待后续 socket 线程再次进行数据发送，并且发送了一条网络消息 \u0026lsquo;W\u0026rsquo; 来打开本套接字的写事件监听。\n触发 epoll 的写事件以后，如果有之前 direct write 阶段没发完的数据，会首先把剩余的数据加入到高优先级队列的首部。发送阶段，会优先先发高优先级队列的数据，然后再发低优先级队列中的数据，如果低优先级队列中的数据没有全部发完，则会借助一个叫做 raise_uncomplete 的操作，把剩余的数据提到高优先级队列中。\n在两种情况下会触发关闭套接字写事件的监听，首先是如果 write 如果返回了一个错误，且不是 EINTR（信号打断） 或者 EAGAIN（非阻塞套接字缓冲区写满） 错误的情况下会关闭写事件。还有就是当套接字的高优先级队列和低优先级队列都发送完毕的时候也会关闭写事件。\nread 　当 epoll 中的套接字变为可读以后，如果是 TCP 连接，则使用 forward_message_tcp 读取套接字中的数据。\n每次读取的长度是 socket.p.size，初始为 64 字节，如果在本次读取的时候发现套接字中可读长度大于 size 的话，则将 size 扩大为之前的 2 倍，如果发现套接字中的数据比 size 的 1/2 还少的时候，则把 size 变为原来的 1/2 长度。另外，如果本次没有读取完套接字中的数据，则会减少 event_index，使下一次循环依然处理本事件。\n读取到数据以后，转化成消息，给 socket 的源服务发送一条 SKYNET_SOCKET_TYPE_DATA 类型的消息。这个消息会触发网络分包，通过 netpack.filter 进行分包，分包值得单开一篇文章细说一下，此处先一笔带过。\n","date":"2022-02-18T14:53:02+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E5%8D%81%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90/","title":"Skynet笔记（十）网络分析"},{"content":"　snlua 服务绝对是 skynet 中最重要的服务了，是一切 lua 服务的原型，也是 99% 情况下业务中使用的服务。本篇会讲解一下 snlua 服务的实现。\n结构 1 2 3 4 5 6 7 8 9  struct snlua { lua_State *L; // 状态机  struct skynet_context *ctx; // 关联的 skynet context  size_t mem; // 已经使用的内存  size_t mem_report; // 触发内存警告的阈值  size_t mem_limit; // 内存的上限设置  lua_State *activeL; // 目前正在运行的状态机  ATOM_INT trap; // 打断状态 };   　snlua 的核心数据就是一个 lua 状态机，围绕状态机，skynet 做了一些扩展工作，主要是为其自定义了内存分配函数，以及为其提供了一个接受外部信号打断其运行状态的机制。\n创建 1 2 3 4 5 6 7 8 9 10 11  #define MEMORY_WARNING_REPORT (1024 * 1024 * 32) struct snlua *snlua_create(void) { struct snlua *l = skynet_malloc(sizeof(*l)); memset(l, 0, sizeof(*l)); l-\u0026gt;mem_report = MEMORY_WARNING_REPORT; l-\u0026gt;mem_limit = 0; l-\u0026gt;L = lua_newstate(lalloc, l); l-\u0026gt;activeL = NULL; ATOM_INIT(\u0026amp;l-\u0026gt;trap, 0); return l; }   　module 的创建主要就是对其私有数据的创建，snlua 中也不例外，过程就是创建一个 snlua 的结构体变量，然后初始化它其中的变量。\n可以看到单个 snlua 服务的内存警告的初始阈值是 32M 大小。这个并不是硬上限，只是一个警告阈值，超过了会触发一次警告，然后这个值会被修改为原来的两倍，直到下一次触发警告。内存分配使用了自定义的 lalloc 来做，还有一点需要注意，在创建状态机的时候，把 snlua 的指针也传进去了，这个在协程部分会用到。\n初始化 　初始化的时候用了一个有趣的操作，并没有直接在 snlua_init 里面做初始化，而是给自己发了一条消息，然后在消息的回调函数 launch_cb 里面再通过调用了 init_cb 做了初始化。这里暂时不太懂为什么要这样设计，理论上来说这一步直接完成 Lua 状态机的初始化也不是不行啊。\nLua 状态机的环境都是在 init_cb 中进行初始化的。大概进行了以下几点操作\n 加载标准库 加载 skynet.profile 库 使用 skynet.profile 库中的 resume 和 wrap 替换掉标准库协程中的同名函数 加载 skynet.codecache 库 从环境变量中拿取配置的那些文件加载的地址数据，加载进来 加载 loader.lua 通过 loader 加载指定的服务文件 检查是否设置了 lua 服务的内存上限，如果有则要设置  内存分配 　自定了 Lua 的内存分配函数为 lalloc，主要的操作就是处理 snlua 中跟内存有关的三个变量 mem / mem_limit / mem_report\n 修改当前占用的内存 mem 检查内存分配上限 mem_limit，如果超了上限不能分配 检查内存使用的警告阈值，如果超过了阈值，要给出警告，并且把警告阈值翻倍  　因为自定义了内存分配函数，所以同时也支持了输出当前使用的内存，可以通过 debug 后台发信号查询指定服务的内存占用。\n信号打断 　snlua 在自己的 signal 接口中，支持了打断正在运行的脚本的功能。这个功能主要是用来打断可能陷入死循环的服务的。\n打断的实现需要用到 snlua 中的 activeL 和 trap 这两个变量。trap 是个原子变量，用来标识当前是否正在打断过程中，之所以需要是原子变量是因为 signal 可能并行触发。activeL 变量用来标识当前正在执行的 Lua 状态机。\n要理解 activeL 变量的作用需要了解一些 Lua 的协程实现才可以。Lua 的每个协程对应的结构也是 lua_State，和 Lua 的主虚拟机是相同的，在协程内部的调用中会使用协程自己的 lua_State 执行各种操作。这就有了一个问题，要打断当前运行的脚本，需要给 lua_State 设置钩子函数，所以需要一个变量来记录当前是哪个 lua_State 正在执行，又因为切换 lua_State 的过程是发生在对协程的 resume 调用中的，所以需要自己实现一个 resume 来替换掉标准库中的 coroutine.resume，也就是 luaB_coresume 了。在发生 lua_State 切换的时候，snlua 都会修改 activeL 变量指向新的活动状态机。\n打断脚本的操作本身很简单，通过 Lua 的钩子函数机制调用 signal_hook，signal_hook 中使用 lua_getallocf 拿到创建 lua_State 的时候存进去的 snlua 结构体的指针，将其 trap 变量修改为可打断状态，然后调用 luaL_error 报错来打断当前的执行逻辑。\nprofile 　snlua 中还实现了 profile 的功能，通过在脚本中调用 profile.start 和 profile.stop 来拿到中间的时间间隔。实现是通过在 start 的时候创建一个 start_time 变量，在 stop 的时候计算时间间隔。中间涉及协程的调用时，也是通过自定义的 resume 函数来实现的，在 resume 协程之前和之后，分别记录时间，将其汇总到总时间中去。\n","date":"2022-02-15T03:10:34+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E4%B9%9Dsnlua%E6%9C%8D%E5%8A%A1%E8%AF%A6%E8%A7%A3/","title":"Skynet笔记（九）snlua服务详解"},{"content":"　skynet 使用的并发模型是 actor 模型，这就使得两个 actor 之间进行交互的时候全都是通过消息来实现的，而且不单单是 actor 之间交互使用了消息，前文提到的定时调用和网络事件也都是通过消息来通知目标服务的。可以说 skynet 中消息是一切交互的依托形式，本篇会尝试理清楚消息相关的实现。\n消息的结构 1 2 3 4 5 6  struct skynet_message { uint32_t source; // 消息源的地址  int session; // 消息的 session id  void *data; // 消息的数据  size_t sz; // 消息的长度和类型，高 8bit 为类型，其余为长度 };   　消息的结构非常简单，其中 session 由源服务生成，是给 call 调用提供支持的，如果是 send 消息的话则 session 会被设为 0。\n发送消息 　不管是从 Lua 层还是 C 层发送消息，最终调用的接口都是 skynet_send 函数。\n1 2 3 4 5 6 7  int skynet_send(struct skynet_context *context, uint32_t source, uint32_t destination, int type, int session, void *data, size_t sz)   　可以看到这是一个参数非常多的函数，但是仔细一看就会发现参数的内容跟上面消息的结构基本上吻合。事实上这个函数做的事情也简单，用参数生成一个 struct skynet_message 结构的消息，然后 push 到目标服务的消息队列中去即可。\n服务消息队列 　skynet 中的每个服务都有一个 struct message_queue 结构的消息队列，它对应了 actor 模型中的 mailbox，保存着本服务的全部待处理消息。\n1 2 3 4 5 6 7 8 9 10 11 12 13  struct message_queue { struct spinlock lock; // 自旋锁  uint32_t handle; // 消息队列所属服务的 handle  int cap; // 容量，动态长度，可以扩容  int head; // 消息的头指针  int tail; // 消息的尾指针  int release; // 标记是否已经被释放  int in_global; // 标记是否在全局队列中  int overload; // 现在的负载  int overload_threshold; // 超载警告的阈值  struct skynet_message *queue; // 消息队列数组  struct message_queue *next; // 下一个消息队列，链表结构 };   　从成员变量中可以看到消息队列的实现方式。首先它是自带有一个自旋锁的，因为很多线程都有可能同时修改某个服务的消息队列，所以修改的时候都需要加锁。它通过 head，tail，cap 和 queue 组成了一个环形动态长度的数组，初始的容量为 64，当容量不够用时，每次扩容到以前容量的 2 倍。此外消息队列通过 overload 和 overload_threshold 实现了一个负载警告的功能，当拿取消息的时候，如果当前消息数量超过了当前设置的负载阈值，会输出一条服务负载过重的警告信息。\n全局消息队列 1 2 3 4 5  struct global_queue { struct message_queue *head; struct message_queue *tail; struct spinlock lock; };   　skynet 中有一个全局的消息队列，当服务的消息队列中有待处理的消息时，它会被放在全局消息队列中来。worker 线程每次就是先从全局消息队列中拿一个服务队列出去，然后处理这个服务消息队列中的消息。\nstruct global_queue 的结构比较简单，是一个 message_queue 的单向链表。带有一个自旋锁，修改链表要先加锁。\n消息处理 注册回调函数 　每个服务都需要有一个回调函数，回调函数被用来处理消息的数据。通过调用 skynet_callback 来注册回调函数。\n1  void skynet_callback(struct skynet_context *context, void *ud, skynet_cb cb)   　skynet_cb 是一个如下形式的函数指针。\n1 2 3 4 5 6 7  typedef int (*skynet_cb)(struct skynet_context * context, void *ud, int type, int session, uint32_t source, const void * msg, size_t sz);   　不同的服务有不同的回调函数设置的位置，例如最常用的 snlua 服务会在 lua 脚本中调用 skynet.start 函数，这个函数里面就会把 lua_skynet.c 中的 _cb 设为回调函数。\n数据打包与解包 　消息在发送之前有时需要对数据进行打包，比如说要发送的数据是一个 lua table 这种。如果消息的数据有进行过打包的话，那么目标服务在处理消息的时候就需要进行对应的解包操作。\n这就要求在消息的发送方和接收方两边对同一个类型的协议有相同的配置，起码 pack 和 unpack 这两个函数一定要可以实现互相转换。一个最常见的例子，在 snlua 服务中，\u0026ldquo;lua\u0026rdquo; 类型的协议的打包和解包函数分别为 skynet.pack 和 skynet.unpack，前者可以把 lua table 转为二进制数据，后者可以把前者转换的结果再次转为 lua table 格式。\n执行消息回调 　执行回调本身只负责调用注册的回调函数，需要各个服务在回调函数中正确处理自己的业务。回调可以很简单很直接，也可以很复杂。比如在 snlua 服务的回调函数就比较复杂，当消息的回调被调用时，首先调用的是直接注册的 _cb，它会从索引表中取到注册时存进去的 lua 回调函数也就是 skynet.dispatch_message 并且执行它，执行中会根据协议的类型，在 proto 中找到对应的协议类型的参数，从中拿到之前注册过的消息分发函数 dispatch，用它创建一个协程，然后执行协程。\nLua 层接口 初始化服务 　skynet.start 是绝大部分 snlua 服务会在服务的入口文件里调用的方法。它通过 C 接口，把消息回调函数设为了 _cb，_cb 相当于是一个 wrapper 函数，它会把通过参数传入的 Lua 函数 skynet.dispatch_message 保存在 LUA_REGISTRYINDEX 中，等到 _cb 被调用的时候，它会取出这个函数，把参数传给它，并且调用它。也就是说基本上所有的 snlua 服务都是用的同一个回调函数 skynet.dispatch_message 来处理消息的。\n传给 skynet.start 的函数可以看作是初始化函数，一般会在里面调用 skynet.dispatch 来为关心的消息类型指定处理函数。\n注册消息协议 　可以通过 skynet.register_protocol 来注册一类消息，其中 lua, response 和 error 是在 require skynet 的时候就已经注册过的了，不再需要重复注册。\n可以通过调用 skynet.dispatch 来修改某个消息类型的处理函数。在绝大多数 snlua 服务的初始脚本中，都包含对 skynet.dispatch 的调用，比如下面这个就是在注册 \u0026ldquo;lua\u0026rdquo; 协议的消息的回调函数。\n1 2 3  skynet.dispatch(\u0026#34;lua\u0026#34;, function(...) -- ... end)   　snlua 服务中会维护一个 proto 的 table 用来保存各类协议的数据，其中的数据正是由上文提到的 skynet.register_protocol 注册进来的。一个典型的协议会包含以下几种数据\n1 2 3 4 5 6 7 8  { name = \u0026#34;lua\u0026#34;, id = skynet.PTYPE_LUA, pack = skynet.pack, unpack = skynet.unpack, dispatch = function() end, trace = nil }   发送消息 　skynet.send 是最常用的消息发送方法，有目标服务的句柄或是名字都可以发送消息。它会根据消息的类型，找到类型对应的打包函数，将其打包，调用 C 层的接口发送出去。\nskynet.call 可以理解为类似远程调用的那种感觉，执行以后会挂起本协程，等待目标服务对本消息进行返回以后，会再次从挂起点开始执行。call 的实现深度依赖了 Lua 的协程，发送消息的部分跟 send 是一样的，只是它会拿到 send 返回的 session，同时将 session 和当前运行的协程绑定，然后调用 yield 阻塞当前协程，等待唤醒。\n处理消息 　在 skynet.dispatch_message 中可以看到，处理消息是使用 pcall 调用 raw_dispatch_message 来实现的。除了处理消息外，本函数还处理了 fork 相关的调用。\nraw_dispatch_message 中，如果消息的类型为 PTYPE_RESPONSE 也就是对 call 调用的回复，则会通过消息的 session 找到之前调用 call 挂起自己的协程，然后把结果通过 resume 传给协程，使其从挂起点之后继续执行。\n如果是其它消息类型，则通过类型拿到对应的处理函数，拿到一个新的协程，通过类型指定的解包方法解包数据，然后用协程执行处理函数和消息数据。\n","date":"2022-02-14T16:10:16+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E5%85%AB%E4%B8%80%E5%88%87%E9%83%BD%E6%98%AF%E6%B6%88%E6%81%AF/","title":"Skynet笔记（八）一切都是消息"},{"content":"　skynet 进程中一共有五种线程，主线程、monitor 线程、timer 线程、socket 线程和 worker 线程。每种线程各司其职，本篇会讲述一下各种线程的作用。\n主线程 　skynet 中的主线程只负责初始化整个进程，在前篇讲启动流程的时候，那些流程都是发生在主线程中的。它不参与业务的处理，在初始化完所有的全局变量以后，它按配置创建出了剩下的四种线程，并且等待全部线程的结束。\nmonitor 结构 　在介绍其余线程之前，首先要看一个结构体 monitor，它跟后面介绍的 monitor 线程同名，但是并不是其专用的数据，而是穿插在所有的线程中使用。\n1 2 3 4 5 6 7 8  struct monitor { int count; // worker 线程的总数  struct skynet_monitor **m; // 全部的线程 monitor 指针  pthread_cond_t cond; // 给 worker 线程挂起用的全局条件  pthread_mutex_t mutex; // 有锁结构  int sleep; // 睡眠的 worker 线程数量  int quit; // 退出标记 };   　monitor 结构体变量每个进程只有一个，可以看到它是一个有锁的结构，这里可以发现一个与其它地方明显的区别，锁直接使用了 pthread 库的 mutex 来做，而没有用 skynet 中更常用的 spinlock 来实现，主要是因为要配合 cond 来做线程挂起。\nmonitor 线程 作用 　monitor 线程的功能比较简单，monitor 顾名思义就是监控，它监控的就是所有 worker 线程的工作状态，如果 worker 线程在处理一条消息的时候用时太久了，monitor 线程会打印出一条错误日志，告诉开发者一条从 A 服务到 B 服务的消息的处理逻辑中可能有死循环存在。\n实现 　每个 worker 线程都有一个与之绑定的 skynet_monitor 的变量，它负责记录的本 worker 线程的检查状态。\n1 2 3 4 5 6  struct skynet_monitor { ATOM_INT version; // 原子 version  int check_version; // 上次检查时的 version  uint32_t source; // 当前处理的消息的源 handle  uint32_t destination; // 当前处理的消息的目标 handle };   　每次当 worker 线程开始处理一条消息的时候，它会修改自己对应的 skynet_monitor 中变量的值。其中 version 会被一直累加，source 和 destination 设为当前处理的消息的源地址和目标地址。\n线程的主函数会进入到一个无限循环中，在循环中，每 5s 会检查一次每个 worker 线程的 skynet_monitor 中的值。如果 version 和 check_version 不相等，则把 check_version 设为 version，如果相等，则说明从上次检查到这次检查也就是 5s 之内，worker 线程都在处理同一条消息。这时候就认为这个 worker 线程可能已经陷入了死循环中。把目标服务的 endless 属性设为 true，然后输出一条错误日志警告开发者。\ntimer 线程 作用 　timer 线程主要负责了两件事情，更新系统的当前时间和唤醒一个睡眠的 worker 线程。前者是为了给时间的获取接口提供时间返回值，以及执行定时任务，后者是让之前因为没有取到消息处理而睡眠的 worker 线程再次尝试去处理消息。\n自己计时的意义 　计时的实现很简单，记录了一个进程的开始时间 starttime 和一个进程从开始到现在经过的时间 current 来完成当前时间的计算，每经过 2.5ms 将 current 的值更新一次。\n之所以要框架自己实现计时的原因主要有两个。首先，业务逻辑中取时间是一个经常有的操作，而 lua 标准库中的取时间的接口 os.time 的实现是用的系统调用 time 来做的，这个系统调用会使进程进入内核态，大量使用的话效率堪忧。而 skynet 每个 tick 更新时间是调用的 clock_gettime，这个接口可以说不是系统接口，或者说是因为 linux 的 vdso 机制使这个接口可以很高效的频繁调用。第二个原因是自己实现的计时不会被系统时间影响，在进程执行的过程中如果系统时间被改掉了，自己实现的计时器还是会按以前的步骤执行，这样可以避免一些问题的发生，比如定时器的触发问题。\n定时器的实现 　skynet 中定时器的计时是使用时间轮算法来实现的，时间轮是一个被广泛用于实现高效率定时器的算法，linux kernel 的定时器也是使用时间轮算法实现的。\nskynet.timeout 是 skynet 提供的定时调用接口，它首先拿到一个新的 session 用于接收定时器的唤醒，然后创建一个包裹了等待执行的函数的协程，把协程跟 session 关联起来，然后用 session 和自己的 handle 创建一个 struct timer_event 加入到时间轮中。\n1 2 3 4  struct timer_event { uint32_t handle; int session; };   　等待时间轮转动到了指定的时间点以后，从里面取出这个 timer_event 结构体变量，使用其中的 session 来创建一条 PTYPE_RESPONSE 类型的消息，将消息 push 到目标 handle 的消息队列里。等 worker 线程处理这条消息时，根据 session 拿到对应的 co 执行它，一个定时器的调用就完成了。\nsocket 线程 作用 　socket 线程主要做了三件事，接收并且处理 socket 命令，处理 epoll 事件，然后如果当前全部的 worker 线程都在睡眠中，则唤醒其中的一个。\n需要注意的是，本线程并不会去执行任何 epoll 事件，所有事件都是转换成一条 PTYPE_SOCKET 类型的消息，发送给与相关 socket 绑定的服务。\nsocket 线程的具体实现比较复杂，后面会专门开一篇网络专文，此处只大概讲一下作用，具体实现暂时略过。\nworker 线程 作用 　worker 线程顾名思义，就是处理逻辑的主力线程了。它只做一件事情，处理消息。它会尝试去全局队列中拿到一个服务队列，然后再根据自己的负载参数，处理其中一定比例的消息。如果拿不到消息队列，会把自己投入睡眠中，等待 timer 线程或是 socket 线程唤醒自己。\n数量 　worker 线程是这几类线程中唯一可以通过 config 中的配置参数修改线程数量的。一般把 worker 线程的数量设置为本机的 cpu 核心数即可。\n工作参数 　每个 worker 线程有一个属于自己的参数结构体 worker_parm，用来保存一些本线程的参数。\n1 2 3 4 5  struct worker_parm { struct monitor *m; // 全局 monitor 的引用  int id; // 线程ID  int weight; // 工作权重 };   　其中 weight 表示的是工作权重，目的是为了尽量让不同的 worker 线程的步骤不一样，从而减轻在全局消息队列那里的锁竞争问题。\n1 2 3 4 5 6  static int weight[] = { -1, -1, -1, -1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, };   　worker 线程在拿到服务的消息队列以后，会把队列长度 n \u0026raquo;= weight 来得到本次要处理的消息数量。所以前四个线程每次只处理一条消息，后面的四个每次处理队列中的全部消息，再后面分别是每次处理总长度的 1/2，1/4，1/8 条消息，32 以后的 worker 线程的 weight 一律为 0，也就是每次处理消息队列中的全部消息。\n实现 　只要还要服务没有退出，worker 线程会一直尝试去全局队列中取服务队列并且处理其中的消息，在 skynet_context_message_dispatch 中可以看到具体的实现。\n通过调用 skynet_globalmq_pop 从全局队列中取出一个服务队列，全局队列是个链表，此处加锁以后，从链表头部取出即可。\n拿到服务消息队列以后，需要从里面拿取消息，并且处理。这个过程在一个循环中，循环的次数跟本线程的工作权重 weight 有关，通过计算当前队列中待处理的消息总长度 n \u0026raquo;= weight 得到本次要处理的消息条数。\ndispatch_message 用来处理消息，通过移位拿到消息的类型和长度，累加处理消息计数，然后调用 context 的 callback 函数进行处理。\n本轮消息处理完毕以后，会尝试获取一个新的服务队列，如果能拿到，不管当前处理的队列中还有没有剩余消息，都会把当前队列 push 回全局队列中，如果拿不到，则继续处理本队列，即使它其中已经没有消息了。\n如果一开始拿到的要处理的服务队列中本来就没有消息的话，则会把其 in_global 参数设为 0，且不会将其放回全局队列中，而是会尝试从全局队列中再拿取一个队列返回，来进行下一次的处理。这个没有消息也没有在全局队列中的服务队列会一直保持这个状态，直到下一次有其它服务向其发送消息，这时候会重新把其 in_global 参数设为 MQ_IN_GLOBAL，并把其 push 回全局队列中。\n当 worker 线程已经从全局队列中取不到服务队列时，它会锁上 monitor 结构的锁，然后累加休眠的线程数 sleep，并且使用 pthread_cond_wait 将线程阻塞在 monitor.cond 上，等待 socket 线程或是 timer 线程唤醒。\n","date":"2022-02-13T17:36:02+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E4%B8%83%E5%90%84%E7%A7%8D%E7%BA%BF%E7%A8%8B%E7%9A%84%E4%BD%9C%E7%94%A8/","title":"Skynet笔记（七）各种线程的作用"},{"content":"　handle 作为服务的句柄，在 skynet 的消息发送和消息处理中都起到了不可或缺的作用。handle 的管理模块主要提供了 “通过 handle 查询服务地址” 和 “通过服务名字查询 handle” 这两个功能。本篇来讨论一下 skynet 是如何对 handle 进行管理的。\n管理器结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  struct handle_name { char *name; uint32_t handle; }; struct handle_storage { struct rwlock lock; // 读写锁，因为读取的频率远远高于写入  uint32_t harbor; // 本节点的 harbor id  uint32_t handle_index; // 当前的索引值，会累加  int slot_size; // slot 的长度  struct skynet_context **slot; // 全部 ctx  int name_cap; // 名字列表的容量  int name_count; // 名字列表的总数  struct handle_name *name; // 保存全部的名字 }; static struct handle_storage *H = NULL;   　每个 skynet 进程都有一个全局的 handle 管理器，会在进程启动时被初始化。它是个带有读写锁的结构，因为大部分的操作都是查询操作，所以读写锁保证了查询的效率。保存 handle的哈希→服务 键值对的 slot 是一个动态数组，不够了会扩容，初始长度是 4，每次扩容会在原长度上翻倍。name 用来保存全部的 handle 和名字的对应关系，是一个有序的动态数组，按 handle_name 的 name 字段的字符顺序进行排序，初始长度是 2，每次扩容在原长度上翻倍。\n通过 handle 查询服务地址 注册 handle 　想要查询首先要注册数据。服务在创建的时候会调用 skynet_handle_register 为服务注册一个 handle 并且插入到 slot 中。\n1  uint32_t skynet_handle_register(struct skynet_context *ctx)   　插入数据的部分值得讲一讲，slot 的 key 是通过 handle 对 slot_size 取余数的来的。handle 有一个 handle_index 做标记量，是从 1 开始的。如果不考虑服务退出的话，其实 handle 就一直累加就好了，也不需要取余数了，不够了就扩容。实际在服务退出的时候，handle 不会回收，但是 slot 中占据的位置要清掉。这就造成了 slot 的空洞问题，插入数据就变得麻烦了起来，因为要找到空洞安放数据。\n 以写模式锁住读写锁 把 handle 赋值为 handle_index 的值 用 handle 对 slot_size 取余，拿到一个位置 如果这个位置是空的，则找到了合适的位置，保存在 slot 中，解开读写锁，返回 handle 即可 如果这个位置不为空，则累加 handle，再检查，一直累加整个 slot_size 的数值，此时所有 slot 的位置都被检查了一遍了 如果还没找到，那就要扩容了，把 slot 扩容到原先的两倍，然后重复遍历检查的步骤  　这里有两个小限制，首先 slot_size 不能超过 16777215，还有就是当 handle 超过 16777215 以后，handle 会从 1 再次开始，理论上来说有重复的风险。\n释放 handle 　skynet 会在服务退出后移除掉 slot 中对应的项。在 slot 中形成空洞，这也是为什么在插入的时候要遍历整个 slot 找空位置的原因。\n查询 　可以通过 handle 直接查询到服务的地址。\n1  struct skynet_context *skynet_handle_grab(uint32_t handle)   　有了上面的注册以后，这个查询就很简单了，算出 handle 的哈希，直接从 slot 里面取出来服务的地址返回即可。因为不涉及到修改，所以查询只锁读锁就行了。\n通过服务名字查询 handle 注册名字 　可以通过调用 skynet.name 来注册一个指定服务的名字。该接口最终会调用到 C 层的\n1  const char *skynet_handle_namehandle(uint32_t handle, const char *name)   　这个函数会锁上管理器的写锁，并且把自己的名字和 handle 插入到管理器的 name 数组中去。因为 name 是一个有序的动态数组，所以在插入的时候同样需要维持顺序。实际实现是通过二分查找搜索目标的名字，如果查到了，则说明名字重复了，返回了 NULL，不会覆盖旧名字。如果没有查到，则结果位置就是合适的插入位置，然后把名字和 handle 组合一下插入到 name 的合适位置中去。插入过程有可能引起数组的扩容，每次把容量扩充到当前容量的两倍。\n注册名字是如果是本地名字则应该在前面加上一个点号，类似 \u0026ldquo;.xxx\u0026rdquo; 来区别本地名字和 harbor 的全局名字。本地名字没限制，harbor 的服务名字不能超过 16 个字符。\n查询 　因为 name 是个有序的数组，所以查询这一步同样使用二分查找搜索数组即可。因为不涉及到修改，所以只锁上读锁即可。\n","date":"2022-02-12T03:10:43+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E5%85%ADhandle%E7%9A%84%E7%AE%A1%E7%90%86/","title":"Skynet笔记（六）handle的管理"},{"content":"　skynet 是一个 actor 模型的消息框架，每个 actor 在 skynet 中就是一个服务。服务可以说是 skynet 中最重要的概念，本篇会介绍 skynet 中的服务。\n概述 　skynet 中的服务是基于上文中前文中提到的 module 在创建的，module 为服务提供了私有数据的存储位置。服务作为一个 acotr，拥有自己的消息队列，可以向别的服务发送消息，也可以接收别的服务的消息并把消息交给消息回调函数进行处理。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  struct skynet_context { void *instance; // C 模块副本的引用  struct skynet_module *mod; // C 模块的地址  void *cb_ud; // callback userdata  skynet_cb cb; // 回调函数  struct message_queue *queue; // 服务消息队列  ATOM_POINTER logfile; // 文件指针是个原子指针  uint64_t cpu_cost; // in microsec // 消耗的总 cpu 时间  uint64_t cpu_start; // in microsec // 本次消息处理的起始时间点  char result[32]; // 用来保存命令的执行结果  uint32_t handle; // 本服务注册获得的对应 handle  int session_id; // 消息的 session id 分配器，不断累加  ATOM_INT ref; // 引用着的数量  int message_count; // 处理过的消息总数  bool init; // 初始化完成标记  bool endless; // 死循环标志  bool profile; // 是否开启了 profile  CHECKCALLING_DECL };   　虽然 skynet_context 中的数据项很多，但是其实核心的数据就是跟消息处理有关的那几个。worker 线程会不断为消息队列中每个消息调用回调函数进行处理。\n创建服务 1  struct skynet_context *skynet_context_new(const char *name, const char *param)   　不管是从 C 层还是从 Lua 层创建一个新的服务，最后的创建函数都是 skynet_context_new，这个函数接受一个 module 名字和一个传给 module 的 init 函数的额外参数，返回一个 struct skynet_context 变量指针。\n服务在创建的时候有几个关键步骤：\n 通过 module 名字去查找 module 的地址，然后调用 module 的 create 接口创建一个数据副本，这两个值会保存在 skynet_context 的 mod 和 instance 字段中。 注册服务的 handle，服务的 handle 是用来完成 名字 → handle 和 handle → 服务 的映射。 创建服务的消息队列。 调用 module 的 init 函数初始化之前创建好的 module 数据副本。 把服务的消息队列 push 到全局的消息队列中去。  引用计数 　每个服务都有一个原子类型的引用计数变量 ref，当有地方引用一个服务时，调用 skynet_context_grab 来增加服务的引用计数，在引用结束以后需要手动调用 skynet_context_release 来减少服务的引用计数。\n1 2  void skynet_context_grab(struct skynet_context *ctx) struct skynet_context *skynet_context_release(struct skynet_context *ctx)   　ref 在服务创建完毕以后会的值为 1，正常的引用和解引用是不会使 ref 变为 0 的，如果需要删除一个服务，可以在服务的逻辑中调用 skynet.exit 或者是通过后台命令来减少服务的引用次数，然后等别的地方的引用全部结束了，ref 会变为 0 来触发删除服务的函数 delete_context.\n1  static void delete_context(struct skynet_context *ctx)   　之所以要进入一个引用计数而不是调用一个删除函数直接进行删除，主要原因是因为要删除的目标服务可能正在被别处引用，比如它的消息队列正在被 worker 线程处理，直接删除会有问题，所以虽然维护引用计数比较麻烦，但是其实已经是一个比较巧妙的解法了。\n消息队列 1 2 3 4 5 6 7 8 9 10 11 12 13  struct message_queue { struct spinlock lock; // 自旋锁  uint32_t handle; // 消息队列的 handle  int cap; // 容量，模仿 vector 的实现，不够了会自己扩容  int head; // 消息的头指针  int tail; // 消息的尾指针  int release; // 标记是否已经被释放  int in_global; // 标记是否在全局队列中  int overload; // 现在的负载  int overload_threshold; // 超载警告的阈值  struct skynet_message *queue; // 消息队列数组  struct message_queue *next; // 下一个消息队列，链表结构 };   　每个服务都有属于自己的一个 struct message_queue 结构的消息队列。消息队列是有锁结构，实现方式是一个动态环形数组，使用 head 表示目前队列中最早的一条消息的索引值，tail 表示目前队列中最晚的一条消息的索引值。在 push 消息进来的时候会检查是否还有剩余位置，如果没有了会调用 expand_queue 扩容这个数组，容量初始的时候是 64，每次扩容会把当前容量直接翻倍。\nhandle 　上文提到了，handle 的作用是用来完成 名字 → handle 和 handle → 服务 的映射。首先，skynet 中同进程不同服务之间发送消息的本质就是把消息 push 到目标服务的消息队列中，这个操作中最重要的一个步骤就是拿到目标服务的消息队列的地址，又因为有了目标服务的地址就能拿到其消息队列的地址，所以这个步骤变为了拿到目标服务的地址。\nskynet 中发送消息的时候，指定目标可以通过服务的名字或是 handle，如果目标的参数是名字，需要两步转换，首先要根据服务的名字拿到服务的 handle，然后再根据 handle 拿到服务的地址。如果参数是 handle 则直接根据 handle 拿到服务的地址即可。\n这个过程能不能简化呢？在 C 语言中很难实现，如果是 C++ 的话，可以考虑强制要求服务一定要有名字，然后使用一个 unordered_map 把服务的名字和地址对应起来，发送消息的时候也只能通过名字来指定目标服务，这样可以简化掉 handle 这个转化过程，但是也加了不小的限制。\n","date":"2022-02-11T17:19:18+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E4%BA%94%E8%AE%A4%E8%AF%86%E6%9C%8D%E5%8A%A1/","title":"Skynet笔记（五）认识服务"},{"content":"　module 是 skynet 中一切 actor 的基石，每个 actor 的本质都是一种 module 的具象化。本篇会讲解跟 module 有关的内容。\n概述 　skynet 自带了四个 module, 在 service-src 目录下面每个 service_xxx.c 的文件是一个对应的 module. 编译完成之后，module 所在的目录是 cservice, 目录下的 xxx.so 就是对应的 module 动态库。启动一个服务的操作其实就是启动一个 module 实例，比如最常见的 lua 层的服务，就全部都是 service_snlua 的实例。\n结构 　module 有自己的名字，在查找的时候是通过名字来查找对应的 module 的。每个 module 会被单独编译，编译成一个动态库文件 *.so, 在需要的时候才会被加载。module 可以有四个指定函数，分别为：\n xxx_create xxx_init xxx_release xxx_signal  1 2 3 4 5 6 7 8 9 10 11 12 13  typedef void * (*skynet_dl_create)(void); typedef int (*skynet_dl_init)(void * inst, struct skynet_context *, const char * parm); typedef void (*skynet_dl_release)(void * inst); typedef void (*skynet_dl_signal)(void * inst, int signal); struct skynet_module { const char *name; // 模块的名字  void *module; // 模块的地址  skynet_dl_create create; // create 函数地址  skynet_dl_init init; // init 函数地址  skynet_dl_release release; // release 函数地址  skynet_dl_signal signal; // signal 函数地址 };   　create 接口用于创建一份私有数据结构体出来，init 用来初始化这个私有数据，release 在服务退出的时候被调用，用来释放私有数据，signal 用来接收 debug 控制台的信号指令。\n管理器 　每个 skynet 进程有一个全局的 module 管理器，它会在进程启动的时候被函数 skynet_module_init 给初始化。它本身是有锁的，修改的时候要加锁，不过这个到无所谓，因为很少增加模块。最多只支持 32 个 module 的注册，再多要自己改一下，不过一般不会用到那么多，因为大部分都是 lua 服务。\n1 2 3 4 5 6 7 8 9 10 11  #define MAX_MODULE_TYPE 32  struct modules { int count; // 已经加载的 module 的总数量  struct spinlock lock; // 自旋锁  const char *path; // module 的加载目录  struct skynet_module m[MAX_MODULE_TYPE]; // 全部的模块 }; // 全局对象管理全部的 C 模块 static struct modules *M = NULL;   module 的加载 　在创建一个新服务的时候，创建函数会调用 skynet_module_query 来查找指定名字的 module 地址, 这一步会遍历所有已经加载的 module 来查询，因为总数很少，所以遍历并没有什么问题。如果查到了，则直接返回 module 的地址给查询方。\n如果没查到，则需要走 module 的加载流程。因为 path 是支持多地址的，多地址用 \u0026quot;;\u0026quot; 分隔，所以加载的过程其实就是依次在每个地址下找对应名字的动态库，如果找到了，就使用系统调用 dlopen 加载动态库拿到加载后的地址，然后用系统调用 dlsym 来分别拿到 xxx_create/xxx_init/xxx_release/xxx_signal 的地址，最后将 api 函数的地址，module 的地址，module 的名字保存到管理器的 module 数组的最新一个数据，并且把 count 累加。到这里 module 的加载就完成了。返回 module 的地址给查询方即可。\nmodule 实例的创建 　创建一个 module 的实例，本质上就是创建一个 module 的私有数据结构体，这份数据在不同的 module 中结构不一样，比如在 snlua 中，结构体就是 struct snlua, 其中包含了 lua 虚拟机等数据。同一个 module 创建出的不同实例之间共享 module 提供的接口，但是数据是互相独立的。\n创建过程中，会首先调用 module 的 create 接口创建出一份私有数据，然后再调用 init 接口初始化这份数据。在释放的时候会调用 release 接口释放这份数据。\n","date":"2022-02-10T23:36:55+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E5%9B%9B%E8%AE%A4%E8%AF%86module/","title":"Skynet笔记（四）认识module"},{"content":"　上文写到在 skynet 进程启动过程中，bootstrap 函数会读取传入的配置文件中 bootstrap 对应的项加载启动脚本，其中 bootstrap 一般的配置为 \u0026ldquo;snlua bootstrap\u0026rdquo;, 在执行中实际操作就是启动一个 snlua 服务，启动参数为 bootstrap.\n执行步骤  启动了 launcher 服务 读取 harbor 配置处理 harbor 模式，如果没有使用 harbor 则启动一个 cdummy 服务负责拦截对外广播的全局名字变更，如果使用了 harbor 则在主节点启动 cmaster 服务，并且不管是否主节点都启动 cslave 服务 如果是未使用 harbor 的情况下，启动 datacenterd 服务 启动 server_mgr 服务 如果配置了 enablessl 则会加载 ltls 模块 启动配置文件中 start 配置指定的入口服务  launcher 服务 　虽然名字是叫 launcher, 但是负责的任务除了启动器的责任还多了一点，主要是给 debug 后台提供一些针对服务的操作接口。比如执行 GC, 查看内存，杀死某个服务之类的。\n服务支持两种类型的协议，\u0026ldquo;lua\u0026rdquo; 和 \u0026ldquo;text\u0026rdquo;, 其中 lua 协议是给 lua 层的各种接口用的，text 的唯一用途是，snlua 启动错误的时候，会给 launcher 发送一个 \u0026ldquo;ERROR\u0026rdquo; 消息来报告错误。\n绝大部分支持的命令可以参考 wiki 中 DebugConsole 的部分。其余命令中，最常用的，也是 launcher 的本职工作的是 LAUNCH 它负责调用 skynet.launch 启动一个服务，并且把服务的 handle 保存，以备别的服务查询。\ndatacenterd 服务 　datacenterd 只能用来给 harbor 模式下的服务器架构提供一个数据中心的功能。提供的操作支持很有限，会启动在主节点上，并且 cluster 模式下不可用。可以参考 DataCenter 的内容。\n datacenter.get 获取指定 key 的值 datacenter.set 设置一个键值对 datacenter.wait 等待一个值被设置，设置后会返回  service_mgr 服务 　主要为两个功能提供服务，第一个是 UniqueService, 第二个是 snax. 它会把自己注册成 \u0026ldquo;.service\u0026rdquo; 的名字，可以提供全局服务的名字查询和全局服务创建的功能\n","date":"2022-02-10T23:02:50+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E4%B8%89bootstrap%E6%9C%8D%E5%8A%A1%E8%AF%A6%E8%A7%A3/","title":"Skynet笔记（三）bootstrap服务详解"},{"content":"　本篇大概讲一下 skynet 进程的启动流程，完全了解启动流程基本上也就明白了 skynet 的原理了，本篇会大概提到全部的启动流程。\nmain  根据参数拿到配置文件的路径 初始化全局数据结构 skynet_node *G_NODE 初始化全局环境变量 skynet_env *E 忽略信号 SIGPIPE 创建一个虚拟机加载一段硬代码（hard code）进去并且加载配置文件 把读取到的配置文件的内容写入到环境变量中 从环境变量中拿到 skynet_config config 需要的值初始化它 把加载配置创建出来的虚拟机关闭 把 config 的地址作为参数传给 skynet_start 函数继续后面的步骤  skynet_start  处理信号 SIGHUP 处理 daemon 相关的配置，如果配置了 daemon 启动的话，调用 daemon_init 处理 daemon 相关的操作 初始化 harbor 节点数量 初始化 handle 管理器 handle_storage *H 初始化全局消息队列 global_queue *Q 初始化 C 模块管理器 modules *M 初始化全局时间管理器 timer *TI 初始化全局 socket 管理器 socket_server *SOCKET_SERVER 标记性能测试标志 创建 config 中的 logger 服务 调用 bootstrap函数，传入配置中的 bootstrap 字段内容 调用 start 函数，传入配置中的 thread 字段  bootstrap  根据配置中的 bootstrap 字段内容，启动一个服务，一般皆为 snlua bootstrap, 即为启动一个 bootstrap 服务。该服务会启动 lua 层相关的基础服务，并且最后启动配置中 start 指定的服务。  start  创建一个 pthread_t 结构的数组 pid 来保存所有线程的 pid 创建 monitor *m, 这个变量用来管理全部线程的 monitor 创建 monitor 线程 创建 timer 线程 创建 socket 线程 创建所有的 worker 线程 调用 pthread_join 等待全部线程的结束 ","date":"2022-02-10T16:24:34+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E4%BA%8C%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/","title":"Skynet笔记（二）启动流程"},{"content":"　要学习 skynet 源码，首先需要了解源码的文件分布，skynet 的源码排布还是很有规范的，利于了学习。本篇会以 1.4.0 版本的 skynet 源码为准，对各个文件夹其中包含的代码做一个大致的说明。\n 3rd 使用的第三方库  jemalloc 内存分配默认使用了 jemalloc, 如果不使用也可以，加编译选项 -DNOUSE_JEMALLOC , 参考资料：jemalloc lpeg 一个文本模式匹配库，貌似是 lua 作者写的, 业务里没用，只有 sproto 用到了，可以不管，参考资料：LPeg lua 一直用的最新版的，已经 5.4.4 了，参考资料：lua lua-md5 为 lua 提供 md5 支持，参考资料：md5      examples 一些示例服务，刚接触一个模块的时候可以看看\n login 一个简单的登录示例模块    lualib skynet 自带的 lua 库\n  compat10 为了兼容 skynet 1.0 及以前的目录做的一堆兼容转换，如果要用的话，把本目录加到 lua_path 里。\n  http 提供了对 http 的相关支持，不是特别完善，但是基本上也算是该有的都有了，做后台那些可以用。\n  skynet 提供了大量的框架功能相关的 lua 库\n  datasheet 把一个复杂的有一定限制的 lua 表，转换为一块 C 内存，由多个 lua 服务共享读取。参考资料：skynet_datasheet\n  db 提供了数据库的接口\n redis 包含了两部分，同名子目录里的是 redis 集群的客户端，外面的文件是单机 redis 客户端。 mongo 提供了 mongodb 的接口。 mysql 提供了 mysql 的接口。    sharedata 旧的共享数据模块，已经被上面的 datasheet 取代了。\n    snax 一个由框架提供的简化服务的模块，其实没啥用，本来用法就不复杂，参考资料：snax\n      lualib-src 是 lualib 中的 lua 功能的 c 部分的代码\n sproto 内置了一份 sproto 的代码，如果项目用 protobuf 那就不用看了，貌似几乎没人用这个东西，参考资料：sproto    service 框架提供的一些内置服务\n  service-src 提供了一些 C 模块\n service_gate C 层的一个网关模块，貌似已经用 lua 重构了，参考资料：skynet_gate_lua_version service_harbor harbor, 这个应该没啥人用吧，本意是想抹平本进程和跨进程服务之间的通信的差距，参考资料：skynet_harbor_redesign service_logger 日志模块 service_snlua 最重要的提供 lua 服务能力的模块，所有 lua 服务都是从该模块启动的。    skynet-src 框架本身的代码\n atomic 封装了原子操作的宏定义，在支持 C11 的编译器中，直接用了标准库的 atomic 库，参考资料：原子操作库、skynet_stdatomic malloc_hook 内存相关的操作在这里，包括了服务的内存统计，内存泄露检查等 rwlock 提供了一份读写锁的实现，使用了原子操作来实现 skynet_daemon 封装了 daemon 相关的操作，提供了把指定 pidfile 文件相关的进程转为 daemon 或者取消 daemon 的接口 skynet_env 提供了全局 env 变量的操作，使用了一个 lua 虚拟机进行管理 skynet_error 处理服务的错误信息，把错误信息发送给之前设置过的 logger 服务 skynet_handle 提供了服务名字到 handle 和从 handle 到服务结构的两个映射关系，要通过名字查服务的话都要过这里 skynet_harbor harbor 功能的实现 skynet_imp 一些乱七八糟的东西扔在这里，估计是没地方放的东西 skynet_log 日志功能的实现 skynet_main 进程入口文件，定义了 main 函数，处理了配置 skynet_malloc 声明了内存操作的接口，实际调用了 malloc_hook 中的定义 skynet_module C 模块的管理部分，提供了 C 模块的全局注册，查询等功能 skynet_monitor 每个 work 线程有一个 skynet_monitor 结构，用来给 monitor 线程检查用的 skynet_mq 消息队列，包括了全局队列和服务队列的实现都在这里 skynet_server 服务的管理模块，包含了全局管理结构和各种服务相关的操作接口 skynet_socket socket 相关的功能实现，包括了全局 socket 管理器和 socket 的操作接口实现 skynet_start 框架启动函数，包括了各种线程的启动，各类全局管理模块的初始化 skynet_timer 时间相关功能的实现，除了全局时间管理模块，还有定时器的实现，时间轮 socket_buffer 给 socket 定义了一个发送缓冲结构 socket_epoll IO 复用的 epoll 实现 socket_info 定义了 socket 的连接信息结构 socket_server socket 线程的核心操作，定义了 socket 相关的几乎全部操作 spinlock 自旋锁，也是用原子操作来实现的   ","date":"2022-02-10T15:33:55+08:00","permalink":"https://wmf.im/p/skynet%E7%AC%94%E8%AE%B0%E4%B8%80%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/","title":"Skynet笔记（一）目录结构"},{"content":"　是的，在经历了这些年多次的开坑和弃坑之后，终于又又又一次决定要写博客了。作为再开坑的第一作，首先来写一写这些年写博客折腾的历史以及为什么又要写博客。\n折腾的历史 WordPress 时代 　学生时代的时候，查资料看到别人的博客，觉得挺好的，想给自己也整一个。那时候感觉有个自己的网站挺牛逼的，大概算是个可以吹的点了，就兴冲冲的开始查资料怎么建站。从一开始完全不懂，开始看视频学，买了一个喜欢的域名，又买了一个云服务器。事后证明那视频巨坑，就是商家搞出来忽悠人的，域名托管在了一个不知名的服务商那里，不能转出，操作界面突出一个烂，还经常上不去。这还不是最惨的，最惨的是云服务器也是他家的，一百来块钱一年，配置忘了，反正就是各种卡，各种慢。不过折腾了半天，终于是搞定了，在一个烂服务器上搭了一套 LAMP，可以想象有多卡了，再加上服务器的网络也不行，导致访问完全随缘，写东西更是痛苦。在这种情况下，断断续续写了一点东西，后来就完全懒得上了。写的内容都完全忘记了，只记得当时用的那个小猫主题挺好的。\nCSDN 时代 　毕业以后，在我还在家里摸鱼没找工作，还在准备做一个 Cocos2d-x 前端打工仔的时候，看了不少大佬的博客，学到了挺多有用的知识。那一年的 CSDN 还不是现在的 CSDN，所以就有样学样在 CSDN 开了个坑记一些学习中碰到的问题。写了应该有几十篇，以 Cocos2d-x 有关的内容为主，可能也有几篇 C++ 的。后来工作了，各种阴差阳错，转来做后端了，再加上 Cocos 式微，Cocos 基本再也没碰过了。那时候工作轻松，下了班天天忙着打游戏，博客就弃坑了。\nHEXO 时代 　工作了一段时候以后，有次上网发现了 hexo，感觉挺好的，直接把 Markdown 编译成静态页面访问，做博客完全够用了。那时候正好因为科学上网手里一个搬瓦工的服务器，只用来跑 SS Server 也浪费了点，就开始折腾 hexo 了。hexo 确实比之前那套 LAMP 的方便多了。挑了半天还是选了那个用的人最多的 next 主题。这次学聪明了，从一个大服务商那里买了个域名。凭心而论 hexo 还是不错的，部署方便，主题多，使用也简单，除了文章多了以后生成一次很慢以外别的没什么问题。这次开坑大概也写了几十篇，然后再次沉迷打游戏，慢慢就弃坑了。\nHUGO 旧时代 　又有一次上网的时候，看到有人在讨论 hugo，go 语言写的，速度比 hexo 不知快到哪里去了，而且从 hexo 迁移过去很方便。看到这个的时候，我又想起了自己弃坑好久的博客，想重新捡起来，于是又投入到了折腾 hugo 中。hugo 确实快，不过那时候主题还很少，挑来挑去才找到一个没什么 bug 的主题。但是这时候我科学上网已经不自己部署了，改用机场了，所以就去学了学，把博客部署在了 Github Pages 上。这次又写了一些，然后再次沉迷游戏弃坑了博客。\n笔记时代 　有次跟一个朋友讨论问题，他说这个东西他记过笔记，然后给我截了个图。我感觉挺方便的，完全没有了部署的问题，随想随写，而且反正是完全给自己写的，写起来很随意，就感觉挺喜欢的。而且笔记软件支持的功能比 Markdown 要丰富的多，链接，块编辑等等。我就花了一些时间，把市面上常见的笔记软件都试了试，估计试了十来款，最后挑了一个最喜欢的，把之前博客的 Markdown 文件都导入了进去。这次写了很久，一直写到了前几天。结果碰到了一个软件 bug，把我辛辛苦苦写的几百字给搞没了。当时就有一种游戏没存档结果闪退了的感觉。想来想去，还是继续把博客搞起来吧，这样跟别人讨论问题的时候，可以直接发链接了，而且之前买了一个自己名字的域名，正好也可以用起来。就又把文章都导出成了 Markdown，不过那些时间、标签之类的标记都丢失了，有一些文档的格式也出了点问题。\nHUGO 新时代 　因为之前就喜欢 hugo，所以这次还是选的 hugo 来搞。轻车熟路部署好，剩下的就是整理笔记了。笔记写的很随意，而且导出的时候有一些格式出了问题，所以就不准备原样导入成博文了。准备花点时间把笔记整理一下，感觉已经没用或者写的不好的就不再贴上来了，只把感觉有价值的文章整理成博文发出来。\n为什么要写博客 　回到标题，之所以写博客，对我来说主要是出于记录这个需求，其次才是分享，再次是为了用上一直闲置的域名。在学习过程中总结还是很重要的，不然看了的东西总是看两眼就以为自己会了，但是其实不会。总结的过程会让自己变成一个讲述人，一个东西能有条理的讲清楚的时候，大概就可以算得上是真正掌握了吧。\n总之，希望这次开坑可以一直写下去。\n","date":"2022-02-10T03:19:19+08:00","permalink":"https://wmf.im/p/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%88%E8%A6%81%E5%86%99%E5%8D%9A%E5%AE%A2/","title":"为什么又要写博客"}]