[{"content":"　最近在学习 Go 语言，简单学完语法以后，想找个框架看看。正好 Gin 框架的代码非常精炼，不算 test 和注释的话，只有四千多行，名字也比较好听，注释也写的比较完整，非常适合阅读，就果断 fork 一份开始看了。本篇记录下学习中的要点。\n介绍 Gin is a web framework written in Go (Golang). It features a martini-like API with much better performance, up to 40 times faster thanks to httprouter. If you need performance and good productivity, you will love Gin.\nGin 的官网对自己的介绍是 “高性能” 和 “良好生产效率” 的 web 框架。还提到了两个其它的库，第一个是 martini，它也是一个 web 框架，名字也是酒名，Gin 说自己用了它的 API，但是速度比它快 40 倍。使用了 httprouter 就是能够实现比 martini 快 40 倍的一个重要原因。\n在官网的 Features 中，有 8 个被列出来的关键功能，后面会慢慢看到这些功能的实现。\nFast Middleware support Crash-free JSON validation Routes grouping Error management Rendering built-in/Extendable 从一个小例子入手 来看一个官网文档给出的最小例子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 package main import \u0026#34;github.com/gin-gonic/gin\u0026#34; func main() { r := gin.Default() r.GET(\u0026#34;/ping\u0026#34;, func(c *gin.Context) { c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;pong\u0026#34;, }) }) r.Run() // listen and serve on 0.0.0.0:8080 } 运行该例子，然后使用浏览器访问 http://localhost:8080/ping 即可得到一个 \u0026ldquo;pong\u0026rdquo;，233。\n这个例子很简单，拆分一下只有三步。\n使用 gin.Default() 创建一个默认配置的 Engine 对象。 为 Engine 的 GET 方法的 \u0026ldquo;/ping\u0026rdquo; 地址注册一个回调函数，该函数会返回一个 \u0026ldquo;pong\u0026rdquo;。 启动 Engine，开始监听端口提供服务。 HTTP Method 通过上面小例子中的 GET 方法可以看出在 Gin 中需要使用对应的同名函数注册 HTTP Method 的处理方法。\nHTTP Method 有九种方法，最常用的有四种，分别是 GET/POST/PUT/DELETE，分别对应了查询、插入、更新、删除四种功能。需要注意的一点是，Gin 还提供了 Any 接口，可以直接对一个地址绑定所有的 HTTP Method 处理方法。\n返回的结果一般包含两到三部分，其中 code 和 message 是一定会有的，data 一般用来表示额外的数据，如果没有额外数据需要返回的可以不使用。例子中的 200 就是 code 字段的值，pong 就是 message 字段的值。\n创建 Engine 变量 在上面的例子中，创建 Engine 使用的是 gin.Default()，不过这个函数是对 New 的封装，实际创建 Engine 都是通过 New 接口创建的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func New() *Engine { debugPrintWARNINGNew() engine := \u0026amp;Engine{ RouterGroup: RouterGroup{ // ... 初始化 RouterGroup 的字段 }, // ... 初始化其余字段 } engine.RouterGroup.engine = engine // 将 engine 的指针保存在 RouterGroup 中 engine.pool.New = func() any { return engine.allocateContext() } return engine } 先来简单看一下创建的过程即可，暂时不去关注 Engine 结构中的各种成员变量的含义。可以看到 New 除了创建并初始化了一个 Engine 类型的变量 engine 以外，还把 engine.pool.New 设为了一个调用 engine.allocateContext() 的匿名函数，这个函数的作用后面再说。\n注册路由回调函数 在 Engine 中有一个内嵌结构体 RouterGroup，Engine 的与 HTTP Method 有关的接口都是继承自 RouterGroup 的，官网提到的功能点里的 Routes grouping 就是通过 RouterGroup 结构体来实现的。\n1 2 3 4 5 6 type RouterGroup struct { Handlers HandlersChain // Group 自身的处理函数 basePath string // 关联的基本路径 engine *Engine // 保存关联的 engine 对象 root bool // root 标志，只有 Engine 默认创建的才是 true } 每个 RouterGroup 关联了一个基本路径 basePath，Engine 内嵌的 RouterGroup 的 basePath 为 \u0026ldquo;/\u0026rdquo; 。\n还有一组处理函数 Handlers，所有本组关联的路径下的请求都会额外执行本组的处理函数，主要是用于中间件的调用。Handlers 在 Engine 创建的时候是 nil，可以通过 Use 方法导入一组函数进去，后面会看到这个用法。\n1 2 3 4 5 6 func (group *RouterGroup) handle(httpMethod, relativePath string, handlers HandlersChain) IRoutes { absolutePath := group.calculateAbsolutePath(relativePath) handlers = group.combineHandlers(handlers) group.engine.addRoute(httpMethod, absolutePath, handlers) return group.returnObj() } RouterGroup 的 handle 方法是所有注册 HTTP Method 回调函数的最终入口，一开始那个例子中调用的 GET 以及其它与 HTTP Method 有关的方法都只是对 handle 方法的封装。\nhandle 方法会根据 RouterGroup 的 basePath 和相对地址参数计算出绝对地址，同时调用 combineHandlers 方法得到最终的 handlers 数组。将这些结果作为参数，传给 Engine 的 addRoute 方法来注册处理函数。\n1 2 3 4 5 6 7 8 func (group *RouterGroup) combineHandlers(handlers HandlersChain) HandlersChain { finalSize := len(group.Handlers) + len(handlers) assert1(finalSize \u0026lt; int(abortIndex), \u0026#34;too many handlers\u0026#34;) mergedHandlers := make(HandlersChain, finalSize) copy(mergedHandlers, group.Handlers) copy(mergedHandlers[len(group.Handlers):], handlers) return mergedHandlers } combineHandlers 方法做的事情就是创建了一个切片 mergedHandlers，然后先将 RouterGroup 本身的 Handlers 复制进去，再将参数的 handlers 复制进去，最后将 mergedHandlers 返回。也就是说使用 handle 注册任何方法的时候，实际的结果都是包括了 RouterGroup 本身的 Handlers 的。\n使用基数树加速路由检索 在官网提到的功能点的 Fast 里提到了网络请求的路由是基于基数树（Radix Tree）实现的。这部分并不是 Gin 实现的，而是开篇在 Gin 的介绍里提到的 httprouter 实现的，Gin 使用了 httprouter 来做了这部分的功能。有关于基数树的实现这里暂且不提，暂时只关注它的用法，可能后面会另开一篇专门写一下基数树的实现。\n在 Engine 中有一个 trees 变量，它是一个 methodTree 结构的切片，正是这个变量保存了所有的基数树的引用。\n1 2 3 4 type methodTree struct { method string // method 的名字 root *node // 链表的 root 节点指针 } Engine 为每个 HTTP Method 维护了一颗基数树，这棵树的 root 节点与 Method 的名字一起被保存在了一个 methodTree 变量中，所有的 methodTree 变量又都在 trees 里。\n1 2 3 4 5 6 7 8 9 10 11 func (engine *Engine) addRoute(method, path string, handlers HandlersChain) { // ... 省略一些代码 root := engine.trees.get(method) if root == nil { root = new(node) root.fullPath = \u0026#34;/\u0026#34; engine.trees = append(engine.trees, methodTree{method: method, root: root}) } root.addRoute(path, handlers) // ... 省略一些代码 } 可以看到在 Engine 的 addRoute 方法中，会首先使用 trees 的 get 方法拿到 method 对应的基数树的 root 节点。如果没有取到基数树的 root 节点，则说明之前没有对该 method 注册过方法，会创建一个树节点作为树的根节点并将其加入到 trees 中。\n在拿到 root 节点以后，使用 root 节点的 addRoute 方法为路径 path 注册一组处理函数 handlers，这一步就是为 path 和 handlers 创建一个节点存入基数树中，如果试图注册一个已经注册过的地址，addRoute 会直接抛出一个 panic 错误。\n在处理 HTTP 请求的时候，就需要通过 path 查找对应的节点的值。root 节点有一个 getValue 的方法负责处理查询的操作，后面讲到 Gin 处理 HTTP 请求的时候会提到。\n导入中间件处理函数 RouterGroup 的 Use 方法可以导入一组中间件处理函数。官网提到的功能点里的 Middleware support 就是通过 Use 方法实现的。\n在一开始的例子中，创建 Engine 结构体变量的时候并没有使用 New，而是使用了 Default，我们来看一下 Default 做了什么额外的东西。\n1 2 3 4 5 6 func Default() *Engine { debugPrintWARNINGDefault() // 输出日志 engine := New() // 创建对象 engine.Use(Logger(), Recovery()) // 导入中间件处理函数 return engine } 可以看到它是个很简单的函数，除了调用 New 创建 Engine 对象以外，只对它调用了 Use 来导入了两个中间件函数，Logger 和 Recovery 的返回值。Logger 的返回值是用来记录日志的函数，Recovery 的返回值是用来处理 panic 的函数，这里我们先带过，后面再来看这两个函数。\nEngine 虽然内嵌了 RouterGroup，但是它本身也实现了 Use 方法，不过只是对 RouterGroup 的 Use 方法的调用，以及一些辅助操作。\n1 2 3 4 5 6 7 8 9 10 11 func (engine *Engine) Use(middleware ...HandlerFunc) IRoutes { engine.RouterGroup.Use(middleware...) engine.rebuild404Handlers() engine.rebuild405Handlers() return engine } func (group *RouterGroup) Use(middleware ...HandlerFunc) IRoutes { group.Handlers = append(group.Handlers, middleware...) return group.returnObj() } 可以看到 RouterGroup 的 Use 方法也很简单，就是将参数的中间件处理函数通过 append 加入到了自己的 Handlers 中。\n开始运行 在小例子中，最后一步是调用了 Engine 的 Run 方法，无参数。调用以后整个框架开始运行，使用浏览器访问注册好的地址就可以正确触发回调了。\n1 2 3 4 5 6 7 func (engine *Engine) Run(addr ...string) (err error) { // ... 省略一些代码 address := resolveAddress(addr) // 解析地址，默认地址是 0.0.0.0:8080 debugPrint(\u0026#34;Listening and serving HTTP on %s\\n\u0026#34;, address) err = http.ListenAndServe(address, engine.Handler()) return } Run 只做了两件事情，解析地址和开启服务。这里的地址其实只需要传一个字符串，但是为了实现可传可不传，所以用的参数是一个变参。在 resolveAddress 中处理了 addr 的不同情况下的结果。\n开启服务使用的是标准库 net/http package 中的 ListenAndServe 方法，这个方法接受一个监听地址和一个 Handler 接口的变量。Handler 接口的定义很简单，只有一个 ServeHTTP 方法。\n1 2 3 4 5 6 7 8 func ListenAndServe(addr string, handler Handler) error { server := \u0026amp;Server{Addr: addr, Handler: handler} return server.ListenAndServe() } type Handler interface { ServeHTTP(ResponseWriter, *Request) } 因为 Engine 就实现了 ServeHTTP，这里会将 Engine 本身传给 ListenAndServe 方法，当监听的端口有新的连接时，ListenAndServe 会负责 accept 建立连接，并且在连接上有数据时，会调用 handler 的 ServeHTTP 方法进行处理。\n处理消息 Engine 的 ServeHTTP 是处理处理消息的回调函数，下面就来看一下它的内容。\n1 2 3 4 5 6 7 8 9 10 func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) { c := engine.pool.Get().(*Context) c.writermem.reset(w) c.Request = req c.reset() engine.handleHTTPRequest(c) engine.pool.Put(c) } 回调函数有两个参数，第一个是用于接收请求回复的 w，将回复的数据写入到 w 即可，另一个是持有本次请求的数据的 req，可以从 req 里读后续处理所需的所有数据。\nServeHTTP 做了四件事情，首先从 pool 池里拿取一个 Context，然后将 Context 与回调函数的参数绑定，接着使用 Context 作为参数调用 handleHTTPRequest 方法处理这个网络请求，最后将 Context 放回到池中。\n这里我们先只看 handleHTTPRequest 方法的核心部分。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func (engine *Engine) handleHTTPRequest(c *Context) { // ... 省略一些代码 t := engine.trees for i, tl := 0, len(t); i \u0026lt; tl; i++ { if t[i].method != httpMethod { continue } root := t[i].root // Find route in tree value := root.getValue(rPath, c.params, c.skippedNodes, unescape) // ... 省略一些代码 if value.handlers != nil { c.handlers = value.handlers c.fullPath = value.fullPath c.Next() c.writermem.WriteHeaderNow() return } // ... 省略一些代码 } // ... 省略一些代码 } 在 handleHTTPRequest 方法中最主要做的是两件事，首先根据请求的地址从基数树中拿取之前注册过的方法，这里会将 handlers 赋值给本次处理的 Context，然后调用 Context 的 Next 函数来执行 handlers 中的方法，最后在 Context 的 responseWriter 类型对象中写入本次请求的返回数据。\nContext 在处理 HTTP 请求的时候，所有的上下文相关数据都在 Context 变量中，作者也在 Context struct 的注释里写了 \u0026ldquo;Context is the most important part of gin\u0026rdquo;，足见它的重要性。\n在上面讲到 Engine 的 ServeHTTP 方法时，可以看到 Context 并不是直接创建的，而是通过 Engine 的 pool 变量的 Get 方法取得的，取出来以后先将其状态重置然后再使用，在使用完毕以后又被放回了 pool 中。\nEngine 的 pool 变量是一个 sync.Pool 类型的，此处只要知道它是 Go 官方提供的一个支持并发使用的对象池即可，可以通过它的 Get 方法从池中取出一个对象，也可以使用 Put 方法向池中投入一个对象。在池是空的时候如果使用 Get 方法，则它会通过自己的 New 方法来创建一个对象并将其返回。\n这个 New 方法正是在 Engine 的 New 方法中定义的，再来看一眼 Engine 的 New 方法。\n1 2 3 4 5 6 7 func New() *Engine { // ... 省略其它代码 engine.pool.New = func() any { return engine.allocateContext() } return engine } 从代码中可以看到 Context 的创建方法是 Engine 的 allocateContext 方法。在 allocateContext 方法中并没有什么玄机，只是做了两步切片长度的预分配，然后创建对象并返回。\n1 2 3 4 5 func (engine *Engine) allocateContext() *Context { v := make(Params, 0, engine.maxParams) skippedNodes := make([]skippedNode, 0, engine.maxSections) return \u0026amp;Context{engine: engine, params: \u0026amp;v, skippedNodes: \u0026amp;skippedNodes} } 上文提到的 Context 的 Next 方法会执行 handlers 中的所有方法，来看一下它的实现。\n1 2 3 4 5 6 7 func (c *Context) Next() { c.index++ for c.index \u0026lt; int8(len(c.handlers)) { c.handlers[c.index](c) c.index++ } } 虽然 handlers 是一个切片，但是 Next 方法并未简单实现成一个对 handlers 的遍历，而是引入了一个处理进度的记录 index，它被初始化为 0，在方法一开始的时候执行累加，在一个方法执行结束以后再次执行累加。\nNext 被设计成这样跟它的用法有很大关系，主要是为了跟一些中间件函数配合。比如当某个 handler 执行的时候触发了 panic，在中间件中可以使用 recover 接住错误，然后再次调用 Next，即可继续执行后面的 handler，不会因为一个 handler 的问题影响到整个 handlers 数组。\n处理 panic 在 Gin 中，如果某个请求的处理函数触发了 panic，整个框架并不会直接 crash，而是抛出一条错误信息，然后继续提供服务。有点像 Lua 框架一般会使用 xpcall 来执行消息的处理函数一样。这个操作就是官方文档提到的功能点 Crash-free 了。\n上文提到，当使用 gin.Default 创建一个 Engine 的时候，会执行 Engine 的 Use 方法导入两个函数，其中的一个是 Recovery 函数的返回值，它又是对其它函数的封装，最后调用到的函数是 CustomRecoveryWithWriter，来看一下这个函数的实现。\n1 2 3 4 5 6 7 8 9 10 11 func CustomRecoveryWithWriter(out io.Writer, handle RecoveryFunc) HandlerFunc { // ... 省略其它代码 return func(c *Context) { defer func() { if err := recover(); err != nil { // ... 错误处理代码 } }() c.Next() // 执行下一个 handler } } 这里不关注错误处理的细节，只看它做的事情。这个函数返回了一个匿名函数，在匿名函数中使用 defer 注册了另一个匿名函数，在这个匿名函数中使用了 recover 接住了 panic，然后进行错误处理。在处理结束以后，调用了 Context 的 Next 方法，使原先正在依次执行的 Context 的 handlers 可以继续执行下去。\n","date":"2022-06-18T11:18:17+08:00","permalink":"https://wmf.im/p/gin-%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"Gin 框架源码阅读笔记"},{"content":"　似乎拿一个 C + Lua 实现的框架跟一门语言来比较不太恰当，但是由于 golang 自带了一套并发机制，所以比较一下两者并发机制的实现还是可以的。\n并发模型 Actor 模型 Actor 模型推崇的是 “一切皆为 Actor” ，每个 Actor 都是独立的个体，拥有一个属于自己的 mailbox 用来接收消息。它们可以接收消息，处理消息，发送消息，但是两个 Actor 之间的数据是完全独立的，无法直接通过内存共享。\nskynet 使用的就是 Actor 模型，每个服务是一个 Actor，服务的消息队列对应了 Actor 的 mailbox，服务之间的交互全都通过发送消息来处理。\nCSP 模型 CSP 全称 Communicating sequential processes，译名为通信顺序进程。但是这个译名很少有人用，还是叫 CSP 的人居多。不过从它的全称可以看出它也是一个基于通信进行通信的并发模型。\nGolang 使用的并发模型就是 CSP 模型，使用多个 goroutine 并行执行任务，在 goroutine 之间使用 channel 进行通信。\n区别 Actor 和 CSP 有很多相似之处，最大的共同点是都是使用消息来通信，Go 语言的作者之一也说过一句很有名的话，“不要通过共享内存来通信，应该通过通信来共享内存。”\n虽然都是通过通信来共享内存，但是还是有一些不同。Actor 模式是直接面向 Actor 通信的，在发送消息的时候清楚的知道接收方是谁。CSP 模式是面向通道通信的，在发送消息的时候只针对通道，至于通道的接收方是谁并不不需要关注。\n线程模型 用户线程和内核线程 内核线程是实现在内核态的线程，也就是内核原生提供的线程支持。多个内核线程的指令可以使用多个 CPU 核心来并行执行。但是它的缺点是创建、切换和销毁的操作消耗都比较大。\n用户线程就是实现在用户态的线程，它对内核是透明的，主要对业务层负责。由于实现在用户态，它的策略比较容易做定制修改，同时它的创建、切换和销毁相比内核线程都要小很多。但是它只是一个逻辑概念上的线程，并不能真正执行指令，在执行指令的时候需要绑定到内核线程上才行，这个绑定的规则又有很多种实现。\n1 : 1 使用一个用户线程对应一个内核线程，或者是直接使用内核线程。这种方式的好处是简单直观，没什么理解成本，使用常规的多线程开发思路保证线程安全即可。\n缺点是没有缓解系统线程的使用成本问题，如果使用了大量内核线程的话，上下文切换的成本问题会比较严重。\nM : 1 使用 M 个用户线程绑定到一个内核线程最常见的实现就是协程了。这种实现虽然有多个逻辑线程，但是不会存在真正的并行执行。比如在 Lua 中可以有很多个协程，但是正在执行的永远只有一个，协程会在需要时主动或者被动交出控制权。\n这种模式只是方便了逻辑开发，实际只使用了一个内核线程，所以在开发中不需要考虑线程安全的问题。但是这就导致它无法真正完全用到多核心 CPU 的处理能力，亦无法做到真正的并行执行。\nM : N 使用 M 的用户线程动态绑定到 N 个内核线程是最近几年的流行做法，既兼顾的逻辑开发的便利性，又可以完全使用到 CPU 的多核心处理能力，实现真正的并行执行。\n缺点是高效率的 M : N 调度器比较难实现。在使用这种模式进行开发的时候，因为存在真并行的情况，也需要考虑线程安全的问题。\n实现原理 skynet 的并发模型原理 skynet 是 C 和 Lua 混合实现的，在 C 语言层面使用的是 POSIX 线程，在 Lua 层使用协程来进行消息处理。所以 skynet 采用的是 1 : 1 和 M : 1 混合的线程模型。\n在 skynet 中有四种线程，分别是 monitor 线程、timer 线程、socket 线程和 worker 线程，这四种线程都可以产生消息，但是只有 worker 线程负责处理消息。\n可以看到 skynet 中不管是任何线程产生的动作都是转化为消息的。当需要通知某个服务什么事情的时候，需要知道服务的名字或者地址，以此来找到服务。然后通过服务结构中保存的消息队列的指针找到它的消息队列，将消息加入到消息队列中。\n有一个全局队列中保存了所有非空的服务消息队列，当消息被加入到服务消息队列中时，会检查服务消息队列是否在全局队列中，如果不在的话要将它加入全局队列。\nworker 线程会定期被唤醒，当它被唤醒以后，会尝试从全局队列中拿出一个消息队列。如果可以拿到消息队列，就尝试从消息队列中依次取出待处理的消息，调用服务注册过的消息回调函数，对消息进行处理。\nGolang 的并发模型原理 Golang 的分层比较清晰，采用的是 GMP 模型。其中 G 代表了 Goroutine，M 代表了 Machine 即内核线程，P 代表了 Processor，可以理解为逻辑处理器。线程模型采用了 M : N 模型，每个 Goroutine 是一个用户线程，多个内核线程来负责用户线程的调度。\n每个 M 在任意时刻可以绑定一个任意的 P，P 有一个属于自己的本地队列，它会依次执行其中的 G，当执行队列中没有 G 时，它会尝试去全局队列中取一批 G 放进自己的本地队列。\n调度机制中实现了 工作窃取 ，当 P 尝试取全局队列中取 G 时，如果没有取到，它会尝试将其它 P 的本地队列中一半的 G 拿到自己的本地队列中。\n同时在调度机制中也实现了任务抢占，使用一个特殊线程来监控运行状态，如果单个 Goroutine 单次运行如果超过了一定的时间就会被抢占，抢占的实现提高了平均响应时间。\n区别 在处理的单位上二者不太一样，skynet 以每条消息作为处理单位，每条消息处理一次即完成。golang 以 goroutine 为处理单位，goroutine 更接近于线程的概念，当它尚未执行完毕时会长期存在于进程中。\nskynet 的并发模型的层级更加简单，没有维护一个 Processor 的概念，直接为内核线程分配要处理的消息。golang 中因为存在了 Processor，它有自己的本地队列，这样的设计弱化了全局队列的存在感，减少了部分锁竞争。\ngolang 实现了工作窃取，不过因为 skynet 中没有 Processor 的概念，也就没有了本地队列，所以它并不需要工作窃取，worker 线程每次都是从全局队列中取出消息队列的。\n任务抢占是 golang 的一个优势，skynet 也有一个监控线程，但是它并没有实现抢占功能，如果在处理一条消息的时候碰到了死循环的逻辑，那么会无限期占用一个线程，skynet 只能输出一条警告日志，需要开发者通过后台手动处理。\n","date":"2022-05-05T12:31:17+08:00","permalink":"https://wmf.im/p/skynet-%E4%B8%8E-golang-%E5%B9%B6%E5%8F%91%E6%9C%BA%E5%88%B6%E6%AF%94%E8%BE%83/","title":"skynet 与 golang 并发机制比较"},{"content":"　使用浮点数表示小数在各种开发中都非常常用，但是在游戏开发中有一些场合需要使用整数来模拟定点数代替浮点数表示小数，本篇会简单讲解这两种表示小数方法的区别和使用场景。\n浮点数 现行的浮点数通用运算标准是由 IEEE 委员会在上世纪八十年代制定的，被称为 IEEE 754 标准，它规定了浮点数需要使用 $v$ = $(-1)^s$ * $f$ * $2^e$ 的形式来表示一个小数：\ns 代表符号位（sign），当 s 为 1 时 v 是负数，s 为 0 时 v 是正数。 f 代表小数（fraction）部分。 e 代表指数（exponent）部分，它可以为负数。 一般在单精度浮点数（float）中 s 占 1 位，e 占 8 位，f 占 23 位，一共 32 位。在双精度浮点数（double）中 s 占 1 位，e 占 11 位，f 占 52 位，一共 64 位。\n虽然有了 IEEE 754 标准，但是各家在实现上还是有一些区别，尤其是舍入规则上。这导致了跨平台，尤其是跨 CPU 架构的情况下，执行同一个浮点数计算得到的结果可能不一样。相关内容可以参考这篇回答，Cross Platform Floating Point Consistency。\n由于大部分情况下的开发都不太需要浮点数满足跨平台一致性，所以一般功能的日常开发并无什么影响，但是游戏开发中如果使用了帧同步技术的话，微小的计算差异可能会被慢慢累积，最后变得使得多端的状态不一致。\n定点数 定点数是指小数点固定的数，一般是通过为整数约定一个固定的小数点位置来模拟小数。早期因为 CPU 的浮点计算能力比较差，定点数被广泛使用，比如经典游戏 DOOM 中就使用了定点数。\n由于现代 CPU 的浮点计算能力大幅提升，绝大多数情况下已经完全不需要使用定点数模拟小数了，跨平台小数计算一致性就是定点数为数不多的使用场景之一。\n定点数的实现其实不太难，一般在精度要求不高的情况下，使用 32 位整数模拟一个 16.16 的小数即可满足大部分需求，来看一下 DOOM 的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // m_fixed.h #define FRACBITS 16 #define FRACUNIT (1\u0026lt;\u0026lt;FRACBITS) #define MAXINT ((int)0x7fffffff) #define MININT ((int)0x80000000) typedef int fixed_t; fixed_t FixedMul\t(fixed_t a, fixed_t b); fixed_t FixedDiv\t(fixed_t a, fixed_t b); fixed_t FixedDiv2\t(fixed_t a, fixed_t b); // m_fixed.c fixed_t FixedMul (fixed_t a, fixed_t b){ return ((long long) a * (long long) b) \u0026gt;\u0026gt; FRACBITS; } fixed_t FixedDiv(fixed_t a, fixed_t b){ if ((abs(a) \u0026gt;\u0026gt; 14) \u0026gt;= abs(b)) return (a ^ b) \u0026lt; 0 ? MININT : MAXINT; return FixedDiv2(a, b); } fixed_t FixedDiv2(fixed_t a, fixed_t b){ double c; c = ((double)a) / ((double)b) * FRACUNIT; if (c \u0026gt;= 2147483648.0 || c \u0026lt; -2147483648.0) I_Error(\u0026#34;FixedDiv: divide by zero\u0026#34;); return (fixed_t)c; } m_fixed 提供了乘除法的接口，举一个例子来测试一下定点数和浮点数。\n1 2 3 fixed_t n = FixedDiv(16, 5); // n = 209715 printf(\u0026#34;fixed: %lf\\n\u0026#34;, n / 65536.0f); // fixed: 3.199997 printf(\u0026#34;float: %lf\\n\u0026#34;, 16 / 5.0f); // float: 3.200000 可以看到定点数的精度是不如浮点数的，并且使用同样大小的空间时，定点数可以表示的范围也远不及浮点数。\n定点数的最大的优势是自主实现，规则可控，这就意味着可以非常轻易的做到跨平台的一致性。在部分开发场景下，一致性远比高精度更加重要。\n","date":"2022-04-15T09:21:57+08:00","permalink":"https://wmf.im/p/%E6%B5%85%E8%B0%88%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E6%B5%AE%E7%82%B9%E6%95%B0%E4%B8%8E%E5%AE%9A%E7%82%B9%E6%95%B0/","title":"浅谈游戏开发中的浮点数与定点数"},{"content":"　由于在游戏服务器的架构中，大部分的进程都是有状态的，所以就非常依赖热更新。Lua 方便的热更新是其得以在手游后端开发中大量使用的重要原因，本篇来讲一下我了解过的 Lua 的一些代码加载和热更新方式。\n加载模块 dofile 使用 dofile 进行代码加载是最简单粗暴的，在进程启动的时候，直接将本进程所有要用到的脚本文件使用 dofile 加载进来。\n如果需要重新加载，那么就对修改过的文件再次执行 dofile 重新加载一次。但是这样加载有一个不好的地方，就是每个文件都要有一个全局变量的 Table，而且如果这个 Table 被别的文件里使用 local 引用了，那么即使重新 dofile 加载一次，原来的引用还是保存了原先那个 Table 的地址。\nloadfile 使用 loadfile 加载文件，会得到一个中间函数，这个中间函数执行完就跟直接调用 dofile 加载效果是一样的了。但是因为有了一个中间函数，就有一些操作的机会。\n可以使用 setfenv 将加载结果的函数放在一个新创建的 Table 中执行，这样既解决了需要全局变量的问题，也解决了别的地方引用的问题。给出一个简单的实现方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 local modules = {} function import(filename, reload) local filefunc = loadfile(filename) if not modules[filename] then local m = {} setmetatable(m, {__index = _G}) local wrapfunc = setfenv(filefunc, m) wrapfunc() modules[filename] = m return m else if reload then local m = modules[filename] local wrapfunc = setfenv(filefunc, m) wrapfunc() return m else return modules[filename] end end end local xxx = import(\u0026#34;xxx.lua\u0026#34;) 需要注意的是，因为 Lua 底层机制的改变，setfenv 在 Lua5.2 及以后就废除了，不过也可以通过 _ENV 模拟出来，Implementing setfenv in Lua 5.2, 5.3, and above。\nrequire require 是 Lua 官方提供的加载模块的接口，不仅可以加载 Lua 模块，也可以加载 C 模块。它加载过的模块会被放在 package.loaded 中，对同一个模块第二次加载，不会重复加载，会直接返回旧的模块给调用者。\n需要重新加载的时候需要首先将 package.loaded 中对应模块设为 nil，然后再次执行 require。但是这样也有一个问题，那就是如果在某个地方使用 local 变量引用了 require 的结果，那么更新以后它引用的还是旧的模块。\n热更新 重新加载模块并不是完整的热更新，热更新需要在重新加载模块的时候做很多处理，对各个类型的变量进行有选择性的保留或是覆盖。下面来说一下各个种类的变量要怎么处理。\n外部变量 外部变量包括了模块中的全局变量和 return 出来的局部变量。它们比较容易处理，因为外部可以直接读取到，区分它们处理方式的是它们有没有被外部引用。\n如果不需要处理它们的外部引用，那么一般将整个文件重新加载即可。重新加载以后文件中的变量都会被覆盖掉，如果需要保留部分旧值的话，可以在重新加载之前在外部获取并且保存这些旧值，然后在重新加载之后，再次恢复这些旧值即可。\n如果需要处理它们的外部引用，那么就需要使用热更新脚本，在原地址上进行修改。\n局部变量 局部变量如果不需要保留的话，那就不用额外处理了，局部变量在外部都是以函数的 upvalue 形式存在的，重新加载以后函数会使用新的 upvalue，跟以前的值没关系。如果要保留以前值的话，那就要将旧的 upvalue 赋值给新的 upvalue。\nupvalue 的处理大概是热更新中最麻烦的部分，这部分因为要处理的问题跟实际情况的关联很强，所以除非在开发过程中就有一些强制规范约束，否则基本上不太可能实现类似一键更新这种操作。对于这种情况一般采用的方法需要每次为热更编写一个脚本，然后将需要修改的内容全部收集起来，使用 load 加载热更脚本，收集的内容作为 loadfile 的 env 参数传入。\n处理 upvalue 的时候需要使用到一些 debug 库的方法。\ndebug.getinfo 用来获得函数信息，可以通过使用 debug.getinfo(func, \u0026ldquo;u\u0026rdquo;) 获得函数信息中与 upvalue 相关的部分。 debug.getupvalue 可以获得指定函数指定序号的 upvalue 的 name 和 value。 debug.setupvalue 可以增加函数的 upvalue。 debug.upvalueid 可以得到指定函数指定序号的 upvalue 的一个唯一标识符，可以用来判断两个函数引用的 upvalue 是否是同一个值。 debug.upvaluejoin 让一个闭包中的某个 upvalue 引用另一个闭包的某个 upvalue。 要实现 upvalue 的保留关键就是通过上述的 api 拿到想要保留的旧函数 upvalue 的值，然后再赋值给新函数的 upvalue。\n收集 upvalue 的方法可以参考 skynet 中 inject 的实现稍作修改，将 number 和 string 类型的 upvalue 也加入进来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 local function getupvaluetable(u, func, unique) local i = 1 while true do local name, value = debug.getupvalue(func, i) if name == nil then return end local t = type(value) if t == \u0026#34;table\u0026#34; or t == \u0026#34;number\u0026#34; or t == \u0026#34;string\u0026#34; then u[name] = value elseif t == \u0026#34;function\u0026#34; then if not unique[value] then unique[value] = true getupvaluetable(u, value, unique) end end i = i + 1 end end 收集到以后的修改就非常自由了，既可以把模块整个重新加载一遍，然后将保存的 upvalue 还原，也可以在原来模块的基础上直接修改函数。一般为了避免其它地方引用的问题，所以还是在原地址上直接修改函数的比较多。\n1 2 3 4 5 6 7 8 9 10 11 -- m.lua local m = {} local a = 1 function m.add() a = a + 1 print(\u0026#34;a\u0026#34;, a) end return m 来看一个简单的例子，在 m.add 被执行几次以后，需要进行热更修改，将 a 每次增加的值改为 2，并且需要延续之前 a 的值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 -- hotfix.lua local a = u.a m.add = function() a = a + 2 print(\u0026#34;a\u0026#34;, a) end -- main.lua local m = require \u0026#34;m\u0026#34; m.add() -- a = 2 m.add() -- a = 3 m.add() -- a = 4 local u = {} local unique = {} getupvaluetable(u, m.add, unique) local env = setmetatable({u = u, m = m}, {__index = _ENV}) loadfile(\u0026#34;./hotfix.lua\u0026#34;, \u0026#34;bt\u0026#34;, env)() m.add() -- a = 6 m.add() -- a = 8 可以看到，热更脚本从环境中拿到传进来的 upvalue 直接赋值即可，旧的 m.add 函数和其 upvalue a 都被换成了新的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 local m = {} local a = 1 local step = 10 function m.add() a = a + 1 end function m.set(s) step = s end return m 再来看一个稍微复杂一点的情况，这次再加一个函数，目标是将 add 改成 a = a + step，并且 step 变成两个函数的公共 upvalue，也就是说再次调用 m.set 修改 step 以后，m.add 可以直接使用新的值，而不需要再次热更。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 -- hotfix.lua local a = u.a local step = u.step m.add = function() a = a + step end debug.upvaluejoin(m.add, 2, m.set, 1) -- main.lua local m = require \u0026#34;m\u0026#34; m.add() -- a = 2 m.add() -- a = 3 m.add() -- a = 4 local u = {} local unique = {} getupvaluetable(u, m.add, unique) getupvaluetable(u, m.set, unique) local env = setmetatable({u = u, m = m}, {__index = _ENV}) loadfile(\u0026#34;./hotfix.lua\u0026#34;, \u0026#34;bt\u0026#34;, env)() m.add() -- a = 14 m.set(100) m.add() -- a = 114 这里最关键的一步是使用了 debug.upvaluejoin，将 m.add 的第二个 upvalue step 与 m.set 的第一个 upvalue step 进行了关联，可以看到热更过以后，再次调用 m.add 此时累加的步长已经变成了 step 的当前值 10，调用 m.set 将 step 设为 100 以后，再次调用 m.add，累加的步长变成了 100。\n旧的对象 Lua 中的面向对象是使用元表的 __index 方法模拟出来的，创建出来的对象只有修改过的数据和函数是自己的，未修改过的数据和函数都是使用的类定义的提供的。\n得益于这种实现方法，Lua 中热更新一个类的成员函数基本上不需要做什么额外处理，直接在原函数的位置上修改即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 -- c.lua local c = { count = 1 } function c:new() return setmetatable({}, {__index = self}) end function c:add() self.count = self.count + 1 end return c -- hotfix.lua c.add = function(self) self.count = self.count + 10 end -- main.lua local c = require \u0026#34;c\u0026#34; local o1 = c:new() o1:add() -- o1.count = 2 loadfile(\u0026#34;./hotfix.lua\u0026#34;, \u0026#34;bt\u0026#34;, setmetatable({c = c}, {__index = _ENV}))() o1:add() -- o1.count = 12 local o2 = c:new() o2:add() -- o1.count == 11 可以看到在热更过以后，不管是热更之前创建的旧对象还是热更以后创建的新对象，执行的都是更新过的逻辑。\n","date":"2022-04-10T22:56:27+08:00","permalink":"https://wmf.im/p/lua-%E7%9A%84%E4%BB%A3%E7%A0%81%E5%8A%A0%E8%BD%BD%E5%92%8C%E7%83%AD%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F/","title":"Lua 的代码加载和热更新方式"},{"content":"　游戏开发中如何进行高效稳定的数据存取，一直是非常重要的问题。本篇中会尝试分析一下常见的存储方案。\n数据库选择 如果是早几年的话，这基本上是个不需要讨论的问题，因为 MySQL 是当时绝对的主流选择，但是最近几年这种情况发生了一些变化，让数据库选择有了更多的方案。\n直接读写文件 使用直接读写文件这种方式来保存游戏内数据的公司应该很少很少了，可能只有一些还抱着祖传代码不想撒手的公司还在用。\n不过大部分公司在保存日志或者是战斗录像这种单个文件内容比较大，并且不会修改的数据的时候都还会使用这种方法，尤其是对时间久的一些日志，基本上不会查询，只做留档使用了。\n采用这种方式的实现，一般是将每种类型数据中每一个 key 作为文件名，数据作为文件内容来存储。每种数据可以使用一到多个文件夹来管理。创建和更新数据的时候一般使用某种序列化方式将内存中需要保存的数据打包，然后写入到适当的目录下。读取数据的时候通过数据类型和 key 找到对应目录下的文件，通过文件 I/O 将其读入内存中，再反序列化得到数据。\nMySQL 凭借开源且免费的特点，MySQL 逐渐变成了关系型数据库的代表。在 NoSQL 数据库兴起之前，MySQL 基本上是有统治级别的使用率。\nMySQL 虽强，但是这几年它增加了一批非常有竞争力的对手，也令他的统治地位有点一些动摇。不过 MySQL 目前也依然活跃在相当大比例的游戏公司中，毕竟大部分公司对 MySQL 都有丰富的使用经验，很多坑也都踩过填过了，选用一个成熟可靠的数据库也是很正常的事情。\n使用 MySQL 存储数据的话，有两种用法，第一是完全按照关系型数据库那种每张表分字段的方法。还有一种是当作 key - value 数据库来用，每张表只有两个字段，key 和 value，会把本张表中要保存的所有字段都使用一种方式序列化成一个值，存入一个 blob 类型的 value 中，不过这种方法在数据更新的时候没办法单独更新字段，只能再次将 value 完整序列化替换掉旧值。\nRedis Redis 应该是后起之秀里面攻势最猛的了，因为它的性能优势，使它深受对实时性要求很高的游戏行业青睐。\n它同样开源免费，而且因为全部数据都在内存中，所以它无论是读取还是写入，都对传统的磁盘数据库有碾压的表现。当然缺点也很明显，毕竟相同单位大小的内存比硬盘可是贵的多了，而且系统和硬件支持内存的容量上限也远远小于硬盘。同时数据在内存里也并不安全，如果需要开持久化的话，它的性能会有一些下降。\n在 Redis 中保存数据一般采用 \u0026ldquo;数据类型 + uid\u0026rdquo; 作为 key，每个数据是一个哈希表，其中保存了它的所有字段。同时 Redis 也可以非常方便的保存一些全局数据。\nMongoDB 如果说 Redis 是 NoSQL 数据库中发展最好的内存数据库，那么 MongoDB 就是 NoSQL 数据库中发展最好的磁盘数据库了，在游戏公司中使用的更多。\n模式自由（Schema-free）的数据库对游戏公司的业务是非常合适的。不再需要做业务的时候经常改表的字段了。最重要的是，游戏公司的业务基本上是不需要使用关系型数据库提供的复杂特性，也不需要使用 SQL 进行复杂的查询，这使得关系型数据库的优势难以发挥。\n不过它也有很多为人诟病的地方，比如相对一些成熟的老牌数据库来说稳定性，性能，内存占用等方面都有一些劣势，但是这些年随着版本不断迭代，各方面进步都很大，已经长期霸占 DB Engine 的第五名了。\nMongdoDB 也是 key - value 数据库，不过它在外层有一个集合的概念，基本上相当于关系型数据库中的表了，使用的时候将不同功能的数据放在不同的集合中即可。\n存储方案 一个完整开发的较大规模的游戏项目，基本上不会只是用一种存储数据的方式。因为游戏中对各种数据的使用和保存方式都有很大区别，这导致了很难有一种完全的方法可以搞定所有数据的存储。所以一般都是采用多种方式混合的数据存储方案。\n加载数据 数据被加载大概有两种处理方式，启服时全量加载不释放，按需加载使用完释放。\n适合全量加载不释放的数据必须规模不是太大，且要使用非常频繁，这样才有常驻内存中的价值。一般都是用来保存进程的全局数据。\n按需加载使用完释放，一般用来保存跟个体有关的数据，这部分数据当个体没有操作的时候，大部分情况下是完全用不到的。这种形式可以再加上一层缓存层来加快热点数据加载，这个缓存层如果需求比较简单可以自己实现，如果需要一些高级功能，可以使用 Redis 来做。\n写入数据 写入数据的方式大概有四种，每次修改直接落地，定时全量落地队列，定时脏数据落地队列，数据离线全量落地。\n每次修改直接落地的代价比较大，要严格控制这类操作的使用范围，一般只有高优先级的操作引起的数据变更会考虑使用每次修改直接落地的操作。\n定时全量落地队列，是指在某个对象的数据发生变更以后，将其加入到一个变更对象列表中，然后每隔一段时间，将列表中对象的所有数据进行一次落地。\n定时脏数据落地队列，是指在对象的某个数据发生变更以后，将对象和修改的字段加入到变更对象列表中，然后每隔一段时间，将列表中的变更对象被修改字段的数据落地。\n数据离线全量落地，是某个对象要从内存中删除这种情况，在删除之前会对这个对象进行一次全量的保存，一般作为别的方案的保底。\n数据分类 选择存储方案需要根据实际需求来考量，数据的不同使用场景和范围，都对应了不同的存储方案。\n玩家数据 游戏中玩家数据的总量是很大的，即使是日活月活都不太高的游戏，它的注册用户可能也是很惊人的一个量。这就导致了玩家数据在起服时全部加载到内存中变成不现实的事情。\n玩家的数据大概有两类组成，第一类是别的系统可能会在玩家离线的时候读取的部分，可能有的是玩家的名字，uid，签名，等级，所属工会等基本信息。另一类是别的系统不会读取，只有玩家自己登录以后才会使用的数据，比如背包中的物品数量，经验值，体力值这些。在开发中有时也会称第一类为对外数据，第二类为对内数据。\n对内数据和对外数据因为使用场景不同，一般会分表来保存，不放在一起，方便读取，加载策略也有一些区别。对内数据只有在玩家登录的时候才会从硬盘上加载进游戏服的内存中，玩家下线以后就可以释放掉这部分内存了，一般不会缓存这部分数据，而是可能会做一个延迟删除对象的设定，来应对玩家可能的立马再次上线。对外数据经常要被其它系统读取，比如好友之类的系统，一些高等级玩家的数据可能会经常被访问，所以一般会使用带缓存的按需加载。\n玩家数据的变更是非常频繁的，一个在线玩家即使是完全不做任何操作，可能也会有一些定时器会修改他的数据。如果每次的修改都落地到硬盘显然是不现实的。一般采用定时脏数据落地队列的方案进行数据更新，有时也会再加上数据离线全量落地作为保底。\n全局数据 游戏中有一些全局数据，这些数据一般单个服务器只有一份，数据量不会太大，但是访问频率很高。像是当前开放的活动中的数据，以及一些功能的全服状态。\n这种全局数据直接在启动时全部加载进内存即可，整个功能的开放周期中都存在于内存里，数据保存方式可以按需选择，一般也用定时脏数据落地队列即可。\n功能数据 很多功能性系统的数据是会脱离玩家数据单独保存的，比如像公会，聊天这种，这部分数据相对比较独立，一般只有自己系统会用。\n这些系统的总数据量可能会非常大，尤其是像需要保存全部记录的聊天系统，会越来越大。这些数据如果也是启动时全部加载是不太现实的，一般使用按需加载使用完释放，并且要加上缓存系统，因为这些数据会有明显的优先级，比如高排名的公会信息可能经常被查看，同样在一个会话中最新的聊天记录也会被经常查看。\n日志数据 日志类数据不仅包含了日志，还有类似战斗录像这种数据。如果说其它数据只是比较大的话，那日志类数据就真的是巨大了，远超其它数据很多个数量级，这就导致了它需要一些特殊的手段来处理。\n一般处理日志类数据有两种方法，存数据库，或者是把每小时的日志存进一个独立文件里。存数据库可以方便直接进行查询，不过用这种方法的公司比较少，因为日志数据实在是太多了。存成文件的好处是，可以直接通过目录查看，清晰直观，也不需要生产环境的数据库权限就可以查看这些文件，如果需要进行分析，可以使用一些日志分析工具进行后期分析。\n有一些项目要求只要保存一定时间以内的日志即可，比如 30 天，这种就比较简单，每天把最后一天的日志删除即可，总量一直在一个可以接受的范围内。但是有一些项目是需要永久保存日志的，这种需求就需要将非临近日期的日志进行二次处理，一般是将其打包压缩，然后可以选择一种更加便宜的存储介质来保存，比如说磁带。\n","date":"2022-04-08T22:38:27+08:00","permalink":"https://wmf.im/p/%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/","title":"游戏开发中的存储系统设计"},{"content":"　游戏开发中，只要有地图，基本上就避免不了要实现寻路。哪怕没有给玩家提供寻路功能，游戏中的 AI 实现也需要借助寻路来移动。\n地图建模 不管是什么形式的地图，拿到它的第一步应该是设法为其建模，一份无法建模的地图数据是不能进行计算的，建模的目的是为了得到一个方便寻路算法使用的模型。\n常用的寻路建模方式有划分格子（Grid），放置路点（WayPoint），生成导航网格（NavMesh）这三种。\n划分格子 将地图分割成紧密相邻的若干个大小形状完全相同的正多边形，一般会使用是正方形，偶尔会用正六边形，基本没有别的形状的实现。\n一般会以地图上的对象可以通行的最小宽度为单位来划分格子，使用二维数组即可方便保存和读取建模后的数据。使用不同的数字可以标识出地图格子不同的状态，比如可以简单的将 1 作为可以行走的格子，0 作为不可行走的格子。\n1 2 3 4 5 6 7 8 9 10 11 local map = { {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1}, {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1}, {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1}, {1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1}, {1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1}, {1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1}, {1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1}, {1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1}, {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1}, } 划分格子一般用在二维地图上，它的优点是实现比较简单，最直观，也不需要额外的工具辅助修改。缺点是内存开销会比较大，内存占用是随着地图大小线性增长的。\n放置路点 路点顾名思义就是路上的点，在地图上根据需求在所有需要寻路的区域放置路点。路点一般是需要人工选择位置放置的，不过不用担心，一般会有策划同学负责搞这个，233。\n路点的总集合在 Lua 中一般可以使用一个 Table 来记录，每个路点包含了两份数据，自己的位置和从自己出发可以去的点，可达点还需要记录到它的路径开销，这个路径开销并不一定是简单的坐标距离计算，可以是考虑了地图点优先级的开销。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 local waypoint = { a = { {1, 1}, {{\u0026#34;b\u0026#34;, 10}} }, b = { {3, 5}, {{\u0026#34;a\u0026#34;, 10}, {\u0026#34;c\u0026#34;, 5}} }, c = { {12, 20}, {{\u0026#34;b\u0026#34;, 5}} } } 路点寻路的缺点是点需要手工配置，经常会有一些点因为配的不太好，比如连通性配置有问题之类的，导致寻路的效果很奇怪，比如会莫名其妙绕路。而且因为对象一开始大概率不会正好在一个路点上，所以还要解决怎么从当前位置去到离得最近的路点这个问题。一般如果地图的路点不多的话，可以遍历路点，找到一个离自己近且中间无阻挡的路点作为起始点，直线跑过去以后开始后续的寻路。\n路点的优点是相比划分格子，因为路点的数量会少得多，所以它占用的内存会少，而且因为点少，所以它在后续配合算法寻路时效率也更高。还有一个优点是有时候策划想要对象在寻路时按照他配置的线路跑，比如在城市中寻路，有的策划喜欢让角色在路上跑时可以跑在路中间，而不是沿着建筑物边缘跑最更近的路程。\n导航网格 导航网格是这几种里面最复杂的，使用了非常多的前端技术来处理地图。有一个非常广泛使用的开源库 recastnavigation，基本上是业界标杆水平。\n导航网格的核心是将地图分为凸多边形网格（Poly Mesh），之所以这样做是因为凸多边形有一个特性，在它内部包括边缘任意取两个点连线都不出超出它本身的范围。Poly 是不带高度的，每个 Poly 会进一步划分成若干个三角形，每个三角形可以保证是一个平直面。划分的三角形可以保存下来用于寻路。整个处理过程有一篇文章写的很好，游戏的寻路导航 1：导航网格。\n导航网格的优点是它可以很好的用在 3D 地图上。缺点是概念比较难以理解，这导致难以做一些需求定制，同时处理很大的地图时生成结果的过程非常慢。\n寻路算法 虽然标题是寻路算法，但是现在才真正开始讲寻路算法。\n寻路算法是需要基于上述几种寻路数据建模的结果来计算的。理论上任何一种寻路算法都可以跟上面的任何一种建模方式配合使用。\n一些概念 为了更好的了解寻路算法，首先要简单说几个概念。\n曼哈顿距离，是一种在大城市估算城市距离的方法，它假设开始点和结束点直接都是高楼大厦，无法对角线移动。它的计算很简单，在 2D 坐标轴中两个点的曼哈顿距离就是它们在 x 轴上的距离 + 它们在 y 轴上的距离。\n欧几里得距离，就是两个点之间的真实距离，它不做任何移动方向上的限制，直接使用勾股定理计算两个点的直线距离。\n启发式函数，用来进行一些寻路中需要参考的值的预估，在算法中使用不同的启发式函数，可以得到不同的寻路结果。\n开放集合（OpenList），保存的是目前需要关注的点，一般是当前寻路点周围的点。在寻路计算中经常需要从开放集合中取出经过启发式函数计算的估值最小的点，所以开放集合一般使用有序的数据结构实现。\n封闭集合（CloseList），保存的是所有已经被启发式函数计算过估值的点，当一个点在寻路计算中被处理完毕以后，它会被放在这个集合中。因为经常需要判断一个点是否在封闭集合中，所以它一般使用哈希表之类的数据结构实现。\n贪心算法 贪心两个字是指算法不做长期的的规划，而仅仅着眼于眼前。它每次选择下一步的点时，都直接选择待选点中通过启发式函数计算开销最低的点。如果在没有任何阻挡的情况下，那么这样是路径最短的，但是如果有了阻挡以后，这样做往往可能会选择出一条非最优路线。\n贪心算法的步骤如下：\n首先将起点设为当前点，并且将它加入封闭集合中。 依次处理当前点的所有可达点中不在封闭集合里的点，将其前置点设为当前点，然后检查它是否在开放集合中，如果不在则计算其预估开销，并且将其加入开放集合中。 检查开放集合，如果为空，则地图搜索已经结束了，跳至步骤 5。 如果开放集合不为空，则取出其中预估开销最小一个点设为当前点，比较它的位置是否与目标终点一致，如果一致，则搜索结束，否则将它加入到封闭集合中，重复步骤 2。 当搜索结束时，如果当前点的位置不是目标终点，那么寻路失败。否则从当前点也就是终点开始一直找前置点，这条线路的逆序就是寻路的结果。 接下来看一个贪心算法的搜索步骤例子，例子使用曼哈顿距离作为期望开销，在图中使用中间有点的格子代表了该格子已经被加入到了封闭列表中，有路径数值的格子代表了当前在开放列表中的格子。\n可以看到贪心算法导致了从起点开始没有直接向下，而是一路向右，直到碰到了阻挡又绕了一圈过去，最终找到了目标点，但是没有得到最佳的路线。\nA* 算法 A* 算法应该是使用最广泛的寻路算法了，它兼顾了性能与路径质量，同时实现也比较简单，只比贪心复杂了一点点。\nA* 算法的步骤如下：\n首先将起点设为当前点，并且将它加入封闭集合中。 依次处理当前点的所有可达点中不在封闭集合里的点，检查其是否在开放集合中，如果不在则将其前置点设为当前点，计算其总开销，并且加入开放集合中。如果它本来就在开放集合中，则以当前点作为它的前置点再计算一次它的总开销，如果比之前的少，则将它的前置点改为当前点并且更新它的总开销。 检查开放集合，如果为空，则地图搜索已经结束了，跳至步骤 5。 如果开放集合不为空，则取出其中总开销最小的一个点设为当前点，比较它的位置是否与目标终点一致，如果一致，则搜索结束，否则将它加入到封闭集合中，重复步骤 2。 当搜索结束时，如果当前点的位置不是目标终点，那么寻路失败。否则从当前点也就是终点开始一直找前置点，这条线路的逆序就是寻路的结果。 可以看到，A* 算法与贪心算法的区别都在步骤 2 里，主要有两点，首先 A* 算法中是使用的两个路径和来计算的，其次 A* 算法在处理已经在开放集合中的可达点时，会比较它当前的总开销与以自己为前置点的总开销，当以自己为前置点的总开销小的时候才会更新可达点的前置点。\n接下来使用 A* 算法处理刚刚贪心算法处理过的那个问题，图中的 h 为预估开销，g 为前置路径开销，f 为总开销，也就是 h 和 g 的总和。当计算当前点的可达点时，会计算出以自己为前置点的开销，列出 h 和 g，不是当前点的可达点的点只列出它目前的开销和，方便比较。\n可以看到 A* 寻发在一开始一样是一路向右边走，遇到阻挡以后，此时开放集合中有六个点，在第三张图里分成了三列，每一列上下两个点的父节点都是它们中间那个点，这时六个点的总开销都是 6。\n因为开放集合里的点路径相等，所以随机取一个点作为当前点，最左边图中假设取到了右上角的点。可以看到它只有一个相邻点可以走并且不在封闭集合中，但是它从本点走过去的路径和是 8，大于其原来的 6，所以不做更新操作。中间的图假设取到了下面的点，也是一样的情况。现在开放集合中还剩下四个点，并且路径和都为 6。\n这三张图处理的是开放集合中，处于中间列的两个点的情况，跟上面的情况一样，也找不到更短的路径，不做任何更新，只把自己加入到封闭集合中。\n继续看后续的操作，这三种图中，第一张图处理第一个点，它周围已经没有任何不在封闭集合中的点了，无法进行操作，直接把自己放入封闭集合中。然后处理第一列下面的那个点，将它更下面的点加入到开发集合中。\n接下来的事情就顺理成章了，开放集合中只有一个点，一直走下去就可以了。当前节点位置等于目标点时，说明寻路结束，反转链表即可得到寻路线路。可以看到 A* 最终搜索到了最优路线。\nA* 算法被提出已经有五十多年了，至今已经有了诸多针对它的优化算法，但是它依然是使用最广泛的寻路算法。这主要归功于它的适用场景十分广泛，它的改进型都或多或少增加了一些场景限制，如果覆盖不到自己的适用场景，那效率再高也是镜花水月。\n关于 A* 算法有两篇非常好的文章，Introduction to the A* Algorithm 讲述了 A* 算法的概念。Implementation of A* 给出了 A* 算法的几种实现。\nJPS 算法 JPS（Jump Point Search）算法，有时候也翻译成跳点算法。它是 A* 的一种改进方法，实现了效率的大幅度提升。相较于它的老前辈们，JPS 算法的历史非常短，提出至今不过十年左右的时间，但是因为它出色的效率，现在已经基本上成为了 A* 改进型的标杆算法了。\n但是因为算法特性问题，它的使用场景比较受限，它只支持规则格子地图上的可用和阻挡两种情况，无法处理更复杂的地形，比如地图上某些地块有更高的优先级这种情况，而且它需要两个点之间可以对角线行走。\nJPS 进行的优化在于，它根据当前地图上的状况，把一些点给排除了，不需要加入到开放集合中。开放集合中的点少了，需要计算的次数自然就少了。而可以加入到开放集合中的点被称为跳点，这也就是跳点算法名字的由来。\n以上两个路径图使用 A Visual Explanation of Jump Point Search 页面中的程序实现，图中灰色的点是已经在封闭集合中的，蓝色的点是在开放集合中的。这篇文章不仅有可自定义的寻路小程序，还对 JPS 算法有非常详细的解释，强烈推荐阅读。 在这个例子中可以看到，两个算法找到的路径虽然不一样，但是总开销是一样的，都可以认为是最优解。JPS 处理过的点比 A* 要多，但是绝大部分点因为不符合跳点规则，所以都被忽略了，除开起点和终点，寻路过程中只有六个点被加入到开放列表中处理过，而 A* 在本例中有几十个点都被加入过开放列表，多了大量的运算。\n在解释 JPS 的步骤之前，需要明确两个它的独有概念，跳点和强制邻居点。虽然说是两个概念，但是其实是两个强关联的概念。如下图是几种强制邻居点的情况，P 是前置点，C 是当前节点，N 是强制邻居点。如果 N 是 C 的邻居，且 N 的邻居中有阻挡点，并且从 P 到 N 的最短路径一定要过 C，则 N 就是 C 的强制邻居点，而 C 就是跳点。如果前置点到当前点是斜向的话，那么当前点两个分量方向上延申的点如果有强制邻居点的话，那么当前点也是跳点。\nJPS 算法的步骤要复杂一些，大概如下：\n首先将起点设为当前点，并且将它加入封闭集合中。 从当前点开始，检查当前点是否有前置点，如果没有前置点的，说明是起始点，起始点的扩散方向为周围全部的 8 个方向。如果不是起始点的，则会首先判断自己有没有强制邻居点，如果有的话，将强制邻居点加入到开放列表中，然后根据前置点和自己的位置判断自己所在的方向，如果自己是在平直方向上，则沿着该方向搜索，如果是在斜角方向上，则先向斜角两个向量方向搜索，然后再沿着斜角搜索。 在延展搜索的过程中，如果碰到了地图边界或者是阻挡，则停止这个方向上的搜索。对搜索到的每一个点检查它是否有强制邻居点，如果有，则它就是跳点，停止当前的搜索，并且将该点加入到开放列表中。 检查开放集合，如果为空，则地图搜索已经结束了，跳至步骤 6。 如果开放集合不为空，则取出其中总开销最小的一个点设为当前点，比较它的位置是否与目标终点一致，如果一致，则搜索结束，否则将它加入到封闭集合中，然后跳回步骤 2。 当搜索结束时，如果当前点的位置不是目标终点，那么寻路失败。否则从当前点也就是终点开始一直找前置点，这条线路的逆序就是寻路的结果。 再回过头来复盘一下这个图的寻路过程。首先从起点开始向 8 个方向搜索，搜索右上角到 A 点时，从 A 点向上边和右边扩散，当扩散到 B 点时，发现 B 有强制邻居 C 点，说明 A 是跳点，将 A 的前置点设为起始点并且加入开放列表中，停止继续搜索。\n从开放列表中拿取一个开销最小的点也就是 A，根据 A 的前置点，得到它的方向是向右上的。向上会碰到边界，向右到达 B 点的时候，可以发现 B 有一个强制邻居点 C，说明 B 是一个跳点，将 B 的前置点设为 A，然后将 B 加入到开放列表中。同时斜角方向发现了跳点 D，将其前置点设为 A 并且加入到开放列表中。\n从开放列表中拿取一个开销最小的点也就是 B，检查它周围，检查到有一个强制邻居点 C，将 C 的前置点设为 B，B 因为 A 的方向的关系，所以它会沿右方处理，并不会找到新的跳点。\n从开放列表中拿取一个开销最小的点也就是 C，检查它的周围，发现一个强制邻居点 D，但是 D 已经在开放列表中了，所以不再处理它。C 因为前置点 B 的方向关系，它会向右上搜索。可以找到 E 有一个强制邻居点 F。将 E 的前置点设为 C，并且将它加入开放列表中。\n从开放列表中拿取一个开销最小的点也就是 E，检查它的周围，发现一个强制邻居点 F，将 F 的前置点设为 E，并且将它加入到开放列表中。E 向右扩展，没有新的发现。\n从开放列表中拿取一个开销最小的点也就是 F，检查它的周围，发现终点，将终点的前置点设为 F，从终点开始逆序，可以得到 Start → A → B → C → E → F → End 的路线。\n总结 寻路算法是一个发展了很久很久的方向，除了上面列出的这三种比较有代表性的算法以外，还有非常非常多的其它算法，不过思路大部分比较类似，都是在前人的基础上做的一些改进。\n在游戏开发中寻路是个偏前端的问题，除了 SLG 游戏以外，寻路一般都是在前端实现的，后端只会拿前端得到的路径判断一下点的合法性就行了。但是寻路算法的复杂性还并不是这些算法的理解，而是要搞定策划的需求，很多时候高效简洁的实现可能无法满足策划的想法，要在算法的基础上做各种贴近业务的定制化修改。\n","date":"2022-04-05T16:28:27+08:00","permalink":"https://wmf.im/p/%E6%B5%85%E6%9E%90%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95/","title":"浅析游戏开发中的寻路算法"},{"content":"　内存对齐是操作系统为了可以让 CPU 高效读取内存中的数据采用的一种策略，它使得数据在内存中都按一定的规则排列。本篇来解释一下内存对齐。\n为什么需要对齐 计算机一般使用字节（byte）作为最小可寻址的内存单位，内存中的每个字节都有一个唯一的数字标识，也就是它的地址了，通常使用一个十六进制的数来表示。如果是占用多个字节内存的数据，则它的地址就是所使用字节中最小的地址。\n虽然字节是最小可寻址单位，但是并不是 CPU 的最小读取单位。CPU 从内存中读取数据时需要通过 cache 来作为中间层，会根据目标地址首先在 cache 中找，如果找不到，就会首先从内存加载数据到 cache 中，然后再读取。单次从内存加载到 cache 的数据大小叫做 cache line，它的大小跟硬件有关，一般是 16 到 256 字节。在 Linux 下可以很方便查询这个值，下面给出两种查询方法，可以看到在我本地的平台上这个长度是 64 字节。\n1 2 3 4 5 $ cat /sys/devices/system/cpu/cpu0/cache/index0/coherency_line_size 64 $ getconf LEVEL1_DCACHE_LINESIZE 64 CPU 在使用数据时，需要再次从 cache 中加载内容到寄存器里，这个步骤每次加载的最大长度是一个字长（word size），字长的大小等于我们常说的 xx 位 CPU 的那个数字，常用的 64 位 CPU 的字长就是 64 比特，也就是 8 个字节。当要处理大于 8 个字节的数据时，CPU 需要多次加载处理。\n到这里铺垫已经结束了，但是还没有回答真正的问题，因为不管一次读多少，只要按字节来读就可以了，并不需要对齐。比如一个 long 类型变量地址是 6，长度是 8，那 CPU 从 6 开始读 8 个字节就可以了，并不会有什么问题。\n真正导致需要内存对齐的原因是，内存条的编址方式导致了 CPU 无法从指定位置开始访问。因为内存条的 IO 采用了多个颗粒并行执行，逻辑上连续的 8 个字节其实是分布在 8 个不同的 bank 上，并行读取可以为内存的访问加速。内存条寻址的规则就是以 8 个字节为单位的，这就导致了 CPU 也只能以 8 个字节为粒度进行读取，且起始地址也要是 8 的倍数。来看一个例子。\n如上图所示，假设一个占用 8 字节的 long 类型整数存放在内存中，如果使用第一张图中的存放方式的话，那么在 64 位环境下，需要读取两次，分别是 0 - 7 字节 和 8 - 15 字节，两次组合才能得到该数字。如果它对齐到 8 的倍数的地址，那么一次读取即可完成。\n实现 对齐的规则 内存对齐的规则大概可以总结为四点：\n对于内置类型，只要它的地址是它长度的整数倍即可。 对于 struct，其中的每个数据都要对齐，struct 本身也要向其中最大的那个数据对齐。 对于 union，按照其中最长的那个数据对齐。 对于数组，无需特殊处理，因为其中每一个数据都对齐了，数组本身就对齐了。 对齐的操作 在 struct 中，如果内存紧密排布可能会出现其中一些数据无法对齐的情况。\n1 2 3 4 struct s{ int i; double d; }; 比如结构体 s，如果紧密排布的话，i 占用 0 - 3 字节，d 占用 4 - 11 字节，这样就导致了 d 没有对齐。所以会在 i 和 d 之间增加一个填充部分（padding），长度为 4 字节，使 i 和 d 都对齐。\n可以看到填充部分的增加使结构体中每一个数据都对齐了，但是也浪费了一部分的内存。当使用 sizeof 查询 s 的长度时，得到的结果是 16 字节。\n1 2 3 4 struct s{ double d; int i; }; 如果将 d 和 i 的顺序反过来，情况会有所不同，虽然看起来 d 和 i 都对齐了，但是它本身的长度为 12 字节，没有对 8 字节对齐，需要在它的末尾追加一个 8 字节的填充部分。\n使用 sizeof 查询 s 的长度可以发现它的长度依然是 16 字节。\n优化内存占用 可以看到因为填充部分的存在，如果我们设计 struct 时随意写数据的顺序可能会造成一定程度的空间浪费，而当熟悉对齐的规则时就可以排布出一个最省内存的顺序。来看一道 CSAPP 上的题目。\n1 2 3 4 5 6 7 8 9 10 struct{ char *a; short b; double c; char d; float e; char f; long g; int h; }rec; rec 当前以 8 字节对齐，长度为 56 字节。其中为了让 c 对齐在 b 和 c 之间有一个 6 字节的填充，为了让 e 对齐在 d 和 e 之间有一个 3 字节的填充，为了让 g 对齐在 f 和 g 之间有一个 7 字节的填充，为了让整个 rec 对齐在 h 后面还有一个 4 字节的填充。一共 36 字节的结构，多使用了 20 字节的填充，浪费率比较惊人。下面来重排一下它的顺序。\n1 2 3 4 5 6 7 8 9 10 struct rec{ char *a; double c; long g; float e; int h; short b; char d; char f; }; 经过重排以后，可以发现数据之间已经完全不需要填充部分了，不过因为 rec 要向 8 字节对齐，所以最后的 4 字节填充是必不可少的。重排以后的长度为 40 字节，减少了 16 字节的空间浪费。\n我不想对齐行不行 可以看到内存对齐在大部分情况下不管怎么优化，都会浪费一部分空间。它虽然提升了一点内存访问的速度，但是可能很多时候我们并不十分在乎，反而更在意内存的额外占用。\n早年间的一些 CPU 是不支持没有对齐的内存访问的，但是在现代 CPU 上已经没有了这个问题，编译器也提供了一些选项可以让我们明确指出不需要内存对齐。\n1 2 3 4 struct __attribute__((packed)) testc { int i; char c; }; 如果加上了 attribute((packed))，则编译器会使内存紧密排布，去掉所有对齐的优化。这个时候再输出类的长度，可以发现它的长度是 5，就是一个 int 加上一个 char 的长度。如果去掉紧密排布的声明，则类的长度是 8，char 后面会增加 3 个字节的填充。\n让类的空间密集排布一定会影响内存的读取效率，但是实际测试一下就会发现，这个影响其实非常非常有限，基本上可以忽略不计了。所以如果对内存占用更敏感的程序，可以主动放弃内存对齐，比如 Redis 的 sds 的结构体就全都是紧密排布的。\n指定对齐的字节数 默认情况下类内的对齐是按照本类中最大长度的成员变量对齐的，可以指定让一个类向一个固定的值对齐。\n1 2 3 4 5 struct __attribute__((aligned(8))) testc { char c1; int i; char c2; }; 如果没有指定对齐到 8 字节的话，testc 的长度应该是 (1 + 3) + 4 + (1 + 3) = 12，当指定向 8 字节对齐以后，testc 的长度就是 (1 + 3) + 4 + (1 + 7) = 16。\n需要注意的是，指定的长度是整个结构对齐的长度。即使指定了对齐的长度，c1 和 i 之间的填充部分还是只填到 i 长度的整数倍就可以了，只是最后整个 testc 需要对齐的 8 字节的倍数，所以向后补充了 7 个字节。\n","date":"2022-04-02T12:16:57+08:00","permalink":"https://wmf.im/p/%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/","title":"内存对齐"},{"content":"　有时因为业务需要，要将一个 Lua Table 序列化，如果不在乎序列化结果的可读性，可以直接将 Table 序列化成二进制数据块，如果需要结果的可读性，可以将 Table 序列化成字符串。\n要序列化的内容 因为 Lua 的 Table 中可以保存各种各样的内容，不可能都序列化，而且基本也不会有这种需求，比如像是 Function 这种变量，序列化出来并无意义，因为在另一个环境下反序列化以后就不能用了。\n出于这个原因， Function 和 Thread 变量不管是 key 还是 value 都不会处理。如果 key 是 Table 变量，则也不处理，同时 Table 中不能有对其它 Table 的循环引用。\n虽然看起来有一些限制，但是因为序列化 Table 一般都是用在保存或者发送上，所以其实本身就基本上不会有这种受限制的需求。\n二进制 将 Table 序列化为二进制数据块的速度是最快的，体积也是最小的，如果没有可读性要求，且序列化远比反序列化的次数多，那就果断选择序列化成二进制。\n云风大佬有一个现成的实现可以直接拿来用 lua-serialize，自己实现一个也不难，就是使用 Lua 提供的 C Api 遍历 Table，然后根据 key 和 value 的类型，不断写入到一个可以动态扩容的内存块链表中，全部写入完成以后再将链表中的所有块拷贝进一个连续内存中。\n除了结果没有可读性以外，序列化成二进制还有一个缺点就是结果要反序列化成 Table 的时候依然需要逆向解析一遍。\n字符串 序列化成字符串的优点是，结果可读性好，而且不需要额外实现反序列化，可以直接使用 load 来加载字符串实现反序列化。\nLua 实现 用 Lua 来实现是最顺畅最简单的，不用写 C 代码，也不需要编译。如果没有高频使用的话，用 Lua 序列化就足够用了。\n有一个非常好的第三方实现可以用来解决这个问题，serpent，功能强大，不仅可以序列化，还可以将 Table 以可读方式打印出来。\nC/C++ 实现 由于工作需要高频将 Table 序列化成字符串，且单个 Table 数据量也不少。所以一直想找一个 C/C++ 语言的实现版本，但是 github 上找了半天也没找到。后来周末抽了个时间自己实现了一个 luaseri-cpp。\n实现并不复杂，但是碰到了一个问题，就是 snprintf 在处理 double 的时候非常慢。搜索以后发现了一个非常切合我需求的项目，dtoa-benchmark，是腾讯的大佬叶劲峰实现的，学 C++ 的应该都听说过他，《C++ Primer 5th》中文版的审校之一。该项目给出了他对 Grisu2 算法的实现，效率是挺好的，就直接拿来用了。\n性能对比 对比以上的三种实现，经过测试，可以得出性能的大致对比结果。\n在序列化一个大型 Table 的时候，以 lua-serialize 的用时为基准 n，luaseri-cpp 的用时为 1.8n - 2n，serpent 的用时大概为 40n 左右。lua-serialize 序列化出来的结果也比其余两种结果的长度小很多，具体比例与 Table 中的数据有关系，当 Table 中的数字越多时，长度差距越大。\n反序列化的时候，lua-serialize 需要使用它自带的反序列化函数，同样以它的反序列化事件为基准 m，序列化为字符串的实现使用 Lua 自带的 load 函数加载字符串得到 Table，使用的时间大概在 2m 左右。\n可以看到 lua-serialize 无论在序列化和反序列化上时间都占优不少，在空间使用上优势更大。所以如果不需要结果的可读性的话，直接使用它即可。如果需要可读性，那么使用 luaseri-cpp 的效率也是比较快的。如果不喜欢用 C 库，或者对性能没要求的情况下可以使用 serpent 来实现。\n感觉可以在开发版本序列化成字符串，方便调试，在线上版本使用 lua-serialize，效率和空间都能占优不少。\n","date":"2022-03-31T02:18:47+08:00","permalink":"https://wmf.im/p/%E5%A6%82%E4%BD%95%E5%B0%86%E4%B8%80%E4%B8%AA-lua-table-%E5%BA%8F%E5%88%97%E5%8C%96/","title":"如何将一个 Lua Table 序列化"},{"content":"　RUDP 是 Reliable UDP 的简称，指可靠 UDP 协议。它通过一些额外的操作，为 UDP 协议提供了可靠性。本篇会来尝试比较一下 RUDP 和 TCP 的区别。\n引子 UDP 和 TCP 是传输层最常用的两个协议。UDP 是个不可靠的协议，它不保证数据交付，而 TCP 是可靠的，它可以保证接收方接收的字节流与发送方发送的字节流完全相同。\n那么就有了一个问题，既然已经有 TCP 提供可靠服务了，直接用不可以吗？为什么还要魔改 UDP 让它来提供可靠服务呢？\n要回答这个问题，首先要了解一下 TCP 的弱点是什么。\nTCP 的弱点 TCP 之所以可以保证可靠，是因为它使用了包括数据校验和，为报文段增加序号，增加接收方反馈，超时重传，快速重传，在双端增加缓冲区等机制。\n当网络畅通无阻时，这些机制都可以非常有效的运行，也就是说如果网络环境良好时，没必要使用 RUDP 了，直接使用 TCP 就是最好的选择。\n但是实际中的网络环境千差万别，尤其是现在移动互联网盛行，用户可能是在地铁，电梯等信号不太强的地方使用 4G 等方式上网的。这时候如果继续使用 TCP 的话，在一些对网络延迟要求比较高的场景中体验可能就会很差。\n来看一些 TCP 的各种机制存在的问题。\n三次握手 众所周知，TCP 在建立连接的时候需要三次握手才行，三次握手一共需要 1.5 RTT 才能完成。如果是在 HTTPS 上的话，则三次握手以后还要再次进行 TLS 握手，又需要 1.5 RTT 才行。\n虽然大部分情况下握手带来的消耗并不大，但是如果想要更快速的建立起连接，那这就是一个可以优化的地方。\n以 Web 开发已经大量使用的 QUIC 协议来说，它可以合并连接握手和加密握手，最终在 1.5 RTT 内实现连接建立，甚至可以通过对服务器的缓存，实现 0 RTT 建立连接。\n四次挥手 因为 TCP 全双工的特点，且报文段都需要 ACK 来确认。所以 TCP 的断开连接比握手更麻烦一点，要 A 先发送断开报文，B 确认断开报文，B 再发送断开报文，A 确认断开报文，才能完成整条连接的关闭。\n因为报文又涉及到重传之类的问题，所以还需要设计出一个 TIME_WAIT 状态，等待 2 MSL 来确认对方收到了自己对它的断开报文的确认。\nQUIC 对这个过程也进行了不小的简化，比如把关闭改为了单工的，一方发起关闭即代表了整条连接的关闭，同时也简化了连接的状态机，不再需要 TIME_WAIT 状态。\nRTO 倍率 TCP 会为拥有最小还未被确认序号（SendBase）的报文段设置一个重传定时器，如果超时了，会将它重传，并且会再次为它设置定时器。如果收到了确认报文，则会取消当前的定时器，为下一个待确认报文创建新的定时器。\n重传超时的时间被称为 RTO（Retransmission Timeout），它在根据 RTT（Round Trip Time）通过一定的算法计算得来的。在 TCP 的实现中，第一次设置的定时器超时时间为 RTO，如果触发超时以后，下一次为本报文段设置的定时器超时时间会变为 2RTO，如果再次触发，则下一次会是 4RTO，每次会比上一次翻倍。\n之所以这样实现，是因为 TCP 考虑到丢包很可能是因为发送方和接收方之间的一台或多台路由器上有太多数据包来不及处理，如果在短时间内继续重传，不仅依然收不到，而且可能会导致拥堵问题更加严重。\n在弱网情况下，这个每次翻倍的机制可能就会拖累用户的使用体验。并且弱网下的丢包大部分情况下都是因为硬件的信号问题，而不是线路上路由器拥堵。所以一般 RUDP 的实现会把这个倍率调低，通过更加频繁的重发，尝试将报文成功发送出去。\n但是需要注意的是，如果确实是因为线路拥堵的原因导致的丢包，那么调低倍率 RTO 倍率并不会带来任何好处，不仅多浪费了流量，而且还让线路更加拥堵了。\n差错恢复 网络数据流中处理差错恢复的方法主要有：回退 N 步（Go Back N，GBK）和选择重传（Selective Repeat，SR）这两种。TCP 的实现是把这两种混合在一起的。\n这里先不展开讨论这几种差错恢复的全部区别，只把重点放在数据重传上。\nGBN 的接收端不设置缓存，失序报文全部丢掉，返回最后确认的 ACK。如果某个序号为 n 的报文段在发送过程中丢失了，它会把包括 n 在内的所有 n 以后的已发送的报文全部重发一次。\nSR 中每个报文段相对独立，如果一个报文段在发送中丢失了，那么它自己的定时器超时以后只会重传它自己即可。\nTCP 一般的实现中会在接收端缓存失序到达的报文段，并且在收到失序报文时会给发送端回复一个冗余 ACK，三次冗余 ACK 以后会触发 TCP 的快速重传机制，让发送端立即重发缺失的报文段。接收端在收到缺失的报文段以后，会在缓存中查找后续的报文段，根据缓存中的情况，直接返回给发送端最新的 ACK 即可。\nRUDP 的实现一般不太一样，以 KCP 为例，它采用了 SR 的策略，并且接收端不会传冗余 ACK，而是由发送端根据接收到的后续的 ACK 来判断某个报文是否丢失了，从而发起快速重传。\n拥塞控制 如果说前面的都是小问题的话，那么 TCP 的拥塞控制可能就是 RUDP 大行其道的最大原因了。\n拥塞控制是 TCP 的一套试探线路负载上限的策略，包括了慢启动，拥塞避免和快速恢复三种可以互相转化了状态。\n慢启动，是指 TCP 会以 1 个 MSS 为拥塞窗口，逐步向上增加，每次收到一个新的 ACK 时就增加一个 MSS，看起来是好像时线性增加其实是指数增长的，因为如果网络没问题的话，下一次在一个 RTT 后会收到 2 个新的 ACK，会直接增加 2 个 MSS，同理下一次会增加 4 个 MSS。虽然增加的速度已经挺快了，但是一开始的速度会比较慢。\n如果在 TCP 中发生了超时丢包，都会将拥塞窗口重置为 1 个 MSS，并且 ssthresh（Slow Start Threshold，慢启动阈值）会被设为当前拥塞窗口的二分之一。在拥塞窗口超过了 ssthresh 以后，会进入到拥塞避免状态，每个 RTT 只能增加一个 MSS。同时如果发生的是由三个冗余 ACK 触发的丢包，那么 TCP 会将拥塞窗口变为原来的二分之一。\n可以看到在 TCP 中只要发生了丢包，都会倒是速率大幅下降，然后再慢慢升上来。\n同时 TCP 的拥塞控制秉承了公平性的原则，两条链接如果都要大量使用带宽的话，那么最终它们分到的带宽都接近带宽总量的二分之一。\nTCP 的拥塞策略是 RUDP 大量修改的地方，一般会改的激进很多，也不会完全遵循公平原则。不单是 RUDP，谷歌搞得 bbr 已经被合并到了 Linux 新版内核里，也是为了修改拥塞控制。\n难修改 为什么 TCP 有了这些问题，也有了明确的改进方向，但是并没有进行大刀阔斧的改革呢？\n主要是因为 TCP 是运行在内核态的协议，整个实现都在内核中。这就导致了它难开发，也难修改，因为不能把修改的部分简单的集成到客户端中。就算自己魔改了服务器上的 TCP 实现，但是用户的客户端 TCP 协议还是自带的实现，无法与魔改的实现兼容。即使是内核的实现也更新了，也还有大量运行着旧版内核的设备无法使用新特性。\nRUDP 基本上都是在用户态实现的，调试修改都很容易，也可以很简单的集成到客户端程序和服务器程序中，完全不需要改动内核，方便使用和更新。\nRUDP 的问题 运营商 QOS 国内的一些地方的一些运营商，包括御三家，经常会对 UDP 数据进行限制，尤其是出国流量，轻则丢包，重则断连，可能完全无法使用。\n这样的情况导致 RUDP 很难作为单一协议使用，基本上还需要一份 TCP 的实现进行保底。\n为了解决这种情况，有一些特殊的办法。比如可以通过一些手段将 UDP 数据伪装成 TCP 数据。有一个开源项目 udp2raw 就是针对功能这个实现的。或者通过发送冗余数据这种简单粗暴的方法，使用更多流量来对抗丢包，可以尝试使用 UDPspeeder。\n能耗 由于 RUDP 都是运行在用户空间的，每一次的数据收发都需要进行上下文切换，而运行在内核空间的 TCP 的上下文切换会少很多。\n为了应对运营商的劣化，可能需要做的伪装也更加剧了 RUDP 的性能消耗。\n此外因为 TCP 已经作为主流协议使用几十年了，几乎任何需要使用网络的设备都会对 TCP 做各种软件硬件的优化，而这都是 RUDP 们不具备的，缺少优化也是现阶段 RUDP 能耗比 TCP 高的原因，可能需要 QUIC 这样强势的 RUDP 快速普及才会有好转。\n","date":"2022-03-28T02:18:47+08:00","permalink":"https://wmf.im/p/%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-rudp-%E5%8D%8F%E8%AE%AE/","title":"为什么需要 RUDP 协议"},{"content":"　网络编程是后端永远绕不开的话题，毕竟后端的存在就是为了给前端提供服务，而提供服务的方式就是网络。本篇会记录一些学习网络编程中碰到的基础知识。\n网络结构分层 对网络进行分层可以更加清楚的认识当前网络环境的运行状态。分层的方式有很多种，例如 OSI 的 7 层模型和 TCP/IP 的 4 层模型，本文会遵照 4 层模型的设计。\n应用层（Application）：HTTP，SSH 传输层（Transport）：TCP，UDP 网络层（Internet）：IP 链路层（Link）：网络硬件及其驱动程序 从上向下越来越接近底层。可以看到网络开发中经常打交道的 HTTP，TCP，UDP 这些协议都在比较靠上的层中。\n链路层 链路层包括了网卡、网口这类硬件设备，同时也包括了硬件的驱动和硬件在操作系统的抽象。链路层为网络上的主机提供了硬件连接。\n链路层中传输的数据被称为帧（frame），不同链路层协议每帧可以传输的最大数据长度不同，这个长度被称为最大传送单元（Maximum Transmission Unit，MTU），帧在传输中可能经过不同协议的链路，其中最小的 MTU 被称为路径 MTU，超过路径 MTU 的数据链路层会直接丢弃，所以需要在上层做好分片。\n操作系统会为每个网卡提供缓冲区，在网卡启动的时候，会为它分配多个 RingBuffer 来作为接收队列和发送队列。\n当收到网络数据帧的时候，网卡会先把数据通过 DMA 直接把数据写入到自己的接收队列中，超出队列长度的部分会被直接丢弃。\n当需要发送数据时，操作系统内核会将数据写入到网卡的发送队列中，网卡将发送队列中的数据发送出去，然后触发硬件中断清理掉发送队列的内容。\n网络层 网络层运行在路由器和主机上，它为网络上的主机提供了逻辑连接。它主要提供的功能是转发和路由选择，前者由硬件实现，负责将数据从输入的链路层接口转发到合适的输出链路层接口，后者由软件实现，负责确定从源地址到目的地址的端到端路径的网络地址范围。\n网络层中我们主要看一下 IP 协议，它是本层中使用最广泛的协议。IP 协议是一个尽力交付协议，不保证交付，它分为 IPv4 和 IPv6 两种，目前都在使用中，并且会逐步实现从 IPv4 向 IPv6 的过渡。\nIPv4 的地址长度是 32 bit，一般采用点分十进制表示，每 8 个 bit 被分在了一段中，例如会使用 233.233.233.233 来表示 1110 1001 1110 1001 1110 1001 1110 1001。首部长度不固定，在 20 - 60 字节，不带有选项的典型首部长度是 20 字节。提供对数据分片和组装的功能。会使用首部校验和对首部的数据进行检查。\nIPv6 的地址长度是 128 bit，通常的表示格式会被分为 8 组，每组的 16 bit 使用 4 个十六进制数来表示，例如 2001:0db8:85a3:08d3:1319:8a2e:0370:7344 就是一个合法的 IPv6 地址。首部长度固定为 40 字节。它不提供分片功能，如果从链路层收到的数据在转发的时候超过了下一个链路层的 MTU，则 IPv6 会直接丢弃该数据报。同时它也不再提供首部的数据校验。\n传输层 传输层为网络上的主机中的进程提供了逻辑连接，传输层的协议只在主机中实现，不会在路由器上实现，不管用什么协议，对路由器都是透明的。\n最常见的传输层协议是 TCP 和 UDP，TCP 为进程间提供了可靠，有连接的服务，UDP 提供的则是不可靠，无连接的服务。\n传输层通过多路复用和多路分解来把主机接收到的数据交付给指定的进程。\n应用层 应用层的协议是基于传输层的协议实现的，一般都是把一些常见的开发需求实现成一个规范的协议，避免重复开发。\n比如 SSH 协议，如果完全按照它的规范实现一个服务端程序，那么市面上所有的 SSH 客户端就都可以直接拿来跟这个程序通讯，同理按照它的规范实现一个客户端程序，也可以跟市面上所有 SSH 服务端程序通讯。\n如果不使用通用协议，自己实现双端的协议也是可以的，这种非公开的协议一般被成为私有协议，使用私有协议通讯的软件也比比皆是，尤其是一些大公司的软件，比如微信这种。\n网络编程接口 《Unix 网络编程》里有一个我印象很深刻的例子，是说网络编程就像是打电话的步骤一样。服务端首先要买一台电话机（socket），然后要去运营商注册一个电话号码（bind），接着在电话旁边等待（listen），最后当电话响起的时候接听（accept）。客户端同样需要先买一台电话机（socket），然后注册号码（bind），最后打目标的电话（connect）。\nsocket socket 用来创建网络套接口的系统调用。调用的时候可以指定各种协议组合，返回的就是当前未使用的最小网络套接口的句柄。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #include \u0026lt;sys/socket.h\u0026gt; int socket(int domain, int type, int protocol); // domain // AF_INET IPv4 // AF_INET6 IPv6 // AF_LOCAL Unix socket // type // SOCK_STREAM 字节流套接口 // SOCK_DGRAM 数据包套接口 // SOCK_NONBLOCK 非阻塞套接口 // protocol // IPPROTO_TCP TCP 传输协议 // IPPROTO_UDP UDP 传输协议 domain 表示套接口使用的协议域，type 是套接口的类型，protocol 是协议类型。其中前两个参数是一定需要的，第三个参数可以填 0 让内核使用前两个参数匹配的默认协议类型。比如 domain 填 AF_INET，type 填 SOCK_STREAM，protocol 填 0 或者 IPPROTO_TCP，这样创建的套接口就是最常用的 IPv4 TCP 类型。\nsocket 函数默认情况下创建的套接口都是阻塞的，可以先创建好以后再修改，如果想要直接创建出一个非阻塞的套接口，那么可以使用 SOCK_NONBLOCK 参数，比如当我们把参数 type 设为 SOCK_STREAM | SOCK_NONBLOCK 就会创建出非阻塞的套接口。\n需要注意的是，内核可能会复用套接口描述字, 如果程序将套接口描述字保存下来的话，可能会碰到问题。比如一条连接早就关闭了，但是描述字一直保存在某个位置，当下次取出它想要发送数据的时候，它可能已经被内核复用，表示另一条连接了。\n内核在创建套接口的时候会为它创建缓冲区，有发送缓冲区和接收缓冲区，或者叫发送队列和接收队列。当执行 send 的时候，其实 send 只是把数据拷贝到了套接口的发送缓冲区中函数就返回了，具体的发送步骤依靠操作系统来安排。\n套接口在创建的时候可以设置是否阻塞，如果是阻塞的话，当发送缓冲区满了的时候，send 调用会被阻塞住，如果是非阻塞的，则当发送缓冲区满了的时候直接返回一个错误信息。同理 recv 调用在碰到接收队列是空的时候，也会进行类似的操作。\n如果接收缓冲区还有数据的时候，进程通过执行 close 关闭套接口，则会清空接收缓冲区并且发一个 RST 给对方，然后再开始四次挥手。如果发送缓冲区还有数据的时候，进程通过执行 close 关闭套接口，则会把四次挥手的 FIN 放在发送缓冲区的最后，也就是说内核会等发送缓冲区的内容发送完毕以后才开始四次挥手。\n套接口选项 每个套接口可以有诸多选项，它们标识了套接口的功能和特性。一般使用 setsockopt 设置套接口的选项，使用 getsockopt 获取套接口的选项。\n1 2 3 4 5 6 7 8 9 10 11 12 13 #include \u0026lt;sys/socket.h\u0026gt; int getsockopt(int sockfd, int level, int optname, void *optval, socklen_t *optlen); int setsockopt(int sockfd, int level, int optname, const void *optval, socklen_t optlen); // 返回值 0-成功 -1-失败 // sockfd 打开的套接口描述字 // level 指定协议 // optname 选项的名字 // optval get 的时候存结果，set 的时候放要设置的值。 // optlen 表示 optval 的长度。 套接口选项的值为 0 则表示该选项没有开启，非 0 则表示选项已经开启。下面列出几个常用的套接口选项名。\nIPPROTO_TCP - TCP_NODELAY，用来关闭 TCP 的 Nagle 算法，小包发送不做等待。 SOL_SOCKET - SO_KEEPLIVE，保持连接存活。 SOL_SOCKET - SO_REUSEADDR，套接口绑定的地址可以重复使用。 使用 getsockopt 除了可以获取指定的选项的值以外，还可以获取套接口的错误信息。如果套接口发生了错误，内核会将 SO_ERROR 选项的值设为一个标准的错误代码，可以通过 strerror 来获取错误信息。\nI/O 阻塞类型 除了上文提到的可以在创建的时候直接通过 type 的参数设置套接口的阻塞类型外，还可以使用 fcntl 函数来修改一个套接口的阻塞类型。\n1 2 #include \u0026lt;fcntl.h\u0026gt; int fcntl(int fd, int cmd, ... /* arg */ ); 参数 cmd 有两个最常用的值，F_SETFL 用来设置文件标志，F_GETFL 用来获取文件标志。但是需要注意的是，设置的时候会覆盖掉之前已经设置过的标志，所以一般的常规用法是先获取，然后修改当前值，然后再设置回去。\n1 2 3 4 5 6 7 static void sp_nonblocking(int fd) { int flag = fcntl(fd, F_GETFL, 0); if (-1 == flag) { return; } fcntl(fd, F_SETFL, flag | O_NONBLOCK); } 这是 skynet 中设置套接口非阻塞的函数，就是 fcntl 的典型用法。\nbind bind 可以将一个套接口描述字绑定到指定的 IP 和端口上。因为主机一般都有不止一个 IP，所以要指定一个具体的 IP 出来。\n1 2 3 4 5 #include \u0026lt;sys/socket.h\u0026gt; int bind(int sockfd, const struct sockaddr *myaddr, socklen_t addrlen); // 返回值 0 成功，-1 失败。 // 参数可以直接用 getaddrinfo 的结果。 // ip 和 端口都在 myaddr 结构中指定。 如果传入的端口号为 0 的话，则内核会自动分配一个可用的端口号来进行绑定。如果传入的地址是 0.0.0.0 或者是 INADDR_ANY 的话，则内核可以自动绑定主机所有可用的地址。\n需要注意的是，bind 并不是一定需要的，不管是在服务端还是客户端，内核在处理没有 bind 的套接口时，会有一些辅助性操作。如果不调用 bind，直接对套接口描述字调用 listen 或者 connect 的话，效果和使用 INADDR_ANY 作为 IP，0 作为端口执行过 bind 是一样的，这一部分的内容可以参考 man 7 ip。\n有时候一些分布式进程并不需要指定端口，在 listen 获得端口以后，再上报自己的 IP 和端口给中心节点。\nlisten 套接口分为主动套接口和被动套接口，主动套接口负责向外发送数据，被动套接口负责接收数据。使用 socket 创建好的套接口在默认情况下都是主动套接口。\n1 2 3 #include \u0026lt;sys/socket.h\u0026gt; int listen(int sockfd, int backlog); // 返回值 0 成功，-1 失败 通过对一个未连接的套接口描述字调用 listen 可以将其改变为被动套接口，内核可以接受一个指向被动套接口的连接请求，同时内核为每个被动套接口维护了两个队列：未完成连接队列和已完成连接队列。\nbacklog 参数在 linux 中的意思是在等待 accept 的已完成连接的最大队列数量。不要设成 0，意义不明。如果超过了上限，会直接回一个 ECONNREFUSED 给客户端。内核对该参数的最大上限是在 /proc/sys/net/core/somaxconn 中，新版内核的实现里该值为 4096。\n当客户端调用了 connect 以后，服务端收到了第一个 SYN 分节时，就会在未完成连接队列里创建一个新的项，一直到收到自己的 SYN 的 ACK 以后，也就是三次握手完毕以后，把该项从未完成连接队列移动到已完成连接队列的最后。\n半连接队列一般使用哈希表来实现，它的最大长度的计算方法是 min(backlog, somaxconn, tcp_max_syn_backlog) + 1 的结果再上取整到 2 的幂，但最小不能小于 16。\n半连接队列的价值是，如果每个 SYN 包内核都创建一个 sock 结构体的话，消耗太大了，所以在三次握手完成之前，内核只为每个 SYN 包创建一个很小的结构体 request_sock 来缓解这种情况，同时可以配合 syn_cookie 机制抵抗 SYN flood 攻击。\n已完成连接队列一般使用双向链表实现，长度是调用 listen 时传入的 backlog 和 /proc/sys/net/core/somaxconn 之间较小的那个值。所有三次握手完毕但是还没有被 accept 调用的连接都在这个队列里。\naccept accept 用来接受连接，它的实际操作就是从指定的被动套接口的已完成连接队列的最前面取得一个连接，并且返回它的描述字，后续可以通过这个描述字跟发起 connect 的客户端进行通信。\n1 2 3 #include \u0026lt;sys/socket.h\u0026gt; // 成功，返回代表新连接的描述符，错误返回-1,同时错误码设置在errno int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen); 如果参数中传入的被动套接口描述字的已完成连接队列中并无连接，那么情况视套接口的阻塞模式而定。如果套接口是非阻塞的，那么直接返回错误，如果套接口是阻塞的，那么会一直等待已完成连接队列非空，拿到连接后才会返回。\n使用 accept 得到的套接口都是阻塞的，可以在拿到套接口描述字以后再将其设为非阻塞的，如果觉得麻烦，想要直接拿到非阻塞的套接字的话，可以使用 accept4 来操作。\n1 2 #include \u0026lt;sys/socket.h\u0026gt; int accept4(int sockfd, struct sockaddr *addr, socklen_t *addrlen, int flags); 使用 accept4 时，当 flags 传入 0 的时候，函数操作与 accept 完全一样。可以为 flags 传入 SOCK_NONBLOCK，这样得到返回的套接口就是非阻塞的。\nconnect connect 是客户端向服务端发起连接的接口，当它被调用时，会触发 TCP 的三次握手过程。\n1 2 #include \u0026lt;sys/socket.h\u0026gt; int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen); 需要注意的是，如果 connect 失败了，那么一定要关闭使用的套接口，再重新创建一个来进行下一次连接。\nclose 可以使用 close 来关闭一个套接口。\n1 2 #include \u0026lt;unistd.h\u0026gt; int close(int fd); 套接口描述字本身有一套引用计数机制，调用 close 只会将当前的引用计数 -1，并不一定会真的触发关闭操作。如果引用计数等于 0 的话，内核会尝试发送当前套接字的发送缓冲区中所有剩余的数据，并且在发送完成后，向对端发送 FIN 分节，开始四次挥手。\n套接口 I/O 因为套接口描述字可以当作一个普通的文件描述字使用，所有有很多可以用来读写套接口的 I/O 函数，根据读写配对，有 read/write，readv/writev，recv/send，recvfrom/sendto，recvmsg/sendmsg 等，它们虽然参数不同，使用场景不太一样，但是功能是基本一致的，这里只写一下最常用的 read/write 好了。\n1 2 3 #include \u0026lt;unistd.h\u0026gt; ssize_t read(int fd, void *buf, size_t count); ssize_t write(int fd, const void *buf, size_t count); read 会尝试从套接口的接收缓冲区中读取长度为 count 字节的数据，保存在 buf 指向的内存中。如果读取错误，会返回 -1 并且设置 errno。如果读取成功，则返回读取到的字节数，这个数字可能比 count 小，也有可能等于 count，比 count 小的原因可能是，接收缓冲区中并没有那么多数据，也可能是 read 在执行时被系统信号打断了。\nwrite 会尝试把 buf 指向的内存中的 count 字节写入到套接口的发送缓冲区中。如果写入失败，则会返回 -1 并且设置 errno。如果写入成功，则会返回写入的字节数，这个数量同样可能会小于等于 count，因为可能套接口的发送缓冲区没有了足够的剩余空间。\nEPOLL epoll 是 Linux 实现的一套高性能的 I/O 复用机制，因为它拥有远超 poll 和 select 的性能，所以它也是目前 Linux 平台上进行 I/O 复用开发最常用的机制。\n使用场景 首先要了解 I/O 复用的使用场景。假设在服务器上有一个服务端程序，监听了一个端口，然后有很多客户端对这个进程发起了连接，这时候通过 accept 可以拿到大量的套接口。\n这些大量的已经连接的套接口，如何可以知道那个可读哪个可写呢？最笨的方法就是每隔一段时间遍历检查一遍所有的已连接套接口，检查它们的可读可写状态，进行操作。当套接口少的时候还可以这样，但是如果有很多很多套接口，这个方法的效率就很低。I/O 复用就是用来解决这个问题的。\nepoll 可以非常高效的管理很多套接口，用户可以对每个套接口设置关注的状态，然后每过一段时间检查 epoll 中就绪的套接口即可。\n创建 epoll 实例 使用 epoll 的第一步是创建一个 epoll 实例出来，用于管理其它套接口。可以通过 epoll_create 和 epoll_create1 来创建 epoll 实例。\n1 2 3 #include \u0026lt;sys/epoll.h\u0026gt; int epoll_create(int size); int epoll_create1(int flags); epoll_create 的参数 size 在旧版本内核中是用来告诉内核为实例的数据结构划分的初始大小，不过在新版的内核中 size 已经没有实际意义了，随意填写一个不小于 0 的数字即可。\nepoll_create1 是内核提供的一个新的创建 epoll 实例的接口。它去掉了已经无用的 size，增加了一个 flags，这个 flags 可以设为 EPOLL_CLOEXEC，表示启动执行即关闭 (close-on-exec)。\n这两个函数返回的都是 int 类型，是代表了创建出的 epoll 实例的文件描述字。\n修改 epoll 的关注列表 有了 epoll 实例，下一步要修改它的关注列表了。可以使用 epoll_ctl 来为一个需要关注的套接口增加关注事件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include \u0026lt;sys/epoll.h\u0026gt; int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); // op // EPOLL_CTL_ADD 增加关注事件 // EPOLL_CTL_MOD 修改关注事件 // EPOLL_CTL_DEL 删除关注事件 typedef union epoll_data { void *ptr; int fd; uint32_t u32; uint64_t u64; } epoll_data_t; struct epoll_event { uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */ }; 参数 epfd 就是上一步中创建出来的 epoll 实例的描述字，op 代表操作，fd 代表关注的套接口描述字，event 代表关注的事件。\ndata 的内容会在事件触发以后传给 epoll_wait 的返回值，可以用它保存一些在事件触发以后处理事件需要使用的数据。一般是需要记录下本描述字的，不然事件触发的时候会拿不到描述字。\n需要注意的是，当对一个描述字增加过关注事件以后，第二次想要增加需要使用修改才行。同时修改只能修改已经在关注列表中的描述字，也就是要先增加过关注事件才能修改。删除也是一定要当前存在关注列表中的，否则会报错。\nevents 是一个掩码位，列举几个常用的选项。\n位掩码 作用 EPOLLIN 普通数据可读 EPOLLOUT 普通数据可写 EPOLLRDHUP 套接口对端关闭 EPOLLET 采用边缘触发事件通知 等待 epoll 事件 设置好 epoll 的事件关注列表以后，就可以等待事件发生了。使用 epoll_wait 来等待一个 epoll 实例上的事件发生。\n1 2 #include \u0026lt;sys/epoll.h\u0026gt; int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); 参数 epfd 就是 epoll 实例的描述字，events 用来接收触发事件的结果，maxevents 是 events 数组的长度，timeout 是等待的超时时间。\n如果调用成功，则返回值是就绪的描述字个数，同时 events 中包含了这些就绪描述字在使用 epoll_ctl 增加事件的时候传入的 event。\ntimeout 如果设为 -1，则会一直阻塞，直到有一个描述字就绪。如果设为 0，则不阻塞，执行一次检测，不管有没有就绪的描述字都直接返回。如果大于 0，则在有任何描述字就绪的时候返回，如果一直没有，那么会最多阻塞在这里 timeout 毫秒的时间。\n触发模式 I/O 复用的事件触发模式可以分为两种，水平触发和边缘触发。\n水平触发指的是，只要当前事件仍然成立，就会返回就绪，比如套接口的接收缓冲区中有数据，那么每一次检查都会返回当前套接口可读。\n边缘触发是指，事件之前不成立，现在成立了，就会返回，一直到下一次再触发。即使缓冲区中的数据没有被读完，在下一次有新的数据进来之前检查也不会返回可读。\nselect，poll 只支持水平触发，epoll 两种都支持，默认情况下使用的是水平触发，可以在增加事件关注列表时对某个套接口的事件使用边缘触发。\n可以看到如果使用边缘触发，那么就需要在收到一个事件以后，不断处理完它，因为再也不会有第二次检测到这个事件的机会了。同时使用边缘触发的话，最好搭配非阻塞套接口使用，因为需要不断读取直到没有数据，如果使用阻塞套接口会很麻烦。\n使用边缘触发的时候需要注意套接口饥饿的问题，因为如果一个套接口上有大量的数据，然后读取的时候使用无限循环的方式调用 read 去读的话，可能会在这一个套接口上花费大量时间，从而导致其它套接口上的数据一直得不到处理。有一种规避的方法是，记录下来所有触发过的套接口描述字，不立即读完其中的数据，而是每次对每个套接口读取一定长度的数据，不断遍历全部的可读套接口，如果某一个套接口上的数据已经读完了，就将它从数组中移除，一直这样直到所有就绪套接口都处理完毕。\n","date":"2022-03-27T20:18:47+08:00","permalink":"https://wmf.im/p/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/","title":"网络编程基础"},{"content":"　AOI 全称 Area Of Interest, 在游戏开发中用来做角色视野内对象显示同步和状态同步，基本上只要有公共地图的游戏都会涉及到。本篇介绍一些常用的 AOI 实现方法。\n全局可见 一个最简单的实现方法是，场景中的所有对象在后端全都互相可见，任何对象的状态修改都会同步给全部的其它对象，在某个客户端上是否要显示某个对象完全依靠客户端来进行选择。\n这种最大的问题有两个，第一个是性能问题，毕竟每次更新都要广播给全部的对象，如果只有几十个对象还能用，多了就不太行了。第二个就是外挂问题，全部可以见意味着全部发送，这样客户端就有全部的对象数据，外挂可以直接拿到。\n存储数据也很简单，只保存一个全局的对象表就可以了，对象发生了任何要广播的事件就给剩下全部对象广播。\n非全局可见 在非全局可见的实现中，每个对象要维护一个被关注列表，它是指包含了所有能看到自己的对象的集合，自己的状态改变需要同步给集合中的所有对象。\n在对象有状态需要更新的情况下，直接广播消息给被关注列表的所有对象即可。比较麻烦的是加入场景和在场景中移动，因为这两种情况下对象需要维护自己的被关注列表，同时也需要更新自己可见的对象的被关注列表。\n自己可见的对象比较容易查询，因为有自己的视距和坐标，遍历范围内所有位置可以拿到所有自己可见的对象，把自己加入到它们的被关注列表即可。在维护自己的被关注列表时会碰到问题，因为自己并不知道其它对象的视距，所以不能直接遍历周围范围拿到。\n处理这个问题最简单的方法就是遍历场景上全部的对象，依次计算距离并且与它的视距比较，判断是否可以看见新加入的对象。这种方法只能用在地图比较小的情况下，如果地图很大，那么全局遍历一次是很难接受的。\n另一种方法就是限制最大视距，因为一般来说不会存在无限视距的对象，所以可以对场景设置一个硬性的视距上限值，任何类型的对象视距都不能超过它。这样在遍历的时候只需要检查最大视距以内的所有对象即可。并且此时可以发现，因为触发修改的对象的视距也不会超过最大视距，所以可以合并两次遍历，即可以通过对最大视距范围内对象的一次遍历完成自己的被关注列表和自己可以看到的对象的被关注列表的更新。\n根据业务需要也可以考虑增加一个关注列表，即自己可以看到的所有对象的集合。因为在业务中可能需要获取某个对象能看见的全部对象，如果只维护了被关注列表的话，这个获取就会非常麻烦。维护关注列表并不需要额外的计算，只需要在更新自己可以看到的对象的被关注列表时把它加入到自己的关注列表里即可。\n优化思路 划分格子 如果以像素为单位遍历地图的话有点太耗费性能了，而且绝大多数业务也不需要这么高的精度，所以一般都会把地图划分成更大的格子来处理。\n比如坐标格子精度为 64 个像素，那么从 (0, 0) 到 (63, 63) 内的所有对象的相对坐标都是 (0, 0)。每个格子会保存一个当前格子内全部对象以 id 为 key 的哈希结构，查询到格子以后可以方便遍历，离开格子时也可以快速移除。\n相等视距 如果游戏中的所有对象的视距都相等的话，那么 AOI 的问题可以被简化很多。\n当视距全都相等时，可以考虑省略掉为每个对象维护的集合，只对每个坐标维护一个对象列表。因为不管怎么改变，都可以根据对象的位置快速确定有关的坐标点，从而拿到全部的需要处理的对象。\n如果视距相等并且不需要高精度的情况下，可以结合划分格子的思路，这就是一个常见的实现，“九宫格法” AOI 的思路，它将每个对象的视野限制为以自己为中心格子的九宫格中。\n十字链表 AOI 算法中还有一个广泛使用的十字链表法，它本身很有特点，跟上面的思路大相径庭。\n十字链表法可以用来处理多维的坐标系，在二维坐标系中，会使用两条有序双向链表，一条表示 X 轴，另一条表示 Y 轴。因为链表代表的坐标轴互相垂直像十字架一样，因此得名。\n它的原理是，如果在 (m, n) 点有一个视距为 20 的对象 O，那么有 x 轴的范围在 (m - 20, m + 20) 的所有点集合 A，y 轴的范围在 (n - 20, n + 20) 的所有点集合 B，A ∩ B 的结果即为 O 的可视范围中所有的点。\n当对象进入场景时，需要在 x 轴和 y 轴上分别准备三个点，假设对象的视距为 n，则这在 x 轴上的三个点是 x - n，x，x + n，在 y 轴上的三个点为 y - n，y，y + n。两边的两个点是边界点，它们用来判断某个点是否在本点的视距内，中间的点就是坐标点，它用来让别的点的判断它是否在其视距内。同时还需要为对象创建两个临时列表分别用来记录待确认的关注列表和待确认的被关注列表。\n假设如图中的 a 和 b 两点，a 的 x 轴坐标为 12，视距是 8，b 的 x 轴坐标为 18，视距为 5。首先 a 先加入到 x 轴中，此时链表为空，直接将 a 加入链表中，然后向前向后分别加入 a+ 和 a- 即可。当 b 加入时，依次会发生以下几个步骤。\nb 跨越 a-，代表了 b 已经进入了 a 的视距中，但是因为还未移动结束，所以会暂时把 a 加入 b 的待确认被关注列表中。 b 跨越 a，因为都是坐标点，所以不做操作。 b 到达目标位置停止移动，此时 a 还在 b 的待确认被关注列表中，因为移动已经停止，所以把 a 加入到 b 的被关注列表中即可。 以 b 的位置为基准点向前查找 b+ 的位置。 b+ 跨越 a+，因为都是边界点，所以不做操作。 找到 b+ 的位置，停止查找，创建节点。 以 b 的位置为基准点向后查找 b- 的位置。 找到 b- 的位置，停止查找，创建节点。 可以看到十字链表法在遍历的时候不是遍历坐标点或者格子的，而是直接遍历对象的，这意味着不需要通过划分格子来优化速度，也就是说可以在相同消耗下保持坐标点级别的精度。同时十字链表法对于视距没有什么要求，不同类型的对象使用不同的视距不会有任何问题。\n但是它也有一个自己的问题，就是在对象加入的时候，最坏情况下需要遍历每个维度坐标轴上所有的点，如果是二维坐标系则最坏结果为 2 * 3 * N 个节点。\n快慢指针 有一种优化链表查询速度的方法是使用快慢指针。它需要创建两个步长不一样的指针，步长比较长的是快指针，步长比较短的是慢指针。\n一开始快慢指针都指向表头，遍历的时候会先让快指针走一次，走过之后比较快指针的位置和目标位置，如果还没有到目标位置，则把慢指针的位置指向快指针的位置，并且让快指针继续走。直到快指针的位置超过了目标位置，此时可以确认目标位置就在慢指针和快指针之间，然后移动慢指针查找目标位置即可。\n快慢指针的优点是实现很简单，而且不需要修改原始的数据结构，缺点是它对查询速度的优化有限，时间复杂度还是 O(n)，且不同的快指针步长也会有不同的优化效果，效率不太稳定。\n提取索引点 优化有序链表的查询还有一个成熟且高效的实现，就是跳表。\n跳表会将链表中的某些点作为索引点提取到上一层组成一个新的有序链表，可以根据需要一共提取出几层快速链表。\n如图是将坐标轴转化为跳表的一个简单例子，在实现的时候可以只选择一些对象坐标点提取为索引点，不提取边界点，因为边界点上没有保存被关注列表。\n如图尝试为坐标轴增加一个对象 c，它的步骤如下。\n首先从最高层级的表头开始查找，找到下一个项为 a 对象。 比如 c 和 a 坐标，c 大于 a，但是 a 后续已经没有其它项，所以从 a 向下查找，来到下一层。 在本层中 a 的后续是 b，比较 b 和 c，发现 b 的坐标更大，此时要从 a 继续向下一层找。 到最下面这一层，首先新增对象要继承当前对象的被关注列表，也包括当前对象本身。也就是 a 要加入到 c 的待确认被关注列表中。 继续向后，首先 c 会经过 b-，将 b 加入 c 的待确认被关注列表。 然后跟 b 比较，发现 b 大于 c，所以已经找到了 c 的位置。 此时 a 和 b 在 c 的待确认被关注列表中，直接将 a 和 b 加入到 c 的被关注列表中即可。 从 c 的位置开始，向前寻找 c+ 的位置，向后寻找 c- 的位置。 c- 向后越过了 a，将 c 加入到 a 的被关注列表中。 c+ 向前越过了 b，将 c 加入到 b 的被关注列表中。 掷色子将 c 提高层次。 最终跳表的结构可能如图所示。\n使用跳表的优点是，速度提升明显，时间复杂度趋近于 O(logn)，远比快慢指针要快很多。缺点也比较明显，首先因为需要额外多层的链表，所以内存占用就更多一些，其次在实现上也会比快慢指针复杂很多，要多写很多代码。\n独立镜头 独立镜头跟上面的方案思路完全不同，它会给每个玩家创建一个镜头对象，这个镜头可以拉近拉远，各种方向移动，移动到哪里就有哪里的视野。\n镜头一般用在 RTS 和 SLG 这两类游戏中，根据游戏类型的不同，实现也有比较大的区别，基本上可以分为全局广播和视野广播。\n全局广播 一般 RTS 游戏都是全局广播的，也就是说只要是在镜头中需要呈现的数据，比如某个对象的血量这种，都会广播给所有的客户端，不管当前改变的数据是否在客户端的镜头下。玩家控制的镜头完全由客户端来实现，客户端拥有完整的数据，根据玩家控制镜头现在的位置显示当前镜头下的对象即可。\n这样做是因为 RTS 游戏对显示实时性的要求很高，对玩家操作的要求也很高，专业玩家一般会非常频繁地切屏，快速移动镜头，如果每次移动以后再由后端发送镜头中的数据，那就会有一个比较高的显示延迟，这样体验是很差的。\n因为客户端时刻都要保存并更新整张地图所有对象的位置，所以一般 RTS 的地图会比较小，而且对部队的总数量也会做一些限制，如果不加限制，可能会存在越玩越卡的情况。\n视野广播 视野广播一般用在 SLG 这类游戏上。SLG 和 RTS 的区别是，SLG 对操作的要求会低很多，战斗基本都是在拼数值，所以对实时性的要求也降低了不少。而且 SLG 的地图一般会比 RTS 大很多，地图上的单位也会多很多，这也要求了 SLG 游戏不能做全局广播。\n一般的实现会把地图分成若干格子，后端根据镜头的位置和视野，会维护一张记录了对每个格子可见的镜头表，镜头发生移动或者是有玩家进入退出游戏，就会改变这张表，实时更新。\n当一个单位的某个属性发生了变化，如果该属性是需要在镜头中呈现的，那么会将数据变更发送给镜头服务，镜头服务根据单位的坐标拿到它所在的格子，通过格子拿到所有可以看到该格子的镜头，把数据更新给持有控制镜头的玩家客户端。\n当镜头移动时，客户端可以等镜头相对静止以后再给后端发送镜头位置变更的消息，这样可以减少玩家移动镜头时的消耗。\n战争迷雾 RTS 或者是 SLG 这类游戏大部分都有战争迷雾，一般 RTS 游戏的迷雾都是在客户端实现的，不管有没有迷雾，后端都会将数据变更都会同步给全部的客户端。之所以这么实现，主要还是因为 RTS 游戏对实时性的要求比较高，如果破迷雾以后再发送数据过去，可能二三百毫秒的延迟，一场战斗的走向就完全变了。因为数据全发送，所以必然会有全图外挂，甚至一些处理的比较粗糙的游戏可以直接听到迷雾下面的声音，比如红警就可以听采矿车声音判断迷雾下面的对手是盟军还是苏俄。脱胎于 RTS 游戏的 MOBA 游戏一般也是一样的设计。\nSLG 采用视野广播，就完全可以做到只广播镜头可见的数据，迷雾下的数据全部不发送。这样可以完全避免全图外挂的可能性。同时 SLG 也可以接受一定的显示延迟。\n","date":"2022-03-27T02:18:47+08:00","permalink":"https://wmf.im/p/%E5%88%86%E6%9E%90%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91%E4%B8%AD-aoi-%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF/","title":"分析游戏开发中 AOI 的实现思路"},{"content":"　Lua 是动态类型语言，变量本身不带有类型，有值了才有类型。当前版本的 Lua 中有八种数据类型，分别是 nil，boolean，number，userdata，thread，string，function 和 table，本篇会分析这些数据类型的实现。\nTValue Lua 中不管是什么类型的值，在 C 层都是用同一个 TValue 结构体变量表示的。这样的实现与为不同数据类型定义不同的结构体相比，可以统一处理接口。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 /* ** Union of all Lua values */ typedef union Value { struct GCObject *gc; /* collectable objects */ void *p; /* light userdata */ lua_CFunction f; /* light C functions */ lua_Integer i; /* integer numbers */ lua_Number n; /* float numbers */ } Value; typedef unsigned char lu_byte; /* ** Tagged Values. This is the basic representation of values in Lua: ** an actual value plus a tag with its type. */ #define TValuefields Value value_; lu_byte tt_ typedef struct TValue { TValuefields; } TValue; 可以看到 TValue 中的结构很简单，只包含了一个宏定义 TValuefields，它里面又包含了一个表示值数据的 value_ 和一个表示值类型的 tt_ 。\nvalue_ 是一个 union Value 联合体变量，它可以表示 Lua 中所有类型值的数据，不同字段对应的类型注释中已经给出，其中所有需要进行 GC 的数据类型的指针都会保存在 gc 字段中。\ntt_ 是一个 unsigned char 类型，它的不同位表示了不同的涵义，如图所示。\n可以看到值的类型由两部分组成，一部分是表示基础类型，比如数字类型就是 LUA_TNUMBER，还有一部分表示变种类型，Lua 中叫 variant，比如数字类型又分成了两类，整数和浮点数，分别用 LUA_VNUMINT 和 LUA_VNUMFLT 表示。下面是全部的基础类型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 /* ** basic types */ #define LUA_TNONE (-1) #define LUA_TNIL 0 #define LUA_TBOOLEAN 1 #define LUA_TLIGHTUSERDATA 2 #define LUA_TNUMBER 3 #define LUA_TSTRING 4 #define LUA_TTABLE 5 #define LUA_TFUNCTION 6 #define LUA_TUSERDATA 7 #define LUA_TTHREAD 8 // LX, LG #define LUA_NUMTYPES 9 /* ** Extra types for collectable non-values */ #define LUA_TUPVAL LUA_NUMTYPES /* upvalues */ #define LUA_TPROTO (LUA_NUMTYPES+1) /* function prototypes */ #define LUA_TDEADKEY (LUA_NUMTYPES+2) /* removed keys in tables */ /* ** number of all possible types (including LUA_TNONE but excluding DEADKEY) */ #define LUA_TOTALTYPES (LUA_TPROTO + 2) GCUnion \u0026amp;\u0026amp; GCObject 在了解 GCObject 之前首先要看一下 GCUnion 这个联合体，它是所有 GC 对象真正保存的位置。在 GCUnion 中，有一个 GCObject gc 的字段，它并不真正代表某一种类型，而是一个抽象的公共字段，来提供类型转换。\n1 2 3 4 5 6 7 8 9 10 union GCUnion { GCObject gc; /* common header */ struct TString ts; struct Udata u; union Closure cl; struct Table h; struct Proto p; struct lua_State th; /* thread */ struct UpVal upv; }; 需要 GC 的值虽然有各自的结构，但是对外的时候都是统一以 GCObject 为标准的。GCObject 中包含了三部分的内容，next 是整个 GC 链表的一部分，所有需要 GC 的对象都通过 next 连接了起来。tt 又是数据类型，它跟上面的 tt_ 的用法是一样的，只不过它里面表示的是 GC 对象的类型。marked 用于在 GC 阶段的标识。\n1 2 3 4 5 6 7 8 9 10 /* ** Common Header for all collectable objects (in macro form, to be ** included in other objects) */ #define CommonHeader struct GCObject *next; lu_byte tt; lu_byte marked /* Common type for all collectable objects */ typedef struct GCObject { CommonHeader; } GCObject; Lua 中定义了很多 GCUnion 到 GCObject 之间互相转换的方法。gco2xx 用来将 GCObject 转为目标类型的 Lua 变量，obj2gco 用来将 Lua 变量转为 GCObject 类型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 #define cast_u(o) cast(union GCUnion *, (o)) #define gco2ts(o) check_exp(novariant((o)-\u0026gt;tt) == LUA_TSTRING, \u0026amp;((cast_u(o))-\u0026gt;ts)) #define gco2u(o) check_exp((o)-\u0026gt;tt == LUA_VUSERDATA, \u0026amp;((cast_u(o))-\u0026gt;u)) #define gco2lcl(o) check_exp((o)-\u0026gt;tt == LUA_VLCL, \u0026amp;((cast_u(o))-\u0026gt;cl.l)) #define gco2ccl(o) check_exp((o)-\u0026gt;tt == LUA_VCCL, \u0026amp;((cast_u(o))-\u0026gt;cl.c)) #define gco2cl(o) check_exp(novariant((o)-\u0026gt;tt) == LUA_TFUNCTION, \u0026amp;((cast_u(o))-\u0026gt;cl)) #define gco2t(o) check_exp((o)-\u0026gt;tt == LUA_VTABLE, \u0026amp;((cast_u(o))-\u0026gt;h)) #define gco2p(o) check_exp((o)-\u0026gt;tt == LUA_VPROTO, \u0026amp;((cast_u(o))-\u0026gt;p)) #define gco2th(o) check_exp((o)-\u0026gt;tt == LUA_VTHREAD, \u0026amp;((cast_u(o))-\u0026gt;th)) #define gco2upv(o) check_exp((o)-\u0026gt;tt == LUA_VUPVAL, \u0026amp;((cast_u(o))-\u0026gt;upv)) #define obj2gco(v) check_exp((v)-\u0026gt;tt \u0026gt;= LUA_TSTRING, \u0026amp;(cast_u(v)-\u0026gt;gc)) 创建 GCObject 可以通过调用 luaC_newobj 接口创建一个 GCObject 变量。它会首先为 GCObject 分配所需大小的内存，然后把 GCObject 的 marked 标记为白色，保存数据类型 tt，最后将自己加入到全局的 global_State 的 allgc 链表的最前面。\nnil nil 在 Lua 中表示空，因为没有值的缘故，它不需要在 Value 中有对应的字段，只要正确设置了它的类型 tt_ 即可。同时在判断值是否为 nil 的时候，直接判断类型即可。\n它的基础类型都是 LUA_TNIL，有三个变种类型，分别用在三种不同情况下。\nLUA_VNIL 的 variant 字段为 0，表示这个值是一个空值。 LUA_VEMPTY 的 variant 字段为 1，表示这是 table 中的一个空 slot 值。 LUA_VABSTKEY 的 variant 字段为 2，表示这是索引 table 中一个不存在的 key 的返回值。 boolean boolean 在 Lua 中有 true 和 false 两个值，这里也是没有保存值，而是直接把两种值保存为了两个变种类型。\n它的基础类型是 LUA_TBOOLEAN，变种类型有两种。\nLUA_VFALSE 的 variant 字段为 0，表示 false。 LUA_VTRUE 的 variant 字段为 1，表示 true。 number 数字的基础类型为 LUA_TNUMBER，有两个变种类型，分别代表了整数和浮点数。\nLUA_VNUMINT 的 variant 字段为 0，表示整数。 LUA_VNUMFLT 的 variant 字段为 1，表示浮点数。 数据保存在 union Value 中，整数使用的是 long long 类型的 i 字段，浮点数使用的是 double 类型的 n 字段。\nuserdata Lua 中的 userdata 分为两类，lightuserdata 和 userdata，其中 lightuserdata 是指内存不需要 Lua 管理的指针，userdata 是需要 Lua 负责管理内存的数据指针。\n在基础类型定义中，lightuserdata 被定义为了 LUA_TLIGHTUSERDATA，userdata 被定义为了 LUA_TUSERDATA，虽然分了两个基础类型，但是在 Lua5.4 中的注释里提到了，应该把 lightuserdata 看作是 userdata 的一个变种类型，只是为了兼容性才保留了两个基础类型。\nlightuserdata 因为不需要保存数据，所以只保存一个指针变量即可，上文提到的 union Value 中的 p 字段就是用来保存 lightuserdata 的指针的。\nuserdata 需要管理数据，相比 lightuserdata 它需要额外创建一个 Udata 变量用来保存自己的数据，同时它属于需要 GC 的类型，会通过 luaC_newobj 进行创建。\n1 2 3 4 5 6 7 8 9 10 11 12 /* ** Header for userdata with user values; ** memory area follows the end of this structure. */ typedef struct Udata { CommonHeader; unsigned short nuvalue; /* number of user values */ size_t len; /* number of bytes */ struct Table *metatable; GCObject *gclist; UValue uv[1]; /* user values */ } Udata; thread thread 在 Lua 中用来指代协程，所有协程的类型都是 LUA_TTHREAD，只有这一个基础类型。变种类型也只有一个 LUA_VTHREAD，它的 variant 值为 0。\n协程在 Lua 中是 lua_State 结构体的变量，由于它不仅仅是一个简单的数据类型，还跟 Lua 的虚拟机有很大关系，所以不在这里做展开了，后面会专门开一篇写这个。\nstring 字符串的基础类型为 LUA_TSTRING，有两个变种类型，分别代表了短字符串和长字符串，当字符串长度小于等于 40 个字节的时候，属于短字符串，反之则属于长字符串。\nLUA_VSHRSTR 的 variant 字段为 0，表示短字符串。 LUA_VLNGSTR 的 variant 字段为 1，表示长字符串。 结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 /* ** Header for a string value. */ typedef struct TString { CommonHeader; // struct GCObject *next; lu_byte tt; lu_byte marked lu_byte extra; /* reserved words for short strings; \u0026#34;has hash\u0026#34; for longs */ lu_byte shrlen; /* length for short strings */ unsigned int hash; // 哈希值 union { size_t lnglen; /* length for long strings */ struct TString *hnext; /* linked list for hash table */ } u; char contents[1]; // 保存字符串的内容 } TString; 短字符串与长字符串使用同一种结构体表示。除了所有 GCObject 都有的 CommonHeader 以外，extra 在短字符串中表示是否是保留字，在长字符串中表示是否已经计算过了哈希值。shrlen 表示短字符串的长度，hash 表示本字符串的哈希值，union u 中的 lnglen 表示长字符串的长度，hnext 用来当作短字符串哈希桶的链表指针，contents 保存了字符串的内容。\n全局字符串表 Lua 将短字符串进行了内化处理，相同内容的字符串只会保存一份。在全局的 global_State 中有一个哈希桶用来保存全部的短字符串指针，它的结构如下。\n1 2 3 4 5 typedef struct stringtable { TString **hash; // 哈希桶 int nuse; /* number of elements */ int size; // 长度 } stringtable; 它的 nuse 表示的是当前哈希桶中总共的元素个数，size 表示的是哈希桶的长度。\n全局字符串表会在 luaS_init 中被初始化，它的初始长度是 128，给哈希桶分配内存以后，会调用 tablerehash 填充哈希桶。\ntablerehash 是用来执行哈希桶的 rehash 的，在需要修改哈希桶的长度的时候就会执行，它会遍历之前所有字符串，把它们按新的长度重新计算在哈希桶中的位置。\n创建字符串 luaS_newlstr 是创建字符串变量的接口，不管长字符串还是短字符串都通过这个函数创建。它在实现的时候通过字符串长度判断，如果长度小于等于 40 则调用 internshrstr 创建一个短字符串，反之则调用 luaS_createlngstrobj 创建一个长字符串。\n不管是 internshrstr 还是 luaS_createlngstrobj，在处理完自己特化的工作以后，最后都会调用到 createstrobj 来创建一个字符串变量，它会首先计算出本变量所需的全部内存大小，然后调用上文提到的 luaC_newobj 创建一个 GCObject 出来，通过 gco2ts 把它转为 TString 结构，使用 hash 字段保存它的哈希值，把 extra 字段设为 0，并且会在 contents 最后补上一个 \u0026lsquo;\\0\u0026rsquo; 表示结束。\n短字符串 短字符串在创建的过程中，会修改全局字符串表，首先拿到 global_State 的哈希桶变量 strt，然后通过 luaS_hash 计算当前字符串的哈希值。这里会发现 global_State 中有一个随机种子变量 seed，它会参与到本进程中字符串哈希值的计算里。\n通过将字符串的哈希结果对哈希桶的 size 取模得到目标桶的位置，依次遍历桶中的数据，通过比较长度和数据，查找是否有跟目标字符串一样的字符串，如果找到了，直接返回它的指针即可。如果没有找到字符串，则通过 createstrobj 创建一个字符串变量，然后使用 memcpy 把待创建字符串复制进来。短字符串使用 shrlen 保存字符串长度，使用 union u 的 hnext 保存哈希桶中链表的下一个值。\n当需要删除一个短字符串时，除了需要处理内存回收，还要把全局哈希桶中的值也移除掉。移除的时候通过字符串的 hash 值计算出对应的链表，然后需要遍历链表查找字符串，找到以后从链表中移除，并且将哈希桶的 nuse 减一。\n长字符串 长字符串在 Lua 中当作普通的 GCObject 处理，在创建的时候，会把字符串的长度保存在 union u 的 lnglen 中。从这里可以看出 Lua 对内存使用的优化是很到位的，如果不是为了内存优化，完全可以把长字符串的长度也保存在 shrlen 中，这里跟短字符串的 hnext 共用一个 union，是为了可以把短字符串的长度类型变为 unsigned char 类型。\n长字符串在调用 createstrobj 创建 TString 时，不会计算哈希值，而是把 global_State 的 seed 保存在了 TString 的 hash 字段中。在 TString 创建完成以后，一样使用 memcpy 把待创建字符串的内容复制进来。\n长字符串的哈希值只有在需要使用的时候才会通过 luaS_hashlongstr 计算，比如说查找 table 的 key 的时候就会需要字符串的哈希值，计算方法与短字符串一样。在计算哈希之前长字符串的 extra 为 0，表示没有计算过哈希值，并且此时 TString 的 hash 中保存的是 global_State 的 seed，计算完以后，会将 hash 改为保存真正的哈希值，并且将 extra 改为 1 表示已经计算过哈希值，下次需要计算的时候，如果 extra 等于 1，则可以直接返回 hash 中的值了。\n比较字符串 在 lvm.c 的 luaV_equalobj 中可以看到，当双方都是短字符串时，比较使用的方法是 eqshrstr，当都是长字符串时，比较使用的是 luaS_eqlngstr 函数。\n因为短字符串是内化的，所以 eqshrstr 的实现很简单，直接比较两个参数的内存地址即可，相同短字符串都是引用的同一个 TString 变量，地址相同即相等，反之则不等。\n由于长字符串并没有进行内化，所以 luaS_eqlngstr 就麻烦一些，会先比较内存地址，如果不等的话，则比较长度，长度相等再使用 memecmp 比较内容。\n保留字符串 Lua 中有一些保留字符串，基本上都是关键字。保留字符串都是短字符串，TString 中的 extra 字段就是用来标识保留字符串的。\n保留字符串会在初始化的时候被全部创建，extra 会被顺序递增。同时所有保留字符串都会通过 luaC_fix 将其固定下来，永远不会被回收。\nproto proto 是 prototype 的简写，表示函数原型。它的基础类型是 LUA_TPROTO，只有一个变种类型是 LUA_VPROTO，它本身并不是 Lua 对外提供的一类值，但是它是单独的一类 GCObject，并且也是 function 变量的重要组成部分，所以在 function 之前先讲一下 proto 的结构。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 typedef struct Upvaldesc { TString *name; /* upvalue name (for debug information) */ lu_byte instack; /* whether it is in stack (register) */ lu_byte idx; /* index of upvalue (in stack or in outer function\u0026#39;s list) */ lu_byte kind; /* kind of corresponding variable */ } Upvaldesc; typedef struct LocVar { TString *varname; int startpc; /* first point where variable is active */ int endpc; /* first point where variable is dead */ } LocVar; typedef struct AbsLineInfo { int pc; int line; } AbsLineInfo; typedef struct Proto { CommonHeader; lu_byte numparams; /* number of fixed (named) parameters */ lu_byte is_vararg; // 是否用了可变参数 lu_byte maxstacksize; /* number of registers needed by this function */ int sizeupvalues; /* size of \u0026#39;upvalues\u0026#39; */ int sizek; /* size of \u0026#39;k\u0026#39; */ int sizecode; // code 的长度 int sizelineinfo; // lineinfo 的长度 int sizep; /* size of \u0026#39;p\u0026#39; */ int sizelocvars; // locvars 的长度 int sizeabslineinfo; /* size of \u0026#39;abslineinfo\u0026#39; */ int linedefined; /* debug information */ // 函数定义的起始位置行数 int lastlinedefined; /* debug information */ // 函数定义的末尾位置行数 TValue *k; /* constants used by the function */ Instruction *code; /* opcodes */ // 字节码保存的位置 struct Proto **p; /* functions defined inside the function */ Upvaldesc *upvalues; /* upvalue information */ ls_byte *lineinfo; /* information about source lines (debug information) */ AbsLineInfo *abslineinfo; /* idem */ LocVar *locvars; /* information about local variables (debug information) */ TString *source; /* used for debug information */ GCObject *gclist; } Proto; 可以看到 proto 的结构中数据还是比较多的，除了所有 GCObject 都有的 CommonHeader 以外，其余的数据大概可以分为几种类型。\n参数相关\n函数的参数可以包括若干个固定参数和一组不定参数，因为 Lua 规定了固定参数必须放在不定参数之前，不存在交叉的可能性，所以只需要使用 numparams 记录固定参数的个数，使用 is_vararg 来记录函数是否接受不定参数即可。 调试信息\ndebug 接口需要获得一些关于函数的调试信息，比如定义的位置等，这些也都在 proto 结构里。linedefined 记录了函数定义开始的行数，lastlinedefined 记录了函数定义结束的行数，source 记录了源文件的地址。不仅这三个，只要需要，其它变量也可以通过 debug 接口来读取。 upvalue\n函数使用的 upvalue 在一个数组 upvalues 中，同时使用 sizeupvalues 变量记录数组的长度。upvalue 有一个自己的结构体 Upvaldesc，每个 Upvaldesc 由名字 name，是否在栈中的状态 instack，索引 idx 和类型 kind 组成。 常量\n函数中使用到的常量会保存在 k 中，每个常量使用 Lua 值的 TValue 来表示。k 是一个数组，它的长度保存在 sizek 中。 指令码\ncode 中保存的是函数的指令码，是一个 Instruction 类型的数组，Instruction 本身是 Lua 中指令码的类型，是一个 unsigned int 类型。sizecode 是 code 的长度。 调用位置\n在 lineinfo 中记录了调用本函数的行数，sizelineinfo 记录了 lineinfo 的长度。 子函数\n函数中的子函数的原型记录在 p 中，是一个 Proto 指针的数组，它的长度在 sizep 里记录。 local 变量\n函数中的局部变量存放在 locvars 中，它是一个 LocVar 结构的数组，sizelocvars 保存了它的长度。每个 LocVar 变量又自己的变量名，开始生效的点和失效的点。 绝对行数\nabslineinfo 保存了绝对行数的信息，它是一个 AbsLineInfo 结构的数组，sizeabslineinfo 保存了它的长度。每个 AbsLineInfo 中保存了一个 pc 和一个行数。 function 函数的基础类型是 LUA_TFUNCTION，有三个变种类型。\nLUA_VLCL 表示 Lua 闭包，variant 值为 0 LUA_VLCF 表示轻量 C 函数，variant 值为 1 LUA_VCCL 表示 C 闭包，variant 值为 2 三种类型的变量都是使用同一个联合体 union Closure 来表示的。\n1 2 3 4 typedef union Closure { CClosure c; LClosure l; } Closure; Lua 闭包 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #define ClosureHeader CommonHeader; lu_byte nupvalues; GCObject *gclist typedef struct UpVal { CommonHeader; lu_byte tbc; /* true if it represents a to-be-closed variable */ TValue *v; /* points to stack or to its own value */ union { struct { /* (when open) */ struct UpVal *next; /* linked list */ struct UpVal **previous; } open; TValue value; /* the value (when closed) */ } u; } UpVal; typedef struct LClosure { ClosureHeader; // gc 相关字段 struct Proto *p; // 函数原型 UpVal *upvals[1]; /* list of upvalues */ } LClosure; 每个 Lua 闭包是一个 LClosure 变量，它主要包括了一个函数原型和一个 upvalue 的数组。这个 upvalue 是函数所在位置的 upvalue，要注意跟函数原型所在位置的 upvalue 区分。\nUpVal 中值保存的情况有两种，open 和 closed，open 代表的是原值没有被释放的情况，比如当子函数在父函数中运行时，父函数里 upvalue 就是 open 状态，在 open 状态下的 upvalue 并不会保存它的值，而是只使用 TValue *v 保存了一个指针。\nclosed 代表原值已经被释放的情况，比如子函数被当作返回值返回，然后在外部拿到返回值以后执行的情况，这时候父函数中的 upvalue 已经释放了，不能再保存指针了，要自己保存这个值，此时会被保存在 union u 的 TValue value 中，这也就是常见的可以用闭包实现计数器的原理。\nC 闭包 1 2 3 4 5 typedef struct CClosure { ClosureHeader; // gc 相关字段 lua_CFunction f; // 函数 TValue upvalue[1]; /* list of upvalues */ } CClosure; 不管是轻量 C 函数还是 C 闭包，都是使用 CClosure 变量表示。upvalue 全部是由 TValue 的值组成的，不存在用指针的情况。当闭包不需要 upvalue 时，也就是只有函数指针的时候，就是轻量 C 函数了。\ntable table 是 Lua 中唯一提供的数据结构，它方便易用，很强大，但是因为把 array 和 hashmap 和在了一起，所以在使用的时候也有很多需要注意的地方。\ntable 的基础类型是 LUA_TTABLE，它只有一个变种类型 LUA_VTABLE，variant 的值为 0。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 typedef union Node { struct NodeKey { TValuefields; /* fields for value */ lu_byte key_tt; /* key type */ int next; /* for chaining (offset for next node)*/ Value key_val; /* key value */ } u; TValue i_val; /* direct access to node\u0026#39;s value as a proper \u0026#39;TValue\u0026#39; */ } Node; typedef struct Table { CommonHeader; lu_byte flags; /* 1\u0026lt;\u0026lt;p means tagmethod(p) is not present */ lu_byte lsizenode; /* log2 of size of \u0026#39;node\u0026#39; array */ unsigned int alimit; /* \u0026#34;limit\u0026#34; of \u0026#39;array\u0026#39; array */ TValue *array; /* array part */ Node *node; // 哈希表 Node *lastfree; /* any free position is before this position */ struct Table *metatable; // 元表 GCObject *gclist; } Table; 由于在 table 中数组和哈希表的混用，所以在结构体中会包含这两类的数据。\n数组部分包括了 array 和 alimit 这两个值，array 是一个 TValue 结构体的数组，alimit 是它的长度上限，不一定是真实的长度。\n哈希表包括了 node，lsizenode 和 lastfree 三个值，node 是一个 Node 结构体的数组，lsizenode 是它的长度以 2 为底的对数值，这个值也说明了哈希表的长度一定是 2 的幂，lastfree 指向了哈希表最后一个位置。\nflags 是一个 unsigned char 类型的值，它里面的每一个 bit 对应了一种元方法是否存在，1 表示不存在，0 表示存在，同时 bit 7 保存了 alimit 是否是 array 的真实长度。\nmetatable 指向了该 table 的元表，因为元表也是一个 table，所以是一个 Table 结构体的指针。\n哈希表中的每个节点使用 Node 结构体变量表示，它主要分为了两部分，NodeKey u 和 TValue i_val 分别是键值对的键和值。\n创建 table 也是 GCObject，所以创建一样使用 luaC_newobj 即可。\n对象创建好以后，使用 gco2t 将其转化为 Table 结构的变量，然后将 matetable 设为 NULL，flags 的每一位都设为 1，array 设为 NULL，alimit 设为 0。\n还要初始化哈希表，因为一开始哈希表也是空的，所以 node 只需要指向一个占位用的数据即可，Lua 已经创建好了一个静态变量 dummynode 来做这个工作。同时 lsizenode 设为 0，且将 lastfree 设为 NULL，这个可以表示当前使用的是占位 node，方便后续修改。\n插入 可以从 C Api 的 lua_settable 入手看一下向 table 中插入一个值的操作。\n在设置一个值的时候会先尝试用 luaH_get 去取这个值，如果可以取到的话，则直接修改它就行了，如果取不到，则会首先把 slot 指向用来占位的 static 变量 absentkey，然后调用 luaV_finishset 新增一个值。\n在 luaV_finishset 中，会先通过检查 metatable 和 flags 判断 table 是否有 __newindex 元方法，如果没有的话，则使用 luaH_finishset 设置键值对，当 slot 是 absentkey 时，最后会使用 luaH_newkey 创建一个新键值对。\ntable 的哈希部分解决冲突使用的是开放定址法，有一个 mainposition 的概念，它是通过计算 key 的哈希值，与当前 table 的 lsizenode 取模的出来的。\n当通过 key 拿到 mainposition 以后把它保存在 mp 中，然后会判断 mp 是否为空，如果是，则直接把要存的值存在 mp 上即可。如果不是，则会拿 mp 的 key 再去计算一次 mainpositon 保存为 othern，这一步是要判断当前在本位制上的值是不是通过哈希计算来的。\n如果是通过哈希计算来的，那就是在本位置上发生了冲突，从 table 的 lastfree 上找到一个空余的点 f，将 f 的 next 设为 mp 的 next，然后将 mp 的 next 设为 f，也就是将 f 链接到了 mp 的后面，最后将值保存在 f 中即可完成。\n如果不是哈希计算来的，则说明它是通过 getfreepos 获取的，也就是说它占了我们现在要存的这个 key 的位置，这时候要将它移动到一个新的 freepos 中。因为它的 mainposition 是 othern，所以它现在的位置 mp 是一定在 othern 的 next 链上的，因为它之所以在这里，就是因为跟 othern 的位置冲突导致的。通过 othern 不断向后遍历找到 mp 的位置，然后将 othern 的 next 指向 f，将 *mp 复制到 *f 上，这样就完成了 mp 上的数据转移，然后将数据保存在 mp 上即可。\n上面完成的是键值对的新增，至于数组部分，当 key 是一个整数或者是可以转化为整数的浮点数时，在查找 key 的时候会检查 key 的范围，如果 key 在 [1, t-\u0026gt;alimit] 中，则直接返回 table 中 array 数组对应的值。如果当前 t 的 array 长度跟 alimit 不等的话，key 等于 alimit + 1，或者 key 小于 luaH_realasize 计算出的值，同样返回 array 中对应的值，并且把 alimit 设置为 key 的值。key 满足在数组部分的时候，直接修改即可。\n查询 从 lua_gettable 入手来看一下在 table 中查询一个指定 key 的值的操作。\n首先通过 luaH_get 来查找 key 对应的值，如果找到了，则直接返回即可。如果没有找到，则会调用 luaV_finishget，其中会检查 __index 元方法，如果没有该元方法的话，则直接返回 nil 即可。\n如果 __index 元方法是一个函数，则调用它，将结果返回。如果是一个 table 则再次使用 luaH_get 尝试返回元表中 key 对应的值。\n长度 Lua 中如果对 table 取长度，则只会拿到数组部分的长度，而且如果 table 是数组和哈希表混用的情况下，这个长度可能会有点问题。从 lua_len 入手来看一下取 table 长度的操作。\nlua_len 会调用 luaV_objlen 对变量取长度。当类型是 table 的时候，首先会取其 __len 元方法，如果有的话，调用它返回结果即可。如果没有的话，会调用 luaH_getn 来计算 table 的长度。\nluaH_getn 的逻辑不长，但是思路比较复杂，Lua 的源码中在函数头给出了 30 行的注释来描述这个函数的作用原理，我尝试把它的大意翻译出来。\n尝试找到 table t 的边界，边界是一个整数索引，例如 t[i] 存在但是 t[i+1] 不存在时 i 就是 t 的边界，或者当 t[1] 不存在时 0 就是 t 的边界，当 t[maxinteger] 存在时，maxinteger 就是 t 的边界。\n(1). 如果 t[limit] 为空，则它一定有一个在它前面的边界，这时候可以检查 limit - 1 是否存在，如果存在则它是 t 的边界。如果不存在，则在 [0, limit] 上做一个二分查找来寻找边界。这两种情况下找到边界以后都要更新 t 的 alimit 字段。\n(2). 如果 t[limit] 不为空，同时数组中有在 limit 之后的数据时，也可以找出 t 的边界。如果 t[limit + 1] 为空，则 limit 就是边界，否则检查数组部分的最后一个元素，如果它是空，则边界一定存在于 limit 和最后一个元素之间，通过二分查找找出来。\n(3). 最后一种情况是 t 的数组部分为空，或者它的最后一个元素存在。这两种情况下需要检查哈希部分。如果哈希部分也为空，或者 limit + 1 不存在，则 limit 为边界。否则通过 hash_search 查找在哈希部分的边界。\n长度计算非常复杂，所以在开发中永远不要在一个 table 里混用 array 和 map。我觉得 Lua 应该彻底分开这两个东西，混合这两类数据结构，表面上看是降低了门槛，其实我认为反而是提高了隐性的使用门槛。\n","date":"2022-03-25T02:18:47+08:00","permalink":"https://wmf.im/p/lua%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0%E4%BA%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","title":"Lua源码笔记（二）数据类型"},{"content":"　Lua 是一门小巧精炼的语言，虽然功能不太强大，但是该有的倒也都有，关键是代码量很少，只有不到两万行，非常适合通过阅读代码来学习带虚拟机的脚本语言的实现。本篇以 Lua-5.4.3 的源码为准，参考官方手册。\n库函数 Lua 的库函数的源码文件分布基本是按照官方手册索引页给出的分类进行划分的，有了索引页的结构做参照，代码分布就会比较清晰。\n基础库\n基础库就是索引中 base 下面的方法，提供的是最基础的功能，比如加载文件，打印输出这种功能。这部分的代码实现是在 lbaselib.c 中。 协程库\n索引中 coroutine 下面的部分是 Lua 所有的协程接口，协程是 Lua 中非常重要的组成部分，它们的实现在 lcorolib.c 中。 调试库\n索引中 debug 下面的部分是 Lua 所有的调试接口，这些接口有时在查 debug 的时候有很好的效果，不过有一些函数运行速度不太好，应该尽量避免在业务逻辑中使用调试库中的函数。调试库的实现是在 ldblia.c 中。 IO 库\nio 下面的函数是 Lua 提供的处理文件输入输出的接口，有两套风格的接口，一套使用隐式的文件句柄，另一套使用显式的文件句柄。IO 库的实现在 liolib.c 中。 数学库\nmath 库提供了一些基础的数学方法，Lua 提供的数学库不算很完善，倒也基本上够用，这部分的代码在 lmathlib.c 中实现。 os 库\n系统库中封装了提供了一些系统接口的封装，平时开发中用的最多的就是时间和日期相关的接口了，系统库的函数在 loslib.c 中实现。 package 库\npackage 库提供的是 Lua 的模块加载功能，其中 require 是直接导出到了全局变量中，其它的函数都在 package 表中。package 表中的函数在开发中不怎么常用，实现是在 loadlib.c 中。 字符串库\nstring 库中包含了所有针对字符串操作的函数，库中的函数不仅可以直接调用，同时也被设为了字符串元表的 __index 域，所以也可以通过面向对象的方式调用。这部分的实现在 lstrlib.c 中。 table 库\ntable 库中是所有针对 table 的操作，提供的操作其实比较少，不太够用，一般需要自己扩展一下。这部分的实现在 ltablib.c 中。 utf8 库\nutf8 库提供的是对 Unicode 编码数据的支持，除了特殊情况，开发中应该很少用到。这部分的实现在 lutf8lib.c 中。 C API\n由于 Lua 胶水语言的定位，所以提供了很多 C API 供 C/C++ 直接调用。在官方手册中，Index 下所有 C API 下的函数的实现都在 lapi.c 中实现。 辅助库\n辅助库也就是索引中 auxiliary library 下面的方法，文档中说辅助库是为用户提供一些编写 C 方法时常用的便利函数，相较于 C API 提供的函数来说，辅助库中的函数是 \u0026ldquo;higher-level functions\u0026rdquo;，这些函数都是在 lauxlib.c 中实现的。 数据类型 需要有自定义结构的数据类型，它们的结构定义都在 lobject.h 中，还有大量的与数据类型相关的宏方法也在其中。\n字符串\n字符串的结构定义在 lobject.h 中，此外还有 lstring.h 和 lstring.c 两个文件，实现了对字符串的创建等一系列底层操作。 函数\nLua 中的函数全部以闭包的形式存在，结构定义在 lobject.h 中，在 lfunc.h 和 lfunc.c 中还有一些针对闭包和函数原型的辅助函数。 表\ntable 除了结构定义在 lobject.h 中以外，其余的逻辑在 ltable.h 和 ltable.c 中，实现了包括 table 创建，修改等一些列操作。 虚拟机 虚拟机的 opcode 部分在 lopcodes.h 和 lopcodes.c 里面定义和实现，VM 中的操作在 lvm.h 和 lvm.c 里面定义和实现，语法解析在 lparser.h 和 lparser.c 里面定义和实现，词法解析在 llex.h 和 llex.c 中定义和实现。\n每个进程中有一个 global_state 和很多个 lua_State 变量，这两个的定义和实现在 lstate.h 和 lstate.c 中。\n其它 保存 chunk\n把脚本编译成字节码保存在二进制文件里的实现在 ldump.c 中。 初始化库\n库的加载方法在 linit.c 中实现。 内存分配\n内存分配的接口在 lmem.h 和 lmem.c 中。 GC\nGC 方法的定义和实现在 lgc.h 和 lgc.c 中。 元方法\n元方法相关的内容在 ltm.h 和 ltm.c 中。 字符类型\n在 lctype.h 和 lctype.c 中实现了一套通过打表快速判断字符类型的方法，比如判断字符是否是空格，是否是数字。这个用在词法解析阶段。 ","date":"2022-03-24T02:18:47+08:00","permalink":"https://wmf.im/p/lua%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0%E4%B8%80%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84/","title":"Lua源码笔记（一）代码结构"},{"content":"　程序员基本上离不开科学上网了，clash 是我感觉体验最好的科学上网软件了，就是相比之下有点吃性能，在路由器上跑的话很多 ARM 架构的路由器都跑不利索，不过 x86 的 CPU 跑到带宽上限一般还是没问题的。\nGUI客户端 如果有条件使用 GUI 客户端，那事情就非常简单了。\n不管 Windows 还是 Linux 下，都有 clash_for_windows_pkg 可以用。Windows 版本支持非常全面，Bug 也很少，不过 Linux 版本就一般了，Bug 不少，而且修复的比较慢，只是堪堪能用。\nAndroid 平台下可以使用 ClashForAndroid，很稳定，功能也比较齐全。\n命令行客户端 下载配置 要使用 clash 需要两个文件，第一个是 clash 的配置文件 config.yaml，另一个是用于判断主机位置的 Country.mmdb 文件。\n第一个文件从服务提供商那里下载，下载以后重命名成 config.yaml 即可。第二个文件可以从 maxmind-geoip 下载最新的一版即可。\nconfig.yaml 中可能有一些参数需要修改，主要涉及四个配置，将它们修改成你需要的即可。port 和 socks-port 可以不再配置，只配置 mixed-port 即可。\n1 2 3 4 port: 7890 # http 代理端口 socks-port: 7891 # socket 代理端口 mixed-port: 7892 # 混合端口，http 和 socket 都可以用 external-controller: 0.0.0.0:9090 # 外部控制 http 服务监听的地址和端口 external-controller 要注意一下，它是给外部控制服务提供的端口，后面会用到它。它默认可能是 127.0.0.1:9090，绑定的地址要按需修改，如果无所谓或者不知道要怎么改，那么使用 0.0.0.0:9090 即可，它会使这个服务在本机和外部都可以访问。\n使用 Docker 部署 创建一个文件夹，将上述的两个配置文件放在里面，然后再在这个文件夹下创建一个 Dockerfile 文件来创建镜像。\n1 2 3 4 ./clash-core ├── clash.Dockerfile ├── config.yaml └── Country.mmdb 因为 docker hub 的仓库中已经有 clash 的作者创建好的一个镜像了，所以可以简单的以这个镜像为基础，将配置文件复制到容器中即可。\n1 2 3 4 5 # syntax=docker/dockerfile:1 FROM dreamacro/clash-premium:latest COPY config.yaml /root/.config/clash/ COPY Country.mmdb /root/.config/clash/ EXPOSE 7892 9090 在本目录中使用一行命令打包生成镜像。\n1 docker build -f ./clash.Dockerfile -t clash:my . 生成好镜像以后，创建容器并执行。\n1 docker run -d -p 7892:7892 -p 9090:9090 --rm clash:my 二进制部署 如果觉得自己做 Docker 镜像太麻烦了，也可以直接使用打包好的二进制程序部署。在 clash 下载适合自己使用的 clash 执行程序。新式 CPU 可以下载 linux-amd64-v3 版本，旧式的 CPU 下载 linux-amd64 版。v3 版本可以使用更多的指令集。\n1 2 3 4 sudo mkdir /etc/clash sudo cp clash /usr/local/bin sudo cp config.yaml /etc/clash/ sudo cp Country.mmdb /etc/clash/ 创建 systemd 的配置文件 /etc/systemd/system/clash.service :\n1 2 3 4 5 6 7 8 9 10 11 [Unit] Description=Clash daemon, A rule-based proxy in Go. After=network.target [Service] Type=simple Restart=always ExecStart=/usr/local/bin/clash -d /etc/clash [Install] WantedBy=multi-user.target 执行下面的命令让服务启动：\n1 2 3 sudo systemctl enable clash sudo systemctl start clash sudo systemctl status clash 部署网页控制台 可以使用网页控制台来控制 clash 服务的配置。有很多可以直接使用的控制台程序，比如由 clash 的作者开发的 clash-dashboard 和广受好评的 yacd 都可以提供比较完善的功能。\n这些网页控制台可以直接访问作者部署好的地址来使用，但是一般速度堪忧，比如 yacd，所以还是自己部署比较方便。因为有了 docker 所以部署变得非常简单，以 yacd 为例，安装好 docker 以后只需要一行命令即可在本地部署完成。\n1 docker run -p 1234:80 -d --rm haishanh/yacd 接下来只要访问 xxx.xxx.xxx.xxx:1234 就是本地的 yacd 服务了。然后在连接上填写上自己的 clash 配置文件中配置的外部控制端口即可使用了。\n使用代理 由于 clash 是自带分流的，不需要在使用的地方进行二次地址过滤，所以使用起来非常简单，直接修改系统代理即可。\nWindows 和 Linux 直接修改系统代理指向目标地址和端口即可，移动设备也可以在 WIFI 设置里填写代理的地址和端口。\n如果不希望修改系统代理，则可以修改浏览器的代理，比如 Firefox 可以直接在 “设置” 的 “网络设置” 里填写代理的设置。Chrome 系的浏览器可能不支持，需要使用 Proxy SwitchyOmega 之类的软件设置代理。\n","date":"2022-03-19T03:43:08+08:00","permalink":"https://wmf.im/p/clash%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/","title":"Clash使用笔记"},{"content":"　Git 是当之无愧的最流行版本控制软件，虽然相比 SVN 有一些复杂，但是也有很多优点。本篇用于记录在使用 Git 中的心得。\n设置代理 在 github 设置部署代码的时候，会非常非常慢，成功率完全随缘。Git 支持针对指定网站的代理设置，非常方便。\nHTTPS 协议 不管在 Windows 还是 Linux 下，都可以通过在用户目录的根目录下，创建一个 .gitconfig 的文件来配置 Git 的代理。\n1 2 3 [http] [http \u0026#34;https://github.com\u0026#34;] proxy = http://x.x.x.x:xxx SSH 协议 如果使用 SSH 协议进行 git 操作的话，则需要设置 ssh 程序的代理。在用户的根目录下，ssh 程序会创建一个 .ssh 目录，在 Linux 下是隐藏的。其中的 config 是 ssh 客户端的配置文件，可以增加配置对指定主机设置是否使用代理。\n1 2 3 Host github.com User git ProxyCommand connect -H xxx.xxx.xxx.xxx:1234 %h %p 上面是 Windows 平台下的配置，但是 Linux 下没有 connect 这个程序，应该使用 nc 程序来连接代理服务。\n1 2 3 Host github.com User git ProxyCommand nc -X 5 -x xxx.xxx.xxx.xxx:1234 %h %p -X 5 表示使用了 socks 5 版本的代理协议。\n设置用户 Git 可以在配置中指定当前使用的用户名和电子邮箱地址，设置过以后就不需要每次操作的时候都输入了，设置的位置同样是在用户根目录下的 .gitconfig 中。\n1 2 3 [user] name = xxx email = xxx@xxx.com Github 配置密钥 通过在 setting → SSH and GPG keys 中增加一个 ssh 的公钥即可实现密钥的配置。如果 ssh 本身的密钥配置没有问题的话，在设置过 Git 的用户以后，即可顺利实现对 Github 仓库的无密码访问。\n默认分支 由于国外轰轰烈烈的 Black Lives Matter 运动，导致了 Github 将自己默认仓库的分支从 master 改为了 main，但是 Git 本身继续使用 master 作为默认分支。\n这就导致了一个问题，如果是在本地通过 git init 创建的分支，又关联了 Github 上创建好的一个远程仓库，就会发现这两个仓库的默认分支不一样，需要把本地的仓库默认分支从 master 改为 main 来适应 Github 的修改。\n也可以在 Github 的 setting → repositories 中设置默认分支的名字。\nTortoiseGit 配置 ssh 工具 Git 的 GUI 工具里，Win 平台下最好用的就是小乌龟了。不过小乌龟有个问题，它是用的自带的 ssh 工具，这样没办法使用配置好的 ssh 密钥，必须再使用它自带的工具生成一个 ppk 格式的密钥来专门给它自己用，比较麻烦。\n可以通过修改小乌龟的设置，让它使用自带的 ssh 软件，这样可以直接用上命令行配置好的密钥。通过修改 setting → network → ssh client 的软件指向已经安装的 git → usr → bin → ssh.exe 即可完成设置。\n","date":"2022-03-19T03:38:47+08:00","permalink":"https://wmf.im/p/git%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/","title":"Git使用笔记"},{"content":"　Lua 脚本的热更新是快速开发和不停服修复 bug 的重要功能，基本上每个基于 Lua 的框架都会提供。但是因为 skynet 开启了众多的 LuaState 的特殊性，所以它的脚本热更新会跟别的框架有很大区别。本文会尝试讲解 skynet 中热更新的操作。\nclearcache 通过 debug 后台的 clearcache 指令可以清空进程的 CodeCache 缓存。更新本地文件，并且清空 CodeCache 的操作有时会被用来作为 skynet 热更新的方法。但是由于 CodeCache 清空时并不会修改旧的引用，所以旧的服务并不能用到更新，只有新启动的服务可以用到新的内容。\n因为这样的特性，导致了这种方法使用范围比较窄。如果服务本身就是不断创建和销毁的，那就可以直接用，旧的服务会按照以前的逻辑执行完毕，新的服务会直接用新的逻辑。比如像一些副本之类的功能可能就会这样设计，每个副本都对应了一个为它提供支持的服务，副本开始时创建服务，结束时销毁服务。\n还有一类服务也可以用这种方法热更新，就是无状态的服务。虽然旧的服务没办法被更新，但是因为是无状态服务，所以可以直接启动等量的新服务，然后让别的服务都把消息发送到新服务，旧的服务销毁即可。但是 skynet 并不提供相关的支持，要自己实现这类功能。\nsharetable sharetable 是 skynet 提供的一种可以在不同 LuaState 中共享只读数据的操作，一般在项目中用于共享策划配置表数据。\nsharetable 中的数据可以进行热更新，针对同一份文件再次执行 loadfile 即可重新加载该文件，然后要在所有需要更新数据的服务里执行 sharetable.update 实现数据更新。\ninject inject 是 skynet 提供的可以用于热更新的后台指令，它的特点比较鲜明，用起来比较麻烦，但是理论上可以覆盖所有情况。\ninject 会把该服务的 skynet.dispatch_message 和 skynet.register_protocol 这两个函数所有的 upvalue 作为 env 传给通过 load 加载的注入脚本，_U 是这两个函数的 upvalue，_P 是按协议类型分类的分发函数的 upvalue，想要替换任何函数或者是 upvalue 都需要自己在脚本中处理。\n这里给出一个尽量简单的例子，用来修改 example 里的 simpledb 服务，将 GET 改为每获取一次为原 value 后面追加一个 \u0026ldquo;A\u0026rdquo; 字母。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 local function getupvalues(func, target) if not func then return end local i = 1 while true do local name, value = debug.getupvalue(func, i) if name == target then return value end i = i + 1 end end local command = _P.lua.command local db = getupvalues(command.GET, \u0026#34;db\u0026#34;) command.GET = function (key) db[key] = db[key] .. \u0026#34;A\u0026#34; return db[key] end print \u0026#34;inject success!!\u0026#34; 首先使用任意 telnet 程序连上 skynet 的 debug 后台，这里要注意一下如果在启动 debug_console 时不指定监听地址的话，debug_console 默认监听的地址是 127.0.0.1，这样的话不能从远端连接该端口。\n输入 list，让 skynet 列出当前进程所有服务的地址。然后使用 inject 命令，指定目标服务的地址和要注入的脚本地址即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ nc 127.0.0.1 8000 Welcome to skynet console list :01000004 snlua cmaster :01000005 snlua cslave :01000007 snlua datacenterd :01000008 snlua service_mgr :0100000a snlua protoloader :0100000b snlua console :0100000c snlua debug_console 8000 :0100000d snlua simpledb :0100000e snlua watchdog :0100000f snlua gate \u0026lt;CMD OK\u0026gt; inject :0100000d examples/injectsimpledb.lua inject success!! \u0026lt;CMD OK\u0026gt; 成功注入以后，启动客户端，向 simpledb 发送 GET 命令。可以看到成功返回，并且每次返回的值都在原来的值后面追加了一个 \u0026ldquo;A\u0026rdquo; 字母。\n1 2 3 4 5 6 7 8 hello result worldA hello result worldAA hello result worldAAA hello result worldAAAA ","date":"2022-03-14T23:12:36+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%8D%81%E5%85%AB%E7%83%AD%E6%9B%B4%E6%96%B0%E6%93%8D%E4%BD%9C/","title":"skynet源码分析（十八）热更新操作"},{"content":"　因为 skynet 中的每一个 snlua 服务都运行在一个独立的 LuaState 中，所以就存在了数据共享的问题。因为 Lua 本身是不支持跨 LuaState 分享数据的，所以 skynet 通过修改 Lua 实现了这一功能，不过有一些局限，只能共享只读数据。本篇会尝试分析这一功能的实现。\n服务 使用 sharetable 的时候会启动一个进程唯一的服务 sharetable，它的内容在 sharetable_service 这个函数中，提供了对共享数据加载，查询，关闭的功能。\n加载 ShareTable 可以通过三种方式加载数据，文件，table 和字符串，不管是什么途径，都要传入一个 filename 来作为文件的标识。因为一般 ShareTable 都是用来存策划配置表的，所以用文件加载的时候最多。\nskynet 中把加载过的共享文件的结构叫做 matrix，在 Lua 层有两个 table 用来管理 matrix 的映射，table matrix 保存的是指针地址到文件被引用信息的映射，table files 保存的是文件名字到 matrix 的映射。\ncore.matrix 是加载共享数据的最终入口函数，执行的时候会为每一份数据创建一个新的 LuaState 来加载和保存文件。文件加载用的是 Lua 自带的 luaL_loadfilex_ 进行，只是有一个关键的步骤，会调用 skynet 在 Lua 中增加的 mark_shared 函数，给加载后的 table 打上 G_SHARED 的标志，并且会递归的给 table 中所有需要 gc 的值都打上 G_SHARED 标志。所有共享的数据，在 gc 过程中都不会被标记和回收。\ncore.matrix 返回的是一个 userdata，也就是 matrix 了，结构是 state_ud，可以看到 state_ud 中只有一个 lua_State 指针，指向的就是为加载共享数据创建的那个 LuaState 变量。filename 到 matrix 的映射关系会被保存在 table files 中。\n1 2 3 struct state_ud { lua_State *L; }; 返回值会被设置一些元表项，close 用来关闭 LuaState 停止分享，getptr 用来获取 matrix 的指针，size 用来获取 LuaState 的内存使用量。\n查询 查询通过的是 sharetable.query 进行，需要指定要查询的 filename，首先在 table files 中查找对应的 matrix，如果没有则说明没有加载过，直接返回即可。\n如果有记录的话，则尝试获取文件的被引用信息，累加文件的被引用计数，并且把引用的源服务地址加入到 table clients 中，然后通过 matrix 的 getptr 拿到其指针，将它返回给源服务，源服务拿到指针以后，会调用 clone 方法将共享的 table 的引用复制到本 LuaState 中。\n更新 要更新 sharetable 首先要重新 load 一次新文件，在 load 同一个文件时，会检查之前是否有过加载的记录，如果有的话，会清掉之前的记录，然后再加载。\n在执行过新的加载以后，还要在使用这张表的服务里调用 sharetable.update 才行。update 会首先重新 query 一次，拿到新的共享表数据。然后对新数据和旧数据进行一次递归对比，找出所有不一样的数据，然后遍历整个 LuaState 中的数据，替换掉全部的旧值。\n字符串处理 这样实现会有一个问题，就是字符串的比较问题。在新版的 Lua 中，短字符串还是内化的，也就是两个相同内容的短字符串是指向的同一份数据。根据这个实现又对字符串的比较做了一个优化，就是只比较地址即可。\n但是因为 sharetable 是在别的 LuaState 里面的，所以就会出现本 LuaState 中的某个字符串与 sharetable 中某个相同字面量的字符串不等的问题。\n为了解决这个问题，skynet 对 Lua 进行了一些改动，在 string 的类型 TString 中加了一个 id 来辅助比较。普通字符串默认都是 0，每个被共享化的字符串都会被赋予一个正整数 id。\n加上 id 以后的字符串比较方式变为了，首先比较地址，如果不同的话，比较 id，如果 id 不为 0 且相等，也可以直接判定字符串相等。如果 id 的比较也不相等的话，则需要老老实实调用 memcmp 进行一次完整比较，如果完整比较相等的话，会把较小的 id 改为较大的 id，这样下一次比较的时候就可以通过 id 直接判断出相等了。\n","date":"2022-03-12T05:42:49+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%8D%81%E4%B8%83sharetable%E7%9A%84%E5%AE%9E%E7%8E%B0/","title":"skynet源码分析（十七）ShareTable的实现"},{"content":"　skynet 在默认情况下使用了 jemalloc 进行内存分配，这种情况下 skynet 提供了自定义的内存管理函数，本篇会尝试讲解这部分功能的实现。\n接口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #define skynet_malloc malloc #define skynet_calloc calloc #define skynet_realloc realloc #define skynet_free free #define skynet_memalign memalign #define skynet_aligned_alloc aligned_alloc #define skynet_posix_memalign posix_memalign void * skynet_malloc(size_t sz); void * skynet_calloc(size_t nmemb,size_t size); void * skynet_realloc(void *ptr, size_t size); void skynet_free(void *ptr); char * skynet_strdup(const char *str); void * skynet_lalloc(void *ptr, size_t osize, size_t nsize); void * skynet_memalign(size_t alignment, size_t size); void * skynet_aligned_alloc(size_t alignment, size_t size); int skynet_posix_memalign(void **memptr, size_t alignment, size_t size); 这一套接口跟标准库的接口命名完全一样，只是在名字前面加了 skynet_ 的前缀，同时参数和功能也是完全一样的。\n这里有一个小技巧，除了声明了这些函数以外，每个接口还有一个同名的 define 定义，如果加了编译选项 NOUSE_JEMALLOC 的话，则这些函数声明都不会有实现，这时候调用这些函数实际就是使用的同名的 define 定义。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 static ATOM_SIZET _used_memory = 0; static ATOM_SIZET _memory_block = 0; struct mem_data { uint32_t handle; ssize_t allocated; }; struct mem_cookie { uint32_t handle; #ifdef MEMORY_CHECK uint32_t dogtag; #endif }; #define SLOT_SIZE 0x10000 static struct mem_data mem_stats[SLOT_SIZE]; 上面是内存数据相关的记录，_used_memory 是本进程内通过 skynet_malloc 分配的所有内存数量，_memory_block 是本进程内通过 skynet_malloc 分配的内存块数，跟分配次数相等。\nmem_data 是针对服务的，每个服务都会有一个 mem_data 结构的变量在 mem_stats 里面保存。handle 用来保存服务的 handle，allocated 用来保存该服务已分配的内存字节数。这里有个小问题，就是 SLOT_SIZE 是小于 handle 的增长上限的，而且数组长度也不会增长，所以并不是所有服务都可以统计分配的内存数量。\nmem_cookie 是每次分配内存的时候都会在分配出的内存块后面加上这样一块后缀数据。如果不开 MEMORY_CHECK 的话，也会记录，不过只会有 handle 的记录。\n实现 分配 分配的接口 skynet_malloc 实现很简单，只有三步操作。首先需要调用 je_malloc 分配一块需求长度 + 后缀长度的内存。然后检查分配结果，如果分配失败则触发 OOM 处理输出错误信息，执行 abort 结束进程。如果分配成功，则为内存块填充后缀，填充完毕以后修改上面提到的各种数据计数。\n如果要使用 skynet_realloc 进行内存的重分配的话，也不会太麻烦，相比 skynet_malloc 只是多了一步要先清除就的后缀的操作。\n释放 调用 skynet_free 可以释放内存。释放内存的主要额外步骤就是要清除后缀了。清除后缀其实并不是真的为了清除后缀，因为后缀不用清除，直接释放即可。\n清除后缀真正做的事情是为了修改分配的是计数，这里要先通过后缀拿到服务的 handle 才行。拿到以后即可将对应的内存分配数据减少。\n检测 skynet 提供了一个简单的内存检测功能，可以通过定义 MEMORY_CHECK 开启。提供了 double free 和 out of bound 的检测。\n检测的原理主要是通过检测 mem_cookie 的 dogtag 的值来判断的。在内存分配的时候，如果开启 MEMORY_CHECK 的情况下，dogtag 会被赋值为 MEMORY_ALLOCTAG，其余并无额外的操作。\n在内存释放的时候，如果 dogtag 不是 MEMORY_ALLOCTAG，则说明之前的 dogtag 被覆盖掉了，则可以看作是内存越界写入了。如果是 MEMORY_ALLOCTAG 则将 dogtag 设为 MEMORY_FREETAG 来标记它已经被释放了。同时在一开始也会检查 dogtag 是否是 MEMORY_FREETAG，如果是的话，则可以说明存在二次释放的问题。\n","date":"2022-03-02T19:24:25+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%8D%81%E5%85%AD%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","title":"skynet源码分析（十六）内存管理"},{"content":"　CodeCache 应该算是 skynet 的一个特色功能了，是用来优化不同 LuaState 加载同一份代码的速度。因为 skynet 大量启动了 LuaState 来作为 Acotr 使用，所以才在重复加载代码上需要这样的优化。本篇会尝试分析 CodeCache 的实现。\n作用 首先要了解 CodeCache 解决的是什么问题。简单来说，是在文件第一次加载的时候，将其从磁盘中读取出来，解析完毕以后，把函数原型缓存在一个独立的 LuaState 中。之后其它 LuaState 有加载需求时首先会尝试从缓存中加载，并且函数原型 Proto 可以直接共享缓存中的数据。\n缓存的加入可以达到减少文件 IO，减少代码解析步骤，减少函数原型的内存占用的目的。云风大佬有一篇博客很详细的写了构思过程和要达到的目的。\n实现 结构 用来从磁盘加载代码并且缓存加载结果的结构是 codecache，它的结构很简单，有一个 lua_State 用来加载和保存结果，还有一个自旋锁，因为加载很可能是并行的，所以修改的时候需要加锁。\n1 2 3 4 5 6 struct codecache { struct spinlock lock; lua_State *L; }; static struct codecache CC; 保存 Lua 加载文件的最终入口是 luaL_loadfilex，skynet 特供版的 Lua 就是通过修改了这个函数来实现的 CodeCache 功能。\n为了让加载的过程实现线程安全，每次需要加载的时候，skynet 都会创建一个新的 LuaState 进行文件加载。加载完毕以后的 Lua 函数原型 Proto 通过 lua_topointer 拿到地址，并且通过函数 save 把它的指针保存在 CC.L 里面。这个为了加载文件创建出的临时 LuaState 并不会被关闭，在整个进程的生命周期中都存在。\nsave 会首先检查 CC 有没有被初始化，如果没有的话会调用 init 来初始化 CC 中的 L 变量。保存的时候会用文件名作为 key 来保存对应的 proto 变量的指针，存在了 CC.L 的 LUA_REGISTRYINDEX 表中。\n加载 修改以后的 luaL_loadfilex 会在一开始的时候首先执行 load 尝试从 CC.L 中加载出之前保存过的文件名对应的 proto 变量。\nload 的执行过程比较简单，先锁住 CC 的锁 lock，然后在 LUA_REGISTRYINDEX 表中按文件名尝试读取 proto 结构体。\n如果读取到了，则可以通过 lua_clonefunction 在源 LuaState 中根据已有的 proto 创建一个 closure 出来。在新创建的 LClosure 结构体变量中，共享的部分就是结构体中 proto 的部分，这部分会直接指向在缓存 LuaState 中读取到的指针，从而可以节约掉这部分数据本来需要占用的内存。\n共享模式 CodeCache 一共有三种共享模式，分别是 OFF/EXIST/ON 模式。共享模式是每个 LuaState 独立的，可以根据本服务的特性来设置。\nOFF 模式会关闭共享，不管什么情况，都自己加载自己使用，既不复用之前的缓存，也不新增缓存。EXIST 模式只会加载已经存在的缓存，如果不存在，则自己加载自己使用，不会新增缓存。ON 模式是默认模式，会先尝试加载之前的缓存，如果不存在缓存的，会从文件中读取，并且新增缓存。\n可以通过 Lua 层的接口 codecache.mode 设置当前 LuaState 的共享模式。C 层接口cache_level 可以用来获取 LuaState 的共享模式。\n清空缓存 CodeCache 加载过的缓存会在本进程的生命时间里一直存在。不过 skynet 提供了一个可以清空所有缓存的接口，一般用在特殊场景的热更新操作中。\nLua 层的接口 codecache.clear 可以清空当前的所有缓存，同时在 debug 后台里也有一个命令 clearcache 支持调用 codecache.clear 清空缓存。\n清空缓存的实现很简单，因为缓存都在 CC.L 里面放着，所以锁上 CC 的锁以后，直接调用 lua_close 关闭掉 CC.L，之后再给 CC.L 创建一个新的 LuaState 即可完成清空。\n可以看到清空缓存其实清掉的只是函数原型缓存的指针而已，并没有释放掉原来的缓存，只是当下一次加载一个文件的时候，会重新生成缓存而已，之前的服务还是引用的旧的函数原型，既不会更新也不会失效。\n","date":"2022-03-02T19:22:28+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%8D%81%E4%BA%94codecache%E7%9A%84%E5%AE%9E%E7%8E%B0/","title":"skynet源码分析（十五）CodeCache的实现"},{"content":"　skynet 中有不少关键功能都是依赖 Lua 协程实现的，skynet 本身也对 Lua 协程有一些管理。本篇会尝试分析 skynet 中对 Lua 协程的使用方法。\n协程池 一般基于 Lua 的框架都会提供协程的复用，复用可以减少不断创建和销毁协程的开销。协程池是大部分框架的选择，在 skynet 中也是通过协程池实现的协程复用。\n协程池的结构就是一个 table 而已，被当作队列来使用，需要协程时通过 table.remove 从队首取出，用完以后放在队尾。协程池一开始是空的，并不会初始化一些协程备用。\n创建协程 在 skynet 中创建协程的接口是 co_create(func)，它会首先尝试从协程池中拿取一个协程，如果拿不到，会自己创建一个协程出来。\n因为协程在用完以后要放回协程池中，所以在创建的时候包裹中的函数除了要执行参数 func 以外，还需要一个无限循环。循环中可以接受新的函数，并且再次执行。\nskynet 通过使用 yield 和 resume 配合，实现了给一个创建好的协程传入要执行的函数。在协程的循环中，协程 yield 了两次，第一次 yield 等待目标函数的传入，第二次 yield 是等待第一次传入的函数的参数。co_create 的调用会把目标函数当作参数对协程调用 resume 完成第一步，并且返回协程，等待第二次调用。\n1 2 f = coroutine_yield \u0026#34;SUSPEND\u0026#34; f(coroutine_yield()) 协程与消息 协程在 skynet 中的大部分使用场景都是用来处理消息的。skyent 中有几个 table 用来记录协程和消息的关系，其中 session_id_coroutine 是 session 到 协程的映射，session_coroutine_id 是协程到 session 的映射，session_coroutine_address 是协程到消息的源服务地址的映射。\n在处理接收到的消息时，会感觉消息类型获取其 dispatch 函数，并且为 dispatch 函数创建一个协程。创建以后会在 session_coroutine_id 和 session_coroutine_address 增加数据。当协程被 resume 执行完消息处理函数之后，协程的循环中会删除掉 session_coroutine_id 和 session_coroutine_address 的记录，并且把协程重新投入协程池中去。\n在发送消息时，如果是通过 skynet.send 发消息的话，不需要做记录，如果是通过 skynet.call 发消息的话，需要向 session_id_coroutine 中新增记录，保存 session 到协程的映射。有了这个映射关系，在处理回复的消息的时候，才能根据 session 拿到在等待它返回的协程，然后清除掉 session_id_coroutine 中的记录，并且把返回的消息交给等待的协程处理。\nsession_coroutine_id 还有一个作用，可以判断是不是忘记回复消息的源服务了。如果在处理消息的时候使用 skynet.ret 回复了源服务的话，则会清掉 session_coroutine_id 中的数据。在协程的循环中，当消息的处理函数调用结束以后，会检查 session_coroutine_id 中本协程对应的 session，如果还没有清掉并且不为 0 的话，则说明这是一条需要回复但是并没有进行回复的消息。\nfork skynet.fork 可以创建一个协程来执行一个函数，一般用在逻辑中需要调用阻塞接口，但是又不想阻塞在这里的情况，比如需要连续多次的 call 调用的情况就可以为每个 call 调用创建一个协程。另外在通过循环 sleep 实现连续的定时调用时也可以用 fork 来包裹需要不断调用的函数。\n它的实现就是用 co_create 创建了一个协程，包裹住目标函数。然后将其投入到了一个叫做 fork_queue 的队列中去，等待被调用。\nfork_queue 的调用时机其实很快，就在处理完本条消息以后，skynet.dispatch_message 就会为 fork_queue 中所有等待的协程执行 resume 来处理它。\n云风说可以把 skynet.fork 当作是更加高效的 skynet.timeout(0) 来看待，通过实现也可以看出，fork 确实比 timeout 要省略了很多步骤。\n睡眠和唤醒 skynet.wait 和 skynet.sleep 都可以让出当前协程，都是需要传入一个 token 来标识挂起的协程。区别是 skynet.sleep 需要指定一个唤醒的定时，如果在到期之前还没有被 skynet.wake 唤醒的话，则会由 timer 线程发送消息来唤醒。skynet.yield 也可以让出当前协程，不过它只是对 skynet.sleep(0) 的简单封装，这里不再赘述。\nwait 和 sleep 的实现没太大区别，最主要的区别是 session 的获取。wait 是直接调用 session 分配接口获得的，sleep 则是注册定时事件，通过定时事件返回 session 获得的。它们都通过同一个接口 suspend_sleep 将自己阻塞，并且把 session 和协程的映射关系保存在了 session_id_coroutine 中，把 token 和 session 的映射关系保存在了 sleep_session 中。\nskynet.wakeup 可以唤醒这两种睡眠的协程，包括还未到期的通过 sleep 接口睡眠的协程。但是逻辑并不在这个函数中，它负责的只是把要唤醒的协程的 token 插入到了 wakeup_queue 中。\nwakeup_queue 中等待唤醒的协程会在消息的处理协程挂起以后，通过 dispatch_wakeup 根据 wakeup 的调用顺序被依次唤醒。\n","date":"2022-02-26T18:12:59+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%8D%81%E5%9B%9B%E5%8D%8F%E7%A8%8B%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"skynet源码分析（十四）协程的使用"},{"content":"　skynet 的定时器采用了时间轮设计，可以高效的处理定时调用，本篇会尝试理清楚定时器的设计思路和实现方法。\n时间轮算法 时间轮是实现高效率定时器的常见算法。虽然经常被形容的很高大上，但是其实时间轮非常好理解，因为它就是取自生活中的例子。\n一个简单的例子，如果要在 2022 年 2 月 22 日 22 点 22 分 22 秒做一件事情，那么首先会看年份是不是对应，年份对应了再看月份，月份对应了再看几号，一级一级的向下看，直到秒也可以对应的上，这时候就是要做这件事情了。\n可以看到如果是参考现实计时法的时间轮的话，那实现的其实就是一个类似闹钟的功能。当有需要定时执行的事件时，将它加入到目标时间对应的事件队列中。然后每秒执行一次时间累计，不断累加时间。每次累加时间以后，检查当前时间对应的事件队列，如果不为空，则依次执行。\n与另一种常见的定时器实现算法最小堆比较，时间轮算法最大的好处是其加入一个新定时事件时的复杂度是 O(1) 的，删除一个事件虽然不是 O(1) 但是大概率也比最小堆要快一些。\n用法 skynet 只提供了创建一个定时事件的接口 skynet.timeout，并没有取消这个事件的接口。通过调用 skynet.timeout(ti, func) 即可启动一个定时调用。\n因为没办法取消定时，但是有时候又确实需要取消，所以 skynet 的文档中给了一个通过修改闭包函数变量把 func 置空的操作。虽然不是真正的从时间轮中移除，但是也算是一份折中的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 function cancelable_timeout(ti, func) local function cb() if func then func() end end local function cancel() func = nil end skynet.timeout(ti, cb) return cancel end local cancel = cancelable_timeout(ti, dosomething) cancel() -- canel dosomething 实现 时间轮结构 虽然上面举例使用的是现实中钟表的例子，但是实际上时间轮的实现基本不会是类似钟表的设计，因为在代码中，使用准确的日期其实并不便于计算。\nskynet 的实现是一个分层时间轮，一共分了五层。最接近要执行的时间点在 near 里，其余的分层在 t 里，有 4 层。每一层用链表实现，事件执行时从头部取，增加事件时从尾部增加。\n因为存在并发的情况，所以时间轮在修改的时候是需要加锁的。这里还是采用了 skynet 中常见的自旋锁来处理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 struct link_list { struct timer_node head; // 头节点 struct timer_node *tail; // 尾指针 }; struct timer { struct link_list near[TIME_NEAR]; // TIME_NEAR 256，工作队列 struct link_list t[4][TIME_LEVEL]; // TIME_LEVEL 64，分层等待队列，最后一层为溢出队列 struct spinlock lock; // 时间轮的自旋锁 uint32_t time; // 时间轮转动次数，会一直累加到 uint32_t 的上限，然后变成 0 以后继续累加 uint32_t starttime; // 开始时间 uint64_t current; // 从 starttime 开始到现在运行了多久 uint64_t current_point; // 以操作系统启动时间为基准的时间戳，单位为厘秒 }; 增加定时事件 timer_node 结构体用来保存单个定时事件，它是每个定时队列的组成部分。timer_event 用来保存发起定时事件的源服务 handle 和与定时事件绑定的 session 这两部分数据。\n1 2 3 4 5 6 7 8 9 struct timer_event { uint32_t handle; // 定时的服务 handle int session; // 定时事件的 session }; struct timer_node { struct timer_node *next; // 链表的下个节点 uint32_t expire; // 过期时间，单位为厘秒 }; 当调用 skynet.timeout 的时候，会向时间轮中加入一个定时节点。处理过程中会首先创建一个 timer_event 结构的变量，然后调用 timer_add 创建一个 timer_node 结构变量，并且把 timer_event 变量的内存直接追加到了 timer_node 变量的内存后面。timer_node 的 expire 会设为 timer.time + 定时的时间。\n增加定时节点时，首先要做的是找到合适的定时队列。skynet 一共为队列分了五层，以到期时间距离当前的时间偏移量计算，分别时如下五档。\n过期时间 分层 差异位 0 - 255 near 0-8 bit 256 - 16383 0 9-14 bit 16384 - 1048575 1 15-20 bit 1048576 - 67108863 2 21-26 bit 67108864 - 正无穷 3 27-32 bit 查找适合队列的时候是用 (A | mask) == (B | mask) 的方式计算的，比计算两数相减差值比较上下限要快一点。确定一个时间应该在哪个队列中的方法是，从 0-8 bit 开始比较目标时间和当前时间，如果差异在 0-8 bit 内，则说明距离当前时间不超过 255 秒，则应该放在 near 层中。如果不在当前层，则将掩码左移 6 位继续比较两个值。如果前四层都确定不了的话，则把剩下的全部加到最后一层中。\n找到了合适的队列以后，直接把 timer_node 链接到 link_list 的最后即可。\n更新时间 timer 线程会每隔 2.5 毫秒更新一次时间。由于 skynet 定时器的最小精度是厘秒，也就是 10 毫秒，所以并不是每次调用的时候都会真正触发时间更新。\n当可以触发时间更新时，会计算当前时间和上次时间的偏移量，然后为每个偏移量调用 timer_update 更新定时器。虽然理论上来说不太可能连续多次触发，但是为了防止特殊情况造成的定时被忽略，所以要对每个偏移量执行 timer_update 操作。\n更新时间的时候，需要检查是否要移动分层队列中节点的层级。这个并不是每次都要检查，而是每当时间轮的计时累加了 256 次以后才会检查一次。相当于是产生了一次进位，进位以后，当前时间在下一层对应的队列中的节点要被移动到 near 里。\n为了便于理解，这里举例说明一下进位操作。比如当前的 T-\u0026gt;time 为 511，当前在 near 队列中的数据为 [256, 511]，此时再次累加，T-\u0026gt;time 变为 512，这时一个 near 单位也就是 256 被处理完毕了，这个时候就要把 [512, 767] 的数据从分层队列中移除，加入到 near 队列中。这部分数据就在第 0 层的 2 节点中，通过将 (time \u0026raquo; 8) \u0026amp; 63 即可计算得出。\n执行事件 执行事件的时候通过 time \u0026amp; 255 即可拿到当前时间对应的事件链表，如果事件链表不为空，则从头到尾依次对事件调用 dispatch_list 即可。在 dispatch_list 中，timer 线程会拿到一开始加入定时事件时保存的源服务的 handle 和关联的消息 session，向源服务发送一条有对应 session 的消息即可，定时事件的回调函数会在后面被 worker 线程调用。\n执行事件本身并没有什么复杂的地方，但是看源码会发现，skynet 在一次 timer_update 里面执行了两次 timer_execute 方法，第一次是在 timer_shift 之前，第二次是在 timer_shift 之后。第二次调用很正常，没什么疑问，但是第一次的调用可能会有点疑惑的地方。\n之所以要在 timer_shift 之前也调用一次，是为了处理在上一次 timer_update 之后，到这次 timer_update 之间，加入的 timeout 为 0 的事件，如果没有在 timer_shift 之前的 timer_execute 调用，则这部分事件会被漏掉。\n但是，其实如果是通过 skynet 提供的接口创建的定时事件的话，应该是用不到前面的那一次 timer_execute 的，因为 skynet 提供的接口的入口是 skynet_timeout，在这个里面当 time \u0026lt;= 0 时，已经过滤掉了这部分的调用，没有经过定时器，而是直接向源服务发送了触发的消息。那么为什么还要保留这个前置调用呢，这里有一个 issue 说了这个原因，timer 不需要关注 skynet 的用法实现。\n","date":"2022-02-26T18:11:47+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%8D%81%E4%B8%89%E8%AE%A1%E6%97%B6%E5%99%A8%E5%AE%9E%E7%8E%B0/","title":"skynet源码分析（十三）计时器实现"},{"content":"　skynet 中支持 harbor 和 cluster 两种方式实现集群，本篇主要讨论 cluster，原因是我个人比较喜欢 cluster 这种方式，XD\n配置 集群功能需要各个远程主机的配置，这个配置可以通过在 config 文件中指定 cluster 变量为一个配置文件的路径来实现，也可以通过 cluster.reload 来加载。\n不管是通过文件还是 reload 加载配置，配置的格式都是一样的，是一个 Lua 的 table 格式。\n1 {NodeName = \u0026#34;[xxx.xxx.xxx.xxx]:xxxx\u0026#34;} 服务 实现 cluster 时使用了很多服务，整个逻辑个人感觉有一点绕。这里先简单讲一下使用到的各个服务大概做了什么事情。\nclusterd 是一个通过 skynet.uniqueservice 创建出来的进程唯一服务。它在 cluster 第一次被 require 的时候就会启动。clusterd 提供集群的管理服务，常用的集群接口 cluster.xxx 大部分都是在向 clusterd 发送消息。\n网关服务 gate 集群相关的部分每个进程一个，由 clusterd 创建，主要负责网络相关的集群节点的端口监听，数据发送，数据接收等操作。\nclustersender 由 clusterd 创建，每个对外的连接都有一个，在发起对外连接的时候创建，主要是负责向本服务关联的连接发送数据。\nclusteragent 由 clusterd 创建，每次 accept 的时候会为新的连接创建一个，用于处理该连接的后续数据。\nlisten 集群中的每个进程都要有一个对外的端口，cluster.open 可以打开端口监听，可以传端口，也可以传配置中有的节点名字。\n实际上处理监听的逻辑在 clusterd 中，当调用 listen 的时候，会启动一个新的网关服务 gate 来处理网络相关操作。启动好 gate 以后，通过 skynet.call 让 gate 服务执行 open 操作，开启端口的监听。\naccept clusterd 作为了 gate 的 watchdog 提供服务。当有外部连接时，watchdog 的 socket 命令中的 open 子命令会被执行。\nclusterd 会为每个新连接创建一个 clusteragent 服务来处理本连接的数据。clusteragent 会注册 client 的消息类型，然后对 gate 服务调用 forward 操作，把本服务设为连接的 agent 来接管后续的数据。\nconnect 集群节点之间对外连接是长连接，连接上以后不会主动断开。在节点第一次对外发送消息的时候会向对方发起连接。\nnode_channel 中保存了全部节点的连接，它的元方法 __index 会调用 open_channel 开启一个对外连接。对外连接是通过另一个服务 clustersender 实现的。\nclustersender 是每个对外连接都会有的服务，它负责通过 socket_channel 发起对外连接，并且负责后续针对这个节点的所有发送消息的操作。\n对外服务名字 服务可以注册一个对外的名字，一定要注册名字以后才能接收远程的消息。通过 cluster.register 可以注册名字。\ncluster.register 会发消息给 clusterd 来保存名字和地址的对应关系，同时 clusterd 还提供了根据名字查询服务地址的功能。\n发送数据 与同节点的不同服务间发送消息的接口类似，cluster.send 可以向指定节点的指定服务发送数据。cluster.call 向指定节点的指定服务执行一个调用。\n消息通过在连接时启动的 clustersender 服务进行处理。发送之前需要进行数据打包，都是通过 lua-cluster.c 中的 packrequest 进行打包的。打包好的数据通过之前服务启动时建立的 socket 连接发送出去。\n接收数据 接收到新数据的时候，gate 会把数据转发给连接的 agent 也就是上面提到的 clusteragent 服务来进行处理。\nclusteragent 使用 lua-cluster.c 中的 lunpackrequest 进行数据解包。解包以后的数据交给消息分发函数进行处理。\n在消息分发函数中，第一次向某个服务发消息之前会去 clusterd 中根据名字查询服务地址，查到以后会保存下来，以后不会再次查询。当名字有变更的时候，clusterd 会发送 namechange 消息给 clusteragent 来变更名字。因为数据已经打包过了，所以这一步的发送不再需要打包数据了，发送消息使用的是 rawsend 和 rawcall 即可。使用对应方法直接把消息转给通过服务名字查询到的服务地址。\n","date":"2022-02-26T18:11:13+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%8D%81%E4%BA%8C%E9%9B%86%E7%BE%A4/","title":"skynet源码分析（十二）集群"},{"content":"　netpack 提供的是网络基础数据包的打包和解包功能，是网络输出传输绕不开的一个点。本篇会尝试分析一下 netpack 的实现。\n概述 netpack 中一共提供了 5 个 Lua 接口。pack 用来打包数据，tostring 用来将一个二进制数据转化为字符串，filter 用来将数据按照类型整理，clear 用来清空待处理数据的队列，pop 用来从待处理队列中弹出一个网络包。\npack pack 可以把一个二进制数据或者是一个字符串打包成一个二进制数据包。\n包头占两个字节，是数据的长度，因为只有两个字节，所以单个数据包的长度需要小于 65536 个字节。值得注意的是，数据的字节序固定采用了大端方式，第一个字节是长度的高 8 位，第二个字节是长度的低 8 位。之所以这样写而不是直接保存长度，主要是担心夸字节序主机之间的交流。包头后面紧跟了数据内容。\ntostring tostring 负责将一个二进制数据转化为字符串格式。接受一个 userdata 和一个长度参数，返回转化后的字符串。实现上全都是通过 Lua 的 api 实现的，比较简单。\n有一点需要注意的是，netpack.tostring 会释放掉本来的 userdata 的内存。\nfilter lfilter 在处理大部分类型的命令时都只是简单的传递了一下参数而已，此处不再赘述。需要重点关注的点是，当处理 SKYNET_SOCKET_TYPE_DATA 类型的网络数据的时候，会执行的对数据进行分包的操作。\n结构 每个服务有一个网络数据队列，它包含了两部分，一部分是每个套接字的未完成部分 hash，它是一个 uncomplete 结构体的哈希桶，存放着每一个连接的一份未完成数据。另一个部分是已经完成分包的数据包队列，它是一个动态长度的环形数组，可以动态扩容，每次扩容 QUEUESIZE 个数据长度，存放着每一个连接的全部待处理网络包，等待消息分发函数调用 pop 来拿取网络包。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #define QUEUESIZE 1024 #define HASHSIZE 4096 struct netpack { int id; // 套接字 id int size; // 长度 void *buffer; // 数据 }; struct uncomplete { struct netpack pack; // 未完成数据对应的网络包结构 struct uncomplete *next; // 哈希桶的下一个 int read; // 已读取的长度 int header; // 当只有 1 字节的数据读取到时，用这个记录一下 }; struct queue { int cap; // queue 的长度 int head; // queue 的头节点索引 int tail; // queue 的尾节点索引 struct uncomplete *hash[HASHSIZE]; // 未完成分包的数据 struct netpack queue[QUEUESIZE]; // 已经完成分包的网络包队列 }; 分包 分包的最终处理函数是 filter_data_，这里就会用上上面提到的结构体，根据不同的情况，分别填充不同的数据，最后返回给 Lua 层的调用。\n函数会首先尝试通过 find_uncomplete 拿到 socket id 对应的 uncomplate 结构体。这里因为保存的数据结构是个哈希桶，所以找到对应哈希节点以后要遍历链表进行查找。\n如果存在对应的 uncomplate 结构体，则说明上次还有未完全读取的数据包。首先判断已读取长度 read 的大小，如果 read 小于 0，说明上次只读取到了本数据包的一个字节数据，这时候会读第二个字节的数据来得到完成的数据包长度。通过 uncompalte 结构体中的数据包总长度 pack.size 和已读取长度 read 可以计算出还需要的数据长度 need 来。\n通过比较还需要的长度和本次处理的数据长度，可以知道本次处理的情况。如果本次的长度还是不够拼出全部的数据，则把本次的数据加到 uncomplate 中即可。如果本次的数据正好可以完成当前的数据包，则直接把数据加到 uncomplate 中并且返回它即可。如果本次的数据会超出当前需要的数据，则首先把本数据包的内容加入到 queue.queue 中去，然后递归调用 push_more 处理剩下的内容，直到数据长度不再够组成一个完整数据包。\n一开始不存在 uncomplate 结构体的处理方法基本一样，只是要额外处理一下本次读取的数据长度为 1 的情况，这种情况不仅要保存 uncompalte 结构体，还要把 read 置为 -1 来作为这种特殊情况的标识。\npop 用来从已完成组装的队列中拿取最早的那一个数据包。上面说了分包的时候如果数据一次读取的比较多，超出了一条消息的范围，则会依次处理并且把每条消息都 push 到 queue 的 queue 数组中去。要处理消息的时候，就调用 netpack.pop 从队列中拿取一条消息出来。\nclear 用来清空整个队列的方法，在 gateserver 中被放在了 CMD 的 __gc 元方法中。会遍历清空 queue 中的 hash 和 queue 这两个结构，释放每一个数据的内存。\n","date":"2022-02-26T18:08:38+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%8D%81%E4%B8%80netpack%E5%88%86%E6%9E%90/","title":"skynet源码分析（十一）netpack分析"},{"content":"　网络相关的部分是我感觉 skynet 中最复杂的部分了，本篇中会尝试尽量完整的分析到网络相关的大部分功能的实现原理。\n线程模型 skynet 使用的是多线程 reactor 模型，有一条 socket 线程用来处理 epoll 事件分发，多条 worker 线程来执行事件。\n与其它使用类似模型的框架相比，skynet 最大的区别应该就是还使用了 Actor 的并发模型。所以在 socket 线程处理 epoll 事件的时候，并不是直接把事件交给了 worker 线程来执行，而是把事件和相关的数据一起转化为一条 Actor 之间通用的消息，放到了 Actor 的消息队列中，等待 worker 线程处理 Actor 的消息队列时来处理这个网络事件。\nsocket 管理器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #define MAX_INFO 128 // MAX_SOCKET will be 2^MAX_SOCKET_P #define MAX_SOCKET_P 16 #define MAX_SOCKET (1\u0026lt;\u0026lt;MAX_SOCKET_P) #define MAX_EVENT 64 #define MAX_UDP_PACKAGE 65535 struct socket_server { volatile uint64_t time; // 当前时间，由 timer 线程更新，socket 线程直接读这个值 int recvctrl_fd; // 接收命令的管道套接字 int sendctrl_fd; // 发送命令的管道套接字 int checkctrl; // 用来标记是否要检查控制台命令的标志 poll_fd event_fd; // 全局的 epoll 套接字 ATOM_INT alloc_id; // 已分配的id，不断累加 int event_n; // 本次调用 epoll 得到的就绪的 fd 个数 int event_index; // 目前处理到的 fd 序号 struct socket_object_interface soi; // userobject 接口 struct event ev[MAX_EVENT]; // 捕获的事件数组 struct socket slot[MAX_SOCKET]; // 全部的 socket 哈希，key 为经过哈希算法计算以后的 id char buffer[MAX_INFO]; // 临时缓冲区 uint8_t udpbuffer[MAX_UDP_PACKAGE]; // udp 数据缓冲区 fd_set rfds; // 要监听的读描述符集合，用于命令的 select }; 每个 skynet 进程都有一个全局的 socket 管理器，它会在 skynet 进程启动的时候被初始化。从其中的变量大概可以看出一些实现的端倪。比较重要的部分是 epoll 相关的部分和命令相关的部分。\n在其初始化函数 socket_server_create 中，主要的操作是\n创建了用于 ctrl 命令的管道套接字 创建了 epoll 套接字 调用 pipe 创建管道并且把其中一端加入 epoll 套接字的管理中 初始化 slot 数组中的全部数据，运行中不会扩容，新的 socket 会被复制到指定的位置上 socket 结构分析 结构概览 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 struct socket { uintptr_t opaque; // 本结构关联的服务 handle struct wb_list high; // 高优先级队列 struct wb_list low; // 低优先级队列 int64_t wb_size; // 等待写入的字节长度 struct socket_stat stat; // 统计数据 ATOM_ULONG sending; // 是否正在发送数据，是一个引用计数，会累加 int fd; // 套接字 int id; // 分配的 id ATOM_INT type; // 当前连接状态 uint8_t protocol; // 连接协议 bool reading; // fd 的 read 监听标记 bool writing; // fd 的 write 监听标记 bool closing; // fd 的 close 标记 ATOM_INT udpconnecting; // udp 正在连接 int64_t warn_size; // 报警阈值 union { int size; uint8_t udp_address[UDP_ADDRESS_SIZE]; } p; // 如果是 tcp 连接则用 size 表示每次读取的字节数，如果是 udp 则用 udp_address 表示地址 struct spinlock dw_lock; // 自旋锁 int dw_offset; // 已经写入的大小 const void *dw_buffer; // 数据指针 size_t dw_size; // 总大小 }; socket 连接是个很复杂的结构，因为内存对齐的缘故，变量的先后顺序排列并不是按照关联性分布的，后面会按照功能相关性分开讨论其中的变量。\nsocket 的基础数据 int fd\nfd 是系统分配的 socket 套接字，是网络连接的基础。不同的网络操作会使用不同的参数来使用系统调用 socket 创建自己的网络套接字。 int id\nid 是由 skynet 分配的用来标识 socket 结构体变量的唯一标识符。之所以在有了 fd 以后还需要 id 是因为内核可能会重用 fd，并不能用 fd 来做唯一标识。 ATOM_INT type\n虽然名字叫类型，但是其实是 socket 当前的状态，这个变量会随着 socket 的状态改变而被修改，目前一共有十种状态。 1 2 3 4 5 6 7 8 9 10 #define SOCKET_TYPE_INVALID 0 // 初始状态，表示未使用 #define SOCKET_TYPE_RESERVE 1 // 保留状态 #define SOCKET_TYPE_PLISTEN 2 // 监听前状态 #define SOCKET_TYPE_LISTEN 3 // 监听中状态 #define SOCKET_TYPE_CONNECTING 4 // 连接中状态 #define SOCKET_TYPE_CONNECTED 5 // 已连接状态 #define SOCKET_TYPE_HALFCLOSE_READ 6 // 半关闭剩下读 #define SOCKET_TYPE_HALFCLOSE_WRITE 7 // 半关闭剩下写 #define SOCKET_TYPE_PACCEPT 8 // 已 accept 但是还未加入 epoll #define SOCKET_TYPE_BIND 9 // 绑定状态 uint8_t protocol\nprotocol 表示的是 socket 关联的协议类型，在处理网络事件时，需要知道它的协议类型来执行不同的消息分发函数。目前一共有 4 中类型，其中 PROTOCOL_UNKNOWN 是一开始的默认类型。 1 2 3 4 #define PROTOCOL_TCP 0 #define PROTOCOL_UDP 1 #define PROTOCOL_UDPv6 2 #define PROTOCOL_UNKNOWN 255 uintptr_t opaque\nopaque 是创建本 socket 的源服务的 handle，因为前文中提到的 skynet 并不会直接处理网络事件，而是会把每个网络事件都转换为消息发送给源服务，所以 socket 要记录下来源服务的 handle 来接受网络事件消息。 union {int size; uint8_t udp_address[UDP_ADDRESS_SIZE];} p\n这里为了省内存使用了一个 union 做了两个作用，当 socket 是 tcp 协议的时候，使用 size 表示本连接每次从 fd 中读取的字节数，这个 size 会被初始化 64，并且会根据每次从 fd 中读取数据的情况增加或者减少，这个参数的存在是为了优化读取的效率。如果是 udp 协议的话，使用 udp_address 用来保存对端的地址。 bool reading, writing\n用来表示套接字当前在 epoll 中的状态，主要被用在修改 epoll 的监听事件时，比如当之前已经设置过 EPOLLIN，又要加上 EPOLLOUT 的时候，如果没有 reading 记录的话，就比较麻烦。 bool closing\n标记 socket 的已关闭状态。 struct socket_stat stat\nstat 用来记录当前 socket 的读写数据状态。 1 2 3 4 5 6 struct socket_stat { uint64_t rtime; // 最近一次读取时间 uint64_t wtime; // 最近一次写入时间 uint64_t read; // 已读取的总数据长度 uint64_t write; // 已写入的总数据长度 }; ATOM_INT udpconnecting\n正在连接中的 udp 数量，发起连接时累加，连接成功后递减。 写入队列相关 struct wb_list high, low\nwb_list 即 write buffer list，发送队列是一个单向链表。每个 socket 连接有两个发送队列，一个高优先级队列和一个低优先级队列，高优先级队列中的数据会优先发送出去。发送数据时，需要指定数据的优先级。 1 2 3 4 5 6 7 8 9 10 11 12 struct write_buffer { struct write_buffer *next; const void *buffer; char *ptr; size_t sz; bool userobject; uint8_t udp_address[UDP_ADDRESS_SIZE]; }; struct wb_list { struct write_buffer *head; struct write_buffer *tail; }; int64_t wb_size\nwb_size 即 write buffer size，是 socket 连接中全部等待写入的数据长度，包含两个写入队列的待写入长度总和。 int64_t warn_size\nsocket 等待发送数据的警告阈值，如果超过了这个值，会输出一条警告信息。阈值每次触发以后会变为原来的两倍。 direct write 相关 struct spinlock dw_lock\n直接写入部分的锁，需要修改 dw 相关的数据时要先加锁。 int dw_offset\n通过 dw 已经发出去数据偏移量，在 socket 线程处理发送的时候需要根据这个值计算出还未发送出去的数据。 const void *dw_buffer\n如果 dw 阶段发送出去的数据不完整的话，调用 clone_buffer 使 dw_buffer 指向待发送数据，等待 socket 线程后续再次发送这个数据。 size_t dw_size\n本次 dw 数据的总大小，也就是 dw_buffer 指向数据的大小。 分配 ID skynet 在分配 socket 的 ID 时，也会碰到空洞位置的问题，因为关闭的 socket 连接会被再次设为可用状态，这就导致了分配 ID 不能简单累加，而是从 alloc_id 开始，遍历完整个 slot 数组，计算哈希的时候直接针对 MAX_SOCKET 取模即可。这里有个小问题，alloc_id 是个原子变量，可能会让这个分配 ID 的函数的效率雪上加霜，最坏情况下在分配一次 ID 的过程中，alloc_id 要被 atomic_fetch_add 累加几万次。\nepoll 相关 skynet 在创建 epoll 套接字的时候，为 epoll_create 传入了参数 1024，这里仅为兼容旧版本的内核，新版内核已经不再需要这个用来提示的参数。\n关于 epoll 的触发模式，skynet 使用的是 epoll 默认的水平触发模式。\nself-pipe 从 socket_server 的初始化函数中可以看到，recvctrl_fd 和 sendctrl_fd 分别是管道 pipe 套接字的两端，并且 recvctrl_fd 被加入到了 epoll 监听中。\n这用到了一个叫做 self-pipe 的技术，《Linux 系统编程手册》65.5.2 有介绍这个技术。它在 skynet 中的应用主要是为了解决单网络线程阻塞在 epoll 的 wait 上这种情况。如果不使用这个技术，则当需要做一些修改，比如修改 epoll 的监听套接字的时候，这时候线程阻塞在了 wait 上，要处理修改只能让 wait 等待一个指定时间以后返回，处理修改，然后再次进入 wait 中。这带来一个问题就是等待时间的选取是比较麻烦的，太久了则修改要等待很久才能生效，太短了则会让 wait 不断返回进入，比较浪费 CPU 资源。\n通过 self-pipe 技术就可以解决这个问题，创建一对管道套接字用于网络命令处理，然后把接收端加入 epoll 管理中，所有对网络操作的修改都发送给管道的发送端。这样就能达到一旦有命令过来，epoll 的 wait 立刻会被唤醒，不需要指定返回时间了。\nsocket 线程的主循环 skynet_socket_poll socket 线程会无限循环调用函数 skynet_socket_poll，在这个函数中，通过调用 socket_server_poll 来获取网络事件和处理的数据 result，针对不同的网络事件，通过调用 forward_message 发送不同格式的消息给与触发事件的套接字绑定的服务。\n函数会创建一个 socket_message 的结构体变量 result，把 result 传给了 socket_server_poll，其处理完网络事件以后，会把结果写入到 result 中，然后 result 会再次给到 forward_message，它会将 result 根据需要处理成一条 skynet 消息，然后 push 到源服务的消息队列中。\n1 2 3 4 5 6 struct socket_message { int id; // socket id uintptr_t opaque; // 目标服务的 handle 句柄 int ud;\t// accept 事件中 ud 是新连接的 id，别的时候是 data 的长度 char * data; //数据指针 }; socket_server_poll 本函数是 skynet 网络部分处理网络事件的最终循环。该函数大部分情况下都阻塞在 sp_wait 上，等待 socket 事件触发或者是网络命令过来。\n从 epoll_wait 中唤醒以后，开始了一轮处理。在每轮的处理中，首先会把检查控制命令的变量 checkctrl 置为 1，然后开始依次处理本次触发的网络事件。\n在网络事件的处理中，如果碰到的是命令事件，则直接 continue 回到循环的最上面去处理网络命令。否则则会去读取 socket 的状态 type，根据不同的 type 执行不同的操作，向 result 中填充相关的数据。\n每次从 epoll 中取到的就绪套接字个数放在 event_n 中，用一个变量 event_index 保存当前处理到了第几个套接字。当 event_index == event_n 的时候，则说明本轮的处理已经结束了，线程会再次调用 epoll_wait 获取下一轮要处理的就绪套接字。\n本函数会填充 result 参数，并且返回一个处理结果类型给 skynet_socket_poll，返回的结果一共包含了八种类型。\n1 2 3 4 5 6 7 8 #define SOCKET_DATA 0 // 读取数据 #define SOCKET_CLOSE 1 // socket 关闭 #define SOCKET_OPEN 2 // socket 连接成功 #define SOCKET_ACCEPT 3 // accept 成功 #define SOCKET_ERR 4 // socket 错误 #define SOCKET_EXIT 5 // 退出 socket 线程 #define SOCKET_UDP 6 // 接收 udp 数据 #define SOCKET_WARNING 7 // socket 警告 网络指令处理 每当 epoll_wait 返回时，新的一轮网络事件的处理就会开始，指令的检查标记 checkctrl 也会被设为 1，来开启指令检查。每轮只会处理一次指令，会一直连续处理指令直到全部处理完。\n通过 has_cmd 来检查管道中有没有还没处理的命令数据。这一步是使用系统调用 select 来实现的，使用 select 来检查 recvctrl_fd 是否可读。虽说 recvctrl_fd 被加到了 epoll 中，但是 epoll_wait 唤醒以后，如果是指令数据唤醒的，不会原地处理，而是等下一个循环处理，所以这里还要再检查一次是否可读，并没有以来 epoll 做标记之类的，可能是为了处理简单一些。\n如果 recvctrl_fd 中有指令等待读取，则调用 ctrl_cmd 读取并执行指令。其中用了两次 block_readpipe 来读取管道中的数据，第一次读取了数据头，包括了命令类型和数据长度，第二次用第一次读取到的数据长度读取了数据。\nblock_readpipe 是用来从管道中读取数据的函数，可以看到一个有意思的地方，read 的返回值只处理了小于 0 的情况，并没有处理 n \u0026gt; 0 \u0026amp;\u0026amp; n \u0026lt; size 的情况。这是因为对管道套接字执行 read 操作是一个原子操作，不会被别的情况比如信号之类的打断，所以只有两种可能，错误和全部读取完成。关于 pipe 套接字的读写后面可以考虑开一篇文章写一下。\n读取到了命令以后，根据命令类型，把数据交给不同的处理函数来处理即可。\n网络请求 概述 skynet 中涉及网络的操作，除了 direct write 以外，都是通过网络请求来实现的。worker 线程根据自己想要做的操作的类型，创建不同结构的请求数据，通过 send_request 发送到命令管道的发送端 sendctrl_fd 中去，等待主循环中接受处理请求。\n请求包的结构 因为 C 中没有面向对象的功能，所以通过 union 来实现了请求包的结构，每一种类型的请求对应了一类的请求结构体，所有的请求都会转化成一个 request_package 结构体，发送到管道中来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 struct request_package { uint8_t header[8]; // 6 bytes dummy, 第 7 个字节表示类型，第 8 个字节表示长度 union { char buffer[256]; struct request_open open; struct request_send send; struct request_send_udp send_udp; struct request_close close; struct request_listen listen; struct request_bind bind; struct request_resumepause resumepause; struct request_setopt setopt; struct request_udp udp; struct request_setudp set_udp; } u; uint8_t dummy[256]; // 预留了 256 个字节 }; 处理请求包 socket 线程的主循环中，会不断读取管道中的数据，每条命令要执行两次 read 读取，第一次要读到 header 中的内容，然后根据 header[1] 的长度数据，读取剩余的数据。然后根据 hander[0] 中的类型数据，进行不同的处理。目前一共有 13 种网络操作类型。\nS Start socket\nB Bind socket\nL Listen socket\nK Close socket\nO Connect to (Open)\nX Exit\nD Send package (high)\nP Send package (low)\nA Send UDP package\nT Set opt\nU Create UDP socket\nC set udp address\nQ query info\n网络接口分析 概述 因为所有的网络操作都是通过发送命令来进行的，所以 skynet 的网络接口都是非阻塞的，不同的接口会完成基本的操作，然后把需要的参数打包成一条上面提到过的 request_package 结构数据发送给命令的接收端。\nconnect 通过调用 socketdriver.connect 可以发起一个对外连接。lconnect 中会从目标地址的字符串中分离出 host 和 port 这两个参数，再获取一个 socket id，然后打包成一个网络请求，给命令接收套接字发送了一个 \u0026lsquo;O\u0026rsquo; 类型的命令。\n在 socekt 线程中的部分里，对应命令的处理方法是 open_socket，首先它会通过系统调用 getaddrinfo 拿到目标主机的全部地址，然后依此对每个地址尝试去执行系统调用 socket 创建一个套接字并且把它设为 keep_alive 和 non_blocking 的，然后执行 connect 系统调用。\n如果套接字创建成功了，则创建 socket 结构体。检查 connect 的调用返回，如果成功了，则直接把 socket 的状态设为 SOCKET_TYPE_CONNECTED，返回 SOCKET_OPEN，连接成功。\n如果连接失败了，且 errno 是 EINPROGRESS，也就是说无法马上连接的状态，则把 socket 的状态设为 SOCKET_TYPE_CONNECTING，并且将套接字加入到 epoll 中，打开写入事件。当 epoll 触发了套接字的 write 事件时，则说明之前的连接已经建立成功了。将 socket 的状态设为 SOCKET_TYPE_CONNECTED，返回 SOCKET_OPEN 表示连接成功。\nSOCKET_OPEN 返回以后，会创建一个 SKYNET_SOCKET_TYPE_CONNECT 类型的消息给发起连接的源服务，但是源服务并不需要处理连接成功的消息，所以 netpack 在进行解包的时候，直接忽略了 SKYNET_SOCKET_TYPE_CONNECT 类型的消息。\nlisten 从 gateserver 的 open 命令来分析一下 skynet 套接字的监听步骤。socketdriver.listen 是 skynet 的监听接口，可以开启一个监听套接字，函数返回值是监听套接字的唯一标识符，也就是上面提到的 id 这个变量。监听的接口执行可以分为两个阶段，第一个是在 worker 线程中执行的部分，第二个是在 socket 线程中执行的部分。\n监听操作在第一阶段的执行中，主要逻辑在 socket_server_listen 函数中，其中依次调用了 socket/bind/listen 等系统调用，完成了网络套接字的创建，绑定和监听，但是并未将套接字加入到 epoll 的管理中去。然后通过 reserve_id 分配了一个 socket 结构的唯一 id，但是这里也不会创建 socket 结构体。创建一个 request_package 的结构体，将上面拿到的参数填入其 request_listen 结构体中，然后将其发送给命令的接收套接字 sendctrl_fd 即可。\n监听操作在第二阶段的执行中，主要逻辑在 listen_socket 函数中。这里逻辑就比较简单了，做的事情就是上段中点明了剩下的两个部分，把监听套接字加入到 epoll 管理中，并且创建了 socket 结构变量。socket 中的 type 会被修改为 SOCKET_TYPE_PLISTEN，只是 pre listen 还没有完全完成监听。还有一点需要注意的是，因为上一阶段已经拿到了 socket 的唯一 id，所以这里是直接修改了之前那个 id 在数组 socket_server.slot 中对应的 socket 结构体。\n到这里 socketdriver.listen 的工作就全部结束了，但是明显可以发现这个时候的 listen 还未完全完成。因为此时还未设置 accept 以后的回调，而且 socket 中的状态也还是未完成的监听状态。我们现在有一个 socket id 是监听套接字的句柄，需要做的是使用这个 id 调用 socketdriver.start 来执行后续的步骤。\nsocketdriver.start 中主要做的事情是给网络命令接收套接字发送了一个 \u0026lsquo;R\u0026rsquo; 请求，这个请求会把 id 对应的套接字加入到 epoll 管理中并且打开读取监听。不过由于 listen 的前期创建 socket 的时候已经把套接字加入到了 epoll 并且默认是打开读取的，所以这里并不会做什么操作。这里最主要修改的是上面提到的 socket 的状态，会把状态改为 SOCKET_TYPE_LISTEN，以让循环中可以正确处理监听，然后还把监听 socket 的源服务改为了调用 start 的服务，也就是说可以实现在某个服务中 listen 创建一个 socket id，然后把它传给另一个服务，由另一个服务调用 start 来接收后续的消息。\naccept accept 由 socket 线程的 epoll 循环触发，如果触发网络事件的套接字是 SOCKET_TYPE_LISTEN 状态的话，则说明触发了 accept 事件。\nreport_accept 是处理 accept 事件的函数，首先通过系统调用 accept 拿到网络套接字，然后拿到 socket 结构要用的唯一 id，client 的 fd 会被设置 keep_live 和 no_blocking，创建 socket 结构，把 socket 的状态设为 SOCKET_TYPE_PACCEPT 类型。\n上述处理结束以后，返回到 skynet_socket_poll 的类型为 SOCKET_ACCEPT，skynet_socket_poll 会调用 forward_message 给 socket 的源服务发消息，socket 的源服务现在是上一步中执行 socketdriver.start 的服务。发送的消息中把消息结构体中的 type 设为了 SKYNET_SOCKET_TYPE_ACCEPT 来标识消息的类型。\n消息会交给 gate 服务处理，它注册了 \u0026ldquo;socket\u0026rdquo; 类型的协议，负责解包的是 netpack.filter 函数。lfilter 在处理 SKYNET_SOCKET_TYPE_ACCEPT 时很简单，只是整理了一下参数而已，压入了操作类型对应的字符串 \u0026ldquo;open\u0026rdquo; 供 dispatch 方法调用。\n在 dispatch 中，\u0026ldquo;open\u0026rdquo; 操作对应了 MSG.open 方法，其中跟网络层有关的就是调用了 handle.connect，在 handle.connect 中，历经千难万苦，如果是用 examples 中提供的示例的话，就是经过了 watchdog 和 agent 的操作，最终调用了 socketdriver.start 来激活客户端的 socket 结构。与 listen 的步骤类似，在 start 中会把 client 的 socket 类型从 SOCKET_TYPE_PACCEPT 改为 SOCKET_TYPE_CONNECTED，socket 关联的服务 handle 会改为调用 socketdriver.start 的服务。\nwrite 发送数据有两种方法，常规的是通过 epoll 的写事件触发。为了减少一些开销，skynet 还做了一个叫做 direct write 的操作，也就是直接写入，不通过 socket 线程，而是在 worker 线程直接尝试把数据发出去。\n首先来看 direct write 的部分，发送数据的接口为 socket_server_send，这个函数首先会检查当前 socket 能不能直接发送，如果在当前 socket 的高或低优先级队列中有数据等待发送的，则不能直接发送。如果可以直接发送的话，则会直接在 worker 线程调用 write 往套接字中写入数据。\n直接发送会有三种结果。如果发送失败了，则忽略这个错误，当作写入了 0 长度的数据。如果完整发送了全部的数据，则可以直接返回，不需要再走后面的步骤了，本次发送已经完成了。如果只发送了部分数据，包括前面发送错误产生的结果，都会设置 dw_buffer/dw_size/dw_offset 这三个变量，等待后续 socket 线程再次进行数据发送，并且发送了一条网络消息 \u0026lsquo;W\u0026rsquo; 来打开本套接字的写事件监听。\n触发 epoll 的写事件以后，如果有之前 direct write 阶段没发完的数据，会首先把剩余的数据加入到高优先级队列的首部。发送阶段，会优先先发高优先级队列的数据，然后再发低优先级队列中的数据，如果低优先级队列中的数据没有全部发完，则会借助一个叫做 raise_uncomplete 的操作，把剩余的数据提到高优先级队列中。\n在两种情况下会触发关闭套接字写事件的监听，首先是如果 write 如果返回了一个错误，且不是 EINTR（信号打断） 或者 EAGAIN（非阻塞套接字缓冲区写满） 错误的情况下会关闭写事件。还有就是当套接字的高优先级队列和低优先级队列都发送完毕的时候也会关闭写事件。\nread 当 epoll 中的套接字变为可读以后，如果是 TCP 连接，则使用 forward_message_tcp 读取套接字中的数据。\n每次读取的长度是 socket.p.size，初始为 64 字节，如果在本次读取的时候发现套接字中可读长度大于 size 的话，则将 size 扩大为之前的 2 倍，如果发现套接字中的数据比 size 的 1/2 还少的时候，则把 size 变为原来的 1/2 长度。另外，如果本次没有读取完套接字中的数据，则会减少 event_index，使下一次循环依然处理本事件。\n读取到数据以后，转化成消息，给 socket 的源服务发送一条 SKYNET_SOCKET_TYPE_DATA 类型的消息。这个消息会触发网络分包，通过 netpack.filter 进行分包，分包值得单开一篇文章细说一下，此处先一笔带过。\n","date":"2022-02-18T14:53:02+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%8D%81%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90/","title":"skynet源码分析（十）网络分析"},{"content":"　snlua 服务绝对是 skynet 中最重要的服务了，是一切 lua 服务的原型，也是 99% 情况下业务中使用的服务。本篇会讲解一下 snlua 服务的实现。\n结构 1 2 3 4 5 6 7 8 9 struct snlua { lua_State *L; // 状态机 struct skynet_context *ctx; // 关联的 skynet context size_t mem; // 已经使用的内存 size_t mem_report; // 触发内存警告的阈值 size_t mem_limit; // 内存的上限设置 lua_State *activeL; // 目前正在运行的状态机 ATOM_INT trap; // 打断状态 }; snlua 的核心数据就是一个 lua 状态机，围绕状态机，skynet 做了一些扩展工作，主要是为其自定义了内存分配函数，以及为其提供了一个接受外部信号打断其运行状态的机制。\n创建 1 2 3 4 5 6 7 8 9 10 11 #define MEMORY_WARNING_REPORT (1024 * 1024 * 32) struct snlua *snlua_create(void) { struct snlua *l = skynet_malloc(sizeof(*l)); memset(l, 0, sizeof(*l)); l-\u0026gt;mem_report = MEMORY_WARNING_REPORT; l-\u0026gt;mem_limit = 0; l-\u0026gt;L = lua_newstate(lalloc, l); l-\u0026gt;activeL = NULL; ATOM_INIT(\u0026amp;l-\u0026gt;trap, 0); return l; } module 的创建主要就是对其私有数据的创建，snlua 中也不例外，过程就是创建一个 snlua 的结构体变量，然后初始化它其中的变量。\n可以看到单个 snlua 服务的内存警告的初始阈值是 32M 大小。这个并不是硬上限，只是一个警告阈值，超过了会触发一次警告，然后这个值会被修改为原来的两倍，直到下一次触发警告。内存分配使用了自定义的 lalloc 来做，还有一点需要注意，在创建状态机的时候，把 snlua 的指针也传进去了，这个在协程部分会用到。\n初始化 初始化的时候用了一个有趣的操作，并没有直接在 snlua_init 里面做初始化，而是给自己发了一条消息，然后在消息的回调函数 launch_cb 里面再通过调用了 init_cb 做了初始化。\nLua 状态机的环境都是在 init_cb 中进行初始化的。大概进行了以下几点操作\n加载标准库 加载 skynet.profile 库 使用 skynet.profile 库中的 resume 和 wrap 替换掉标准库协程中的同名函数 加载 skynet.codecache 库 从环境变量中拿取配置的那些文件加载的地址数据，加载进来 加载 loader.lua 通过 loader 加载指定的服务文件 检查是否设置了 lua 服务的内存上限，如果有则要设置 内存分配 自定了 Lua 的内存分配函数为 lalloc，主要的操作就是处理 snlua 中跟内存有关的三个变量 mem / mem_limit / mem_report\n修改当前占用的内存 mem 检查内存分配上限 mem_limit，如果超了上限不能分配 检查内存使用的警告阈值，如果超过了阈值，要给出警告，并且把警告阈值翻倍 因为自定义了内存分配函数，所以同时也支持了输出当前使用的内存，可以通过 debug 后台发信号查询指定服务的内存占用。\n信号打断 snlua 在自己的 signal 接口中，支持了打断正在运行的脚本的功能。这个功能主要是用来打断可能陷入死循环的服务的。\n打断的实现需要用到 snlua 中的 activeL 和 trap 这两个变量。trap 是个原子变量，用来标识当前是否正在打断过程中，之所以需要是原子变量是因为 signal 可能并行触发。activeL 变量用来标识当前正在执行的 Lua 状态机。\n要理解 activeL 变量的作用需要了解一些 Lua 的协程实现才可以。Lua 的每个协程对应的结构也是 lua_State，和 Lua 的主虚拟机是相同的，在协程内部的调用中会使用协程自己的 lua_State 执行各种操作。这就有了一个问题，要打断当前运行的脚本，需要给 lua_State 设置钩子函数，所以需要一个变量来记录当前是哪个 lua_State 正在执行，又因为切换 lua_State 的过程是发生在对协程的 resume 调用中的，所以需要自己实现一个 resume 来替换掉标准库中的 coroutine.resume，也就是 luaB_coresume 了。在发生 lua_State 切换的时候，snlua 都会修改 activeL 变量指向新的活动状态机。\n打断脚本的操作本身很简单，通过 Lua 的钩子函数机制调用 signal_hook，signal_hook 中使用 lua_getallocf 拿到创建 lua_State 的时候存进去的 snlua 结构体的指针，将其 trap 变量修改为可打断状态，然后调用 luaL_error 报错来打断当前的执行逻辑。\nprofile snlua 中还实现了 profile 的功能，通过在脚本中调用 profile.start 和 profile.stop 来拿到中间的时间间隔。实现是通过在 start 的时候创建一个 start_time 变量，在 stop 的时候计算时间间隔。中间涉及协程的调用时，也是通过自定义的 resume 函数来实现的，在 resume 协程之前和之后，分别记录时间，将其汇总到总时间中去。\n","date":"2022-02-15T03:10:34+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%9Dsnlua%E6%9C%8D%E5%8A%A1%E8%AF%A6%E8%A7%A3/","title":"skynet源码分析（九）snlua服务详解"},{"content":"　skynet 使用的并发模型是 actor 模型，这就使得两个 actor 之间进行交互的时候全都是通过消息来实现的，而且不单单是 actor 之间交互使用了消息，前文提到的定时调用和网络事件也都是通过消息来通知目标服务的。可以说 skynet 中消息是一切交互的依托形式，本篇会尝试理清楚消息相关的实现。\n消息的结构 1 2 3 4 5 6 struct skynet_message { uint32_t source; // 消息源的地址 int session; // 消息的 session id void *data; // 消息的数据 size_t sz; // 消息的长度和类型，高 8bit 为类型，其余为长度 }; 消息的结构非常简单，其中 session 由源服务生成，是给 call 调用提供支持的，如果是 send 消息的话则 session 会被设为 0。\n发送消息 不管是从 Lua 层还是 C 层发送消息，最终调用的接口都是 skynet_send 函数。\n1 2 3 4 5 6 7 int skynet_send(struct skynet_context *context, uint32_t source, uint32_t destination, int type, int session, void *data, size_t sz) 可以看到这是一个参数非常多的函数，但是仔细一看就会发现参数的内容跟上面消息的结构基本上吻合。事实上这个函数做的事情也简单，用参数生成一个 struct skynet_message 结构的消息，然后 push 到目标服务的消息队列中去即可。\n服务消息队列 skynet 中的每个服务都有一个 struct message_queue 结构的消息队列，它对应了 actor 模型中的 mailbox，保存着本服务的全部待处理消息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 struct message_queue { struct spinlock lock; // 自旋锁 uint32_t handle; // 消息队列所属服务的 handle int cap; // 容量，动态长度，可以扩容 int head; // 消息的头指针 int tail; // 消息的尾指针 int release; // 标记是否已经被释放 int in_global; // 标记是否在全局队列中 int overload; // 现在的负载 int overload_threshold; // 超载警告的阈值 struct skynet_message *queue; // 消息队列数组 struct message_queue *next; // 下一个消息队列，链表结构 }; 从成员变量中可以看到消息队列的实现方式。首先它是自带有一个自旋锁的，因为很多线程都有可能同时修改某个服务的消息队列，所以修改的时候都需要加锁。它通过 head，tail，cap 和 queue 组成了一个环形动态长度的数组，初始的容量为 64，当容量不够用时，每次扩容到以前容量的 2 倍。此外消息队列通过 overload 和 overload_threshold 实现了一个负载警告的功能，当拿取消息的时候，如果当前消息数量超过了当前设置的负载阈值，会输出一条服务负载过重的警告信息。\n全局消息队列 1 2 3 4 5 struct global_queue { struct message_queue *head; struct message_queue *tail; struct spinlock lock; }; skynet 中有一个全局的消息队列，当服务的消息队列中有待处理的消息时，它会被放在全局消息队列中来。worker 线程每次就是先从全局消息队列中拿一个服务队列出去，然后处理这个服务消息队列中的消息。\nstruct global_queue 的结构比较简单，是一个 message_queue 的单向链表。带有一个自旋锁，修改链表要先加锁。\n消息处理 注册回调函数 每个服务都需要有一个回调函数，回调函数被用来处理消息的数据。通过调用 skynet_callback 来注册回调函数。\n1 void skynet_callback(struct skynet_context *context, void *ud, skynet_cb cb) skynet_cb 是一个如下形式的函数指针。\n1 2 3 4 5 6 7 typedef int (*skynet_cb)(struct skynet_context * context, void *ud, int type, int session, uint32_t source, const void * msg, size_t sz); 不同的服务有不同的回调函数设置的位置，例如最常用的 snlua 服务会在 lua 脚本中调用 skynet.start 函数，这个函数里面就会把 lua_skynet.c 中的 _cb 设为回调函数。\n数据打包与解包 消息在发送之前有时需要对数据进行打包，比如说要发送的数据是一个 lua table 这种。如果消息的数据有进行过打包的话，那么目标服务在处理消息的时候就需要进行对应的解包操作。\n这就要求在消息的发送方和接收方两边对同一个类型的协议有相同的配置，起码 pack 和 unpack 这两个函数一定要可以实现互相转换。一个最常见的例子，在 snlua 服务中，\u0026ldquo;lua\u0026rdquo; 类型的协议的打包和解包函数分别为 skynet.pack 和 skynet.unpack，前者可以把 lua table 转为二进制数据，后者可以把前者转换的结果再次转为 lua table 格式。\n执行消息回调 执行回调本身只负责调用注册的回调函数，需要各个服务在回调函数中正确处理自己的业务。回调可以很简单很直接，也可以很复杂。比如在 snlua 服务的回调函数就比较复杂，当消息的回调被调用时，首先调用的是直接注册的 _cb，它会从索引表中取到注册时存进去的 lua 回调函数也就是 skynet.dispatch_message 并且执行它，执行中会根据协议的类型，在 proto 中找到对应的协议类型的参数，从中拿到之前注册过的消息分发函数 dispatch，用它创建一个协程，然后执行协程。\nLua 层接口 初始化服务 skynet.start 是绝大部分 snlua 服务会在服务的入口文件里调用的方法。它通过 C 接口，把消息回调函数设为了 _cb，_cb 相当于是一个 wrapper 函数，它会把通过参数传入的 Lua 函数 skynet.dispatch_message 保存在 LUA_REGISTRYINDEX 中，等到 _cb 被调用的时候，它会取出这个函数，把参数传给它，并且调用它。也就是说基本上所有的 snlua 服务都是用的同一个回调函数 skynet.dispatch_message 来处理消息的。\n传给 skynet.start 的函数可以看作是初始化函数，一般会在里面调用 skynet.dispatch 来为关心的消息类型指定处理函数。\n注册消息协议 可以通过 skynet.register_protocol 来注册一类消息，其中 lua, response 和 error 是在 require skynet 的时候就已经注册过的了，不再需要重复注册。\n可以通过调用 skynet.dispatch 来修改某个消息类型的处理函数。在绝大多数 snlua 服务的初始脚本中，都包含对 skynet.dispatch 的调用，比如下面这个就是在注册 \u0026ldquo;lua\u0026rdquo; 协议的消息的回调函数。\n1 2 3 skynet.dispatch(\u0026#34;lua\u0026#34;, function(...) -- ... end) snlua 服务中会维护一个 proto 的 table 用来保存各类协议的数据，其中的数据正是由上文提到的 skynet.register_protocol 注册进来的。一个典型的协议会包含以下几种数据\n1 2 3 4 5 6 7 8 { name = \u0026#34;lua\u0026#34;, id = skynet.PTYPE_LUA, pack = skynet.pack, unpack = skynet.unpack, dispatch = function() end, trace = nil } 发送消息 skynet.send 是最常用的消息发送方法，有目标服务的句柄或是名字都可以发送消息。它会根据消息的类型，找到类型对应的打包函数，将其打包，调用 C 层的接口发送出去。\nskynet.call 可以理解为类似远程调用的那种感觉，执行以后会挂起本协程，等待目标服务对本消息进行返回以后，会再次从挂起点开始执行。call 的实现深度依赖了 Lua 的协程，发送消息的部分跟 send 是一样的，只是它会拿到 send 返回的 session，同时将 session 和当前运行的协程绑定，然后调用 yield 阻塞当前协程，等待唤醒。\n处理消息 在 skynet.dispatch_message 中可以看到，处理消息是使用 pcall 调用 raw_dispatch_message 来实现的。除了处理消息外，本函数还处理了 fork 相关的调用。\nraw_dispatch_message 中，如果消息的类型为 PTYPE_RESPONSE 也就是对 call 调用的回复，则会通过消息的 session 找到之前调用 call 挂起自己的协程，然后把结果通过 resume 传给协程，使其从挂起点之后继续执行。\n如果是其它消息类型，则通过类型拿到对应的处理函数，拿到一个新的协程，通过类型指定的解包方法解包数据，然后用协程执行处理函数和消息数据。\n","date":"2022-02-14T16:10:16+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%85%AB%E4%B8%80%E5%88%87%E9%83%BD%E6%98%AF%E6%B6%88%E6%81%AF/","title":"skynet源码分析（八）一切都是消息"},{"content":"　skynet 进程中一共有五种线程，主线程、monitor 线程、timer 线程、socket 线程和 worker 线程。每种线程各司其职，本篇会讲述一下各种线程的作用。\n主线程 skynet 中的主线程只负责初始化整个进程，在前篇讲启动流程的时候，那些流程都是发生在主线程中的。它不参与业务的处理，在初始化完所有的全局变量以后，它按配置创建出了剩下的四种线程，并且等待全部线程的结束。\nmonitor 结构 在介绍其余线程之前，首先要看一个结构体 monitor，它跟后面介绍的 monitor 线程同名，但是并不是其专用的数据，而是穿插在所有的线程中使用。\n1 2 3 4 5 6 7 8 struct monitor { int count; // worker 线程的总数 struct skynet_monitor **m; // 全部的线程 monitor 指针 pthread_cond_t cond; // 给 worker 线程挂起用的全局条件 pthread_mutex_t mutex; // 有锁结构 int sleep; // 睡眠的 worker 线程数量 int quit; // 退出标记 }; monitor 结构体变量每个进程只有一个，可以看到它是一个有锁的结构，这里可以发现一个与其它地方明显的区别，锁直接使用了 pthread 库的 mutex 来做，而没有用 skynet 中更常用的 spinlock 来实现，主要是因为要配合 cond 来做线程挂起。\nmonitor 线程 作用 monitor 线程的功能比较简单，monitor 顾名思义就是监控，它监控的就是所有 worker 线程的工作状态，如果 worker 线程在处理一条消息的时候用时太久了，monitor 线程会打印出一条错误日志，告诉开发者一条从 A 服务到 B 服务的消息的处理逻辑中可能有死循环存在。\n实现 每个 worker 线程都有一个与之绑定的 skynet_monitor 的变量，它负责记录的本 worker 线程的检查状态。\n1 2 3 4 5 6 struct skynet_monitor { ATOM_INT version; // 原子 version int check_version; // 上次检查时的 version uint32_t source; // 当前处理的消息的源 handle uint32_t destination; // 当前处理的消息的目标 handle }; 每次当 worker 线程开始处理一条消息的时候，它会修改自己对应的 skynet_monitor 中变量的值。其中 version 会被一直累加，source 和 destination 设为当前处理的消息的源地址和目标地址。\n线程的主函数会进入到一个无限循环中，在循环中，每 5s 会检查一次每个 worker 线程的 skynet_monitor 中的值。如果 version 和 check_version 不相等，则把 check_version 设为 version，如果相等，则说明从上次检查到这次检查也就是 5s 之内，worker 线程都在处理同一条消息。这时候就认为这个 worker 线程可能已经陷入了死循环中。把目标服务的 endless 属性设为 true，然后输出一条错误日志警告开发者。\ntimer 线程 作用 timer 线程主要负责了两件事情，更新系统的当前时间和唤醒一个睡眠的 worker 线程。前者是为了给时间的获取接口提供时间返回值，以及执行定时任务，后者是让之前因为没有取到消息处理而睡眠的 worker 线程再次尝试去处理消息。\n自己计时的意义 计时的实现很简单，记录了一个进程的开始时间 starttime 和一个进程从开始到现在经过的时间 current 来完成当前时间的计算，每经过 2.5ms 将 current 的值更新一次。\n之所以要框架自己实现计时的原因主要有两个。首先，业务逻辑中取时间是一个经常有的操作，而 lua 标准库中的取时间的接口 os.time 的实现是用的系统调用 time 来做的，这个系统调用会使进程进入内核态，大量使用的话效率堪忧。而 skynet 每个 tick 更新时间是调用的 clock_gettime，这个接口可以说不是系统接口，或者说是因为 linux 的 vdso 机制使这个接口可以很高效的频繁调用。第二个原因是自己实现的计时不会被系统时间影响，在进程执行的过程中如果系统时间被改掉了，自己实现的计时器还是会按以前的步骤执行，这样可以避免一些问题的发生，比如定时器的触发问题。\n定时器的实现 skynet 中定时器的计时是使用时间轮算法来实现的，时间轮是一个被广泛用于实现高效率定时器的算法，linux kernel 的定时器也是使用时间轮算法实现的。\nskynet.timeout 是 skynet 提供的定时调用接口，它首先拿到一个新的 session 用于接收定时器的唤醒，然后创建一个包裹了等待执行的函数的协程，把协程跟 session 关联起来，然后用 session 和自己的 handle 创建一个 struct timer_event 加入到时间轮中。\n1 2 3 4 struct timer_event { uint32_t handle; int session; }; 等待时间轮转动到了指定的时间点以后，从里面取出这个 timer_event 结构体变量，使用其中的 session 来创建一条 PTYPE_RESPONSE 类型的消息，将消息 push 到目标 handle 的消息队列里。等 worker 线程处理这条消息时，根据 session 拿到对应的 co 执行它，一个定时器的调用就完成了。\nsocket 线程 作用 socket 线程主要做了三件事，接收并且处理 socket 命令，处理 epoll 事件，然后如果当前全部的 worker 线程都在睡眠中，则唤醒其中的一个。\n需要注意的是，本线程并不会去执行任何 epoll 事件，所有事件都是转换成一条 PTYPE_SOCKET 类型的消息，发送给与相关 socket 绑定的服务。\nsocket 线程的具体实现比较复杂，后面会专门开一篇网络专文，此处只大概讲一下作用，具体实现暂时略过。\nworker 线程 作用 worker 线程顾名思义，就是处理逻辑的主力线程了。它只做一件事情，处理消息。它会尝试去全局队列中拿到一个服务队列，然后再根据自己的负载参数，处理其中一定比例的消息。如果拿不到消息队列，会把自己投入睡眠中，等待 timer 线程或是 socket 线程唤醒自己。\n数量 worker 线程是这几类线程中唯一可以通过 config 中的配置参数修改线程数量的。一般把 worker 线程的数量设置为本机的 cpu 核心数即可。\n工作参数 每个 worker 线程有一个属于自己的参数结构体 worker_parm，用来保存一些本线程的参数。\n1 2 3 4 5 struct worker_parm { struct monitor *m; // 全局 monitor 的引用 int id; // 线程ID int weight; // 工作权重 }; 其中 weight 表示的是工作权重，目的是为了尽量让不同的 worker 线程的步骤不一样，从而减轻在全局消息队列那里的锁竞争问题。\n1 2 3 4 5 6 static int weight[] = { -1, -1, -1, -1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, }; worker 线程在拿到服务的消息队列以后，会把队列长度 n \u0026raquo;= weight 来得到本次要处理的消息数量。所以前四个线程每次只处理一条消息，后面的四个每次处理队列中的全部消息，再后面分别是每次处理总长度的 1/2，1/4，1/8 条消息，32 以后的 worker 线程的 weight 一律为 0，也就是每次处理消息队列中的全部消息。\n实现 只要还要服务没有退出，worker 线程会一直尝试去全局队列中取服务队列并且处理其中的消息，在 skynet_context_message_dispatch 中可以看到具体的实现。\n通过调用 skynet_globalmq_pop 从全局队列中取出一个服务队列，全局队列是个链表，此处加锁以后，从链表头部取出即可。\n拿到服务消息队列以后，需要从里面拿取消息，并且处理。这个过程在一个循环中，循环的次数跟本线程的工作权重 weight 有关，通过计算当前队列中待处理的消息总长度 n \u0026raquo;= weight 得到本次要处理的消息条数。\ndispatch_message 用来处理消息，通过移位拿到消息的类型和长度，累加处理消息计数，然后调用 context 的 callback 函数进行处理。\n本轮消息处理完毕以后，会尝试获取一个新的服务队列，如果能拿到，不管当前处理的队列中还有没有剩余消息，都会把当前队列 push 回全局队列中，如果拿不到，则继续处理本队列，即使它其中已经没有消息了。\n如果一开始拿到的要处理的服务队列中本来就没有消息的话，则会把其 in_global 参数设为 0，且不会将其放回全局队列中，而是会尝试从全局队列中再拿取一个队列返回，来进行下一次的处理。这个没有消息也没有在全局队列中的服务队列会一直保持这个状态，直到下一次有其它服务向其发送消息，这时候会重新把其 in_global 参数设为 MQ_IN_GLOBAL，并把其 push 回全局队列中。\n当 worker 线程已经从全局队列中取不到服务队列时，它会锁上 monitor 结构的锁，然后累加休眠的线程数 sleep，并且使用 pthread_cond_wait 将线程阻塞在 monitor.cond 上，等待 socket 线程或是 timer 线程唤醒。\n","date":"2022-02-13T17:36:02+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%83%E5%90%84%E7%A7%8D%E7%BA%BF%E7%A8%8B%E7%9A%84%E4%BD%9C%E7%94%A8/","title":"skynet源码分析（七）各种线程的作用"},{"content":"　handle 作为服务的句柄，在 skynet 的消息发送和消息处理中都起到了不可或缺的作用。handle 的管理模块主要提供了 “通过 handle 查询服务地址” 和 “通过服务名字查询 handle” 这两个功能。本篇来讨论一下 skynet 是如何对 handle 进行管理的。\n管理器结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 struct handle_name { char *name; uint32_t handle; }; struct handle_storage { struct rwlock lock; // 读写锁，因为读取的频率远远高于写入 uint32_t harbor; // 本节点的 harbor id uint32_t handle_index; // 当前的索引值，会累加 int slot_size; // slot 的长度 struct skynet_context **slot; // 全部 ctx int name_cap; // 名字列表的容量 int name_count; // 名字列表的总数 struct handle_name *name; // 保存全部的名字 }; static struct handle_storage *H = NULL; 每个 skynet 进程都有一个全局的 handle 管理器，会在进程启动时被初始化。它是个带有读写锁的结构，因为大部分的操作都是查询操作，所以读写锁保证了查询的效率。保存 handle的哈希→服务 键值对的 slot 是一个动态数组，不够了会扩容，初始长度是 4，每次扩容会在原长度上翻倍。name 用来保存全部的 handle 和名字的对应关系，是一个有序的动态数组，按 handle_name 的 name 字段的字符顺序进行排序，初始长度是 2，每次扩容在原长度上翻倍。\n通过 handle 查询服务地址 注册 handle 想要查询首先要注册数据。服务在创建的时候会调用 skynet_handle_register 为服务注册一个 handle 并且插入到 slot 中。\n1 uint32_t skynet_handle_register(struct skynet_context *ctx) 插入数据的部分值得讲一讲，slot 的 key 是通过 handle 对 slot_size 取余数的来的。handle 有一个 handle_index 做标记量，是从 1 开始的。如果不考虑服务退出的话，其实 handle 就一直累加就好了，也不需要取余数了，不够了就扩容。实际在服务退出的时候，handle 不会回收，但是 slot 中占据的位置要清掉。这就造成了 slot 的空洞问题，插入数据就变得麻烦了起来，因为要找到空洞安放数据。\n以写模式锁住读写锁 把 handle 赋值为 handle_index 的值 用 handle 对 slot_size 取余，拿到一个位置 如果这个位置是空的，则找到了合适的位置，保存在 slot 中，解开读写锁，返回 handle 即可 如果这个位置不为空，则累加 handle，再检查，一直累加整个 slot_size 的数值，此时所有 slot 的位置都被检查了一遍了 如果还没找到，那就要扩容了，把 slot 扩容到原先的两倍，然后重复遍历检查的步骤 这里有两个小限制，首先 slot_size 不能超过 16777215，还有就是当 handle 超过 16777215 以后，handle 会从 1 再次开始，理论上来说有重复的风险。\n释放 handle skynet 会在服务退出后移除掉 slot 中对应的项。在 slot 中形成空洞，这也是为什么在插入的时候要遍历整个 slot 找空位置的原因。\n查询 可以通过 handle 直接查询到服务的地址。\n1 struct skynet_context *skynet_handle_grab(uint32_t handle) 有了上面的注册以后，这个查询就很简单了，算出 handle 的哈希，直接从 slot 里面取出来服务的地址返回即可。因为不涉及到修改，所以查询只锁读锁就行了。\n通过服务名字查询 handle 注册名字 可以通过调用 skynet.name 来注册一个指定服务的名字。该接口最终会调用到 C 层的\n1 const char *skynet_handle_namehandle(uint32_t handle, const char *name) 这个函数会锁上管理器的写锁，并且把自己的名字和 handle 插入到管理器的 name 数组中去。因为 name 是一个有序的动态数组，所以在插入的时候同样需要维持顺序。实际实现是通过二分查找搜索目标的名字，如果查到了，则说明名字重复了，返回了 NULL，不会覆盖旧名字。如果没有查到，则结果位置就是合适的插入位置，然后把名字和 handle 组合一下插入到 name 的合适位置中去。插入过程有可能引起数组的扩容，每次把容量扩充到当前容量的两倍。\n注册名字是如果是本地名字则应该在前面加上一个点号，类似 \u0026ldquo;.xxx\u0026rdquo; 来区别本地名字和 harbor 的全局名字。本地名字没限制，harbor 的服务名字不能超过 16 个字符。\n查询 因为 name 是个有序的数组，所以查询这一步同样使用二分查找搜索数组即可。因为不涉及到修改，所以只锁上读锁即可。\n","date":"2022-02-12T03:10:43+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%85%ADhandle%E7%9A%84%E7%AE%A1%E7%90%86/","title":"skynet源码分析（六）handle的管理"},{"content":"　skynet 是一个 actor 模型的消息框架，每个 actor 在 skynet 中就是一个服务。服务可以说是 skynet 中最重要的概念，本篇会介绍 skynet 中的服务。\n概述 skynet 中的服务是基于上文中前文中提到的 module 在创建的，module 为服务提供了私有数据的存储位置。服务作为一个 acotr，拥有自己的消息队列，可以向别的服务发送消息，也可以接收别的服务的消息并把消息交给消息回调函数进行处理。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 struct skynet_context { void *instance; // C 模块副本的引用 struct skynet_module *mod; // C 模块的地址 void *cb_ud; // callback userdata skynet_cb cb; // 回调函数 struct message_queue *queue; // 服务消息队列 ATOM_POINTER logfile; // 文件指针是个原子指针 uint64_t cpu_cost; // in microsec // 消耗的总 cpu 时间 uint64_t cpu_start; // in microsec // 本次消息处理的起始时间点 char result[32]; // 用来保存命令的执行结果 uint32_t handle; // 本服务注册获得的对应 handle int session_id; // 消息的 session id 分配器，不断累加 ATOM_INT ref; // 引用着的数量 int message_count; // 处理过的消息总数 bool init; // 初始化完成标记 bool endless; // 死循环标志 bool profile; // 是否开启了 profile CHECKCALLING_DECL }; 虽然 skynet_context 中的数据项很多，但是其实核心的数据就是跟消息处理有关的那几个。worker 线程会不断为消息队列中每个消息调用回调函数进行处理。\n创建服务 1 struct skynet_context *skynet_context_new(const char *name, const char *param) 不管是从 C 层还是从 Lua 层创建一个新的服务，最后的创建函数都是 skynet_context_new，这个函数接受一个 module 名字和一个传给 module 的 init 函数的额外参数，返回一个 struct skynet_context 变量指针。\n服务在创建的时候有几个关键步骤：\n通过 module 名字去查找 module 的地址，然后调用 module 的 create 接口创建一个数据副本，这两个值会保存在 skynet_context 的 mod 和 instance 字段中。 注册服务的 handle，服务的 handle 是用来完成 名字 → handle 和 handle → 服务 的映射。 创建服务的消息队列。 调用 module 的 init 函数初始化之前创建好的 module 数据副本。 把服务的消息队列 push 到全局的消息队列中去。 引用计数 每个服务都有一个原子类型的引用计数变量 ref，当有地方引用一个服务时，调用 skynet_context_grab 来增加服务的引用计数，在引用结束以后需要手动调用 skynet_context_release 来减少服务的引用计数。\n1 2 void skynet_context_grab(struct skynet_context *ctx) struct skynet_context *skynet_context_release(struct skynet_context *ctx) ref 在服务创建完毕以后会的值为 1，正常的引用和解引用是不会使 ref 变为 0 的，如果需要删除一个服务，可以在服务的逻辑中调用 skynet.exit 或者是通过后台命令来减少服务的引用次数，然后等别的地方的引用全部结束了，ref 会变为 0 来触发删除服务的函数 delete_context.\n1 static void delete_context(struct skynet_context *ctx) 之所以要进入一个引用计数而不是调用一个删除函数直接进行删除，主要原因是因为要删除的目标服务可能正在被别处引用，比如它的消息队列正在被 worker 线程处理，直接删除会有问题，所以虽然维护引用计数比较麻烦，但是其实已经是一个比较巧妙的解法了。\n消息队列 1 2 3 4 5 6 7 8 9 10 11 12 13 struct message_queue { struct spinlock lock; // 自旋锁 uint32_t handle; // 消息队列的 handle int cap; // 容量，模仿 vector 的实现，不够了会自己扩容 int head; // 消息的头指针 int tail; // 消息的尾指针 int release; // 标记是否已经被释放 int in_global; // 标记是否在全局队列中 int overload; // 现在的负载 int overload_threshold; // 超载警告的阈值 struct skynet_message *queue; // 消息队列数组 struct message_queue *next; // 下一个消息队列，链表结构 }; 每个服务都有属于自己的一个 struct message_queue 结构的消息队列。消息队列是有锁结构，实现方式是一个动态环形数组，使用 head 表示目前队列中最早的一条消息的索引值，tail 表示目前队列中最晚的一条消息的索引值。在 push 消息进来的时候会检查是否还有剩余位置，如果没有了会调用 expand_queue 扩容这个数组，容量初始的时候是 64，每次扩容会把当前容量直接翻倍。\nhandle 上文提到了，handle 的作用是用来完成 名字 → handle 和 handle → 服务 的映射。首先，skynet 中同进程不同服务之间发送消息的本质就是把消息 push 到目标服务的消息队列中，这个操作中最重要的一个步骤就是拿到目标服务的消息队列的地址，又因为有了目标服务的地址就能拿到其消息队列的地址，所以这个步骤变为了拿到目标服务的地址。\nskynet 中发送消息的时候，指定目标可以通过服务的名字或是 handle，如果目标的参数是名字，需要两步转换，首先要根据服务的名字拿到服务的 handle，然后再根据 handle 拿到服务的地址。如果参数是 handle 则直接根据 handle 拿到服务的地址即可。\n这个过程能不能简化呢？在 C 语言中很难实现，如果是 C++ 的话，可以考虑强制要求服务一定要有名字，然后使用一个 unordered_map 把服务的名字和地址对应起来，发送消息的时候也只能通过名字来指定目标服务，这样可以简化掉 handle 这个转化过程，但是也加了不小的限制。\n","date":"2022-02-11T17:19:18+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%BA%94%E8%AE%A4%E8%AF%86%E6%9C%8D%E5%8A%A1/","title":"skynet源码分析（五）认识服务"},{"content":"　module 是 skynet 中一切 actor 的基石，每个 actor 的本质都是一种 module 的具象化。本篇会讲解跟 module 有关的内容。\n概述 skynet 自带了四个 module, 在 service-src 目录下面每个 service_xxx.c 的文件是一个对应的 module. 编译完成之后，module 所在的目录是 cservice, 目录下的 xxx.so 就是对应的 module 动态库。启动一个服务的操作其实就是启动一个 module 实例，比如最常见的 lua 层的服务，就全部都是 service_snlua 的实例。\n结构 module 有自己的名字，在查找的时候是通过名字来查找对应的 module 的。每个 module 会被单独编译，编译成一个动态库文件 *.so, 在需要的时候才会被加载。module 可以有四个指定函数，分别为：\nxxx_create xxx_init xxx_release xxx_signal 1 2 3 4 5 6 7 8 9 10 11 12 13 typedef void * (*skynet_dl_create)(void); typedef int (*skynet_dl_init)(void * inst, struct skynet_context *, const char * parm); typedef void (*skynet_dl_release)(void * inst); typedef void (*skynet_dl_signal)(void * inst, int signal); struct skynet_module { const char *name; // 模块的名字 void *module; // 模块的地址 skynet_dl_create create; // create 函数地址 skynet_dl_init init; // init 函数地址 skynet_dl_release release; // release 函数地址 skynet_dl_signal signal; // signal 函数地址 }; create 接口用于创建一份私有数据结构体出来，init 用来初始化这个私有数据，release 在服务退出的时候被调用，用来释放私有数据，signal 用来接收 debug 控制台的信号指令。\n管理器 每个 skynet 进程有一个全局的 module 管理器，它会在进程启动的时候被函数 skynet_module_init 给初始化。它本身是有锁的，修改的时候要加锁，不过这个到无所谓，因为很少增加模块。最多只支持 32 个 module 的注册，再多要自己改一下，不过一般不会用到那么多，因为大部分都是 lua 服务。\n1 2 3 4 5 6 7 8 9 10 11 #define MAX_MODULE_TYPE 32 struct modules { int count; // 已经加载的 module 的总数量 struct spinlock lock; // 自旋锁 const char *path; // module 的加载目录 struct skynet_module m[MAX_MODULE_TYPE]; // 全部的模块 }; // 全局对象管理全部的 C 模块 static struct modules *M = NULL; module 的加载 在创建一个新服务的时候，创建函数会调用 skynet_module_query 来查找指定名字的 module 地址, 这一步会遍历所有已经加载的 module 来查询，因为总数很少，所以遍历并没有什么问题。如果查到了，则直接返回 module 的地址给查询方。\n如果没查到，则需要走 module 的加载流程。因为 path 是支持多地址的，多地址用 \u0026ldquo;;\u0026rdquo; 分隔，所以加载的过程其实就是依次在每个地址下找对应名字的动态库，如果找到了，就使用系统调用 dlopen 加载动态库拿到加载后的地址，然后用系统调用 dlsym 来分别拿到 xxx_create/xxx_init/xxx_release/xxx_signal 的地址，最后将 api 函数的地址，module 的地址，module 的名字保存到管理器的 module 数组的最新一个数据，并且把 count 累加。到这里 module 的加载就完成了。返回 module 的地址给查询方即可。\nmodule 实例的创建 创建一个 module 的实例，本质上就是创建一个 module 的私有数据结构体，这份数据在不同的 module 中结构不一样，比如在 snlua 中，结构体就是 struct snlua, 其中包含了 lua 虚拟机等数据。同一个 module 创建出的不同实例之间共享 module 提供的接口，但是数据是互相独立的。\n创建过程中，会首先调用 module 的 create 接口创建出一份私有数据，然后再调用 init 接口初始化这份数据。在释放的时候会调用 release 接口释放这份数据。\n","date":"2022-02-10T23:36:55+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%9B%9B%E8%AE%A4%E8%AF%86module/","title":"skynet源码分析（四）认识module"},{"content":"　上文写到在 skynet 进程启动过程中，bootstrap 函数会读取传入的配置文件中 bootstrap 对应的项加载启动脚本，其中 bootstrap 一般的配置为 \u0026ldquo;snlua bootstrap\u0026rdquo;, 在执行中实际操作就是启动一个 snlua 服务，启动参数为 bootstrap.\n执行步骤 启动了 launcher 服务 读取 harbor 配置处理 harbor 模式，如果没有使用 harbor 则启动一个 cdummy 服务负责拦截对外广播的全局名字变更，如果使用了 harbor 则在主节点启动 cmaster 服务，并且不管是否主节点都启动 cslave 服务 如果是未使用 harbor 的情况下，启动 datacenterd 服务 启动 server_mgr 服务 如果配置了 enablessl 则会加载 ltls 模块 启动配置文件中 start 配置指定的入口服务 launcher 服务 虽然名字是叫 launcher, 但是负责的任务除了启动器的责任还多了一点，主要是给 debug 后台提供一些针对服务的操作接口。比如执行 GC, 查看内存，杀死某个服务之类的。\n服务支持两种类型的协议，\u0026ldquo;lua\u0026rdquo; 和 \u0026ldquo;text\u0026rdquo;, 其中 lua 协议是给 lua 层的各种接口用的，text 的唯一用途是，snlua 启动错误的时候，会给 launcher 发送一个 \u0026ldquo;ERROR\u0026rdquo; 消息来报告错误。\n绝大部分支持的命令可以参考 wiki 中 DebugConsole 的部分。其余命令中，最常用的，也是 launcher 的本职工作的是 LAUNCH 它负责调用 skynet.launch 启动一个服务，并且把服务的 handle 保存，以备别的服务查询。\ndatacenterd 服务 datacenterd 只能用来给 harbor 模式下的服务器架构提供一个数据中心的功能。提供的操作支持很有限，会启动在主节点上，并且 cluster 模式下不可用。可以参考 DataCenter 的内容。\ndatacenter.get 获取指定 key 的值 datacenter.set 设置一个键值对 datacenter.wait 等待一个值被设置，设置后会返回 service_mgr 服务 主要为两个功能提供服务，第一个是 UniqueService, 第二个是 snax. 它会把自己注册成 \u0026ldquo;.service\u0026rdquo; 的名字，可以提供全局服务的名字查询和全局服务创建的功能\n","date":"2022-02-10T23:02:50+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%89bootstrap%E6%9C%8D%E5%8A%A1%E8%AF%A6%E8%A7%A3/","title":"skynet源码分析（三）bootstrap服务详解"},{"content":"　本篇大概讲一下 skynet 进程的启动流程，完全了解启动流程基本上也就明白了 skynet 的原理了，本篇会大概提到全部的启动流程。\nmain 根据参数拿到配置文件的路径 初始化全局数据结构 skynet_node *G_NODE 初始化全局环境变量 skynet_env *E 忽略信号 SIGPIPE 创建一个虚拟机加载一段硬代码（hard code）进去并且加载配置文件 把读取到的配置文件的内容写入到环境变量中 从环境变量中拿到 skynet_config config 需要的值初始化它 把加载配置创建出来的虚拟机关闭 把 config 的地址作为参数传给 skynet_start 函数继续后面的步骤 skynet_start 处理信号 SIGHUP 处理 daemon 相关的配置，如果配置了 daemon 启动的话，调用 daemon_init 处理 daemon 相关的操作 初始化 harbor 节点数量 初始化 handle 管理器 handle_storage *H 初始化全局消息队列 global_queue *Q 初始化 C 模块管理器 modules *M 初始化全局时间管理器 timer *TI 初始化全局 socket 管理器 socket_server *SOCKET_SERVER 标记性能测试标志 创建 config 中的 logger 服务 调用 bootstrap函数，传入配置中的 bootstrap 字段内容 调用 start 函数，传入配置中的 thread 字段 bootstrap 根据配置中的 bootstrap 字段内容，启动一个服务，一般皆为 snlua bootstrap, 即为启动一个 bootstrap 服务。该服务会启动 lua 层相关的基础服务，并且最后启动配置中 start 指定的服务。 start 创建一个 pthread_t 结构的数组 pid 来保存所有线程的 pid 创建 monitor *m, 这个变量用来管理全部线程的 monitor 创建 monitor 线程 创建 timer 线程 创建 socket 线程 创建所有的 worker 线程 调用 pthread_join 等待全部线程的结束 ","date":"2022-02-10T16:24:34+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%BA%8C%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/","title":"skynet源码分析（二）启动流程"},{"content":"　要学习 skynet 源码，首先需要了解源码的文件分布，skynet 的源码排布还是很有规范的，利于了学习。本篇会以 1.4.0 版本的 skynet 源码为准，对各个文件夹其中包含的代码做一个大致的说明。\n3rd 使用的第三方库 jemalloc 内存分配默认使用了 jemalloc, 如果不使用也可以，加编译选项 -DNOUSE_JEMALLOC , 参考资料：jemalloc lpeg 一个文本模式匹配库，貌似是 lua 作者写的, 业务里没用，只有 sproto 用到了，可以不管，参考资料：LPeg lua 一直用的最新版的，已经 5.4.4 了，参考资料：lua lua-md5 为 lua 提供 md5 支持，参考资料：md5 examples 一些示例服务，刚接触一个模块的时候可以看看\nlogin 一个简单的登录示例模块 lualib skynet 自带的 lua 库\ncompat10 为了兼容 skynet 1.0 及以前的目录做的一堆兼容转换，如果要用的话，把本目录加到 lua_path 里。\nhttp 提供了对 http 的相关支持，不是特别完善，但是基本上也算是该有的都有了，做后台那些可以用。\nskynet 提供了大量的框架功能相关的 lua 库\ndatasheet 把一个复杂的有一定限制的 lua 表，转换为一块 C 内存，由多个 lua 服务共享读取。参考资料：skynet_datasheet\ndb 提供了数据库的接口\nredis 包含了两部分，同名子目录里的是 redis 集群的客户端，外面的文件是单机 redis 客户端。 mongo 提供了 mongodb 的接口。 mysql 提供了 mysql 的接口。 sharedata 旧的共享数据模块，已经被上面的 datasheet 取代了。\nsnax 一个由框架提供的简化服务的模块，其实没啥用，本来用法就不复杂，参考资料：snax\nlualib-src 是 lualib 中的 lua 功能的 c 部分的代码\nsproto 内置了一份 sproto 的代码，如果项目用 protobuf 那就不用看了，貌似几乎没人用这个东西，参考资料：sproto service 框架提供的一些内置服务\nservice-src 提供了一些 C 模块\nservice_gate C 层的一个网关模块，貌似已经用 lua 重构了，参考资料：skynet_gate_lua_version service_harbor harbor, 这个应该没啥人用吧，本意是想抹平本进程和跨进程服务之间的通信的差距，参考资料：skynet_harbor_redesign service_logger 日志模块 service_snlua 最重要的提供 lua 服务能力的模块，所有 lua 服务都是从该模块启动的。 skynet-src 框架本身的代码\natomic 封装了原子操作的宏定义，在支持 C11 的编译器中，直接用了标准库的 atomic 库，参考资料：原子操作库、skynet_stdatomic malloc_hook 内存相关的操作在这里，包括了服务的内存统计，内存泄露检查等 rwlock 提供了一份读写锁的实现，使用了原子操作来实现 skynet_daemon 封装了 daemon 相关的操作，提供了把指定 pidfile 文件相关的进程转为 daemon 或者取消 daemon 的接口 skynet_env 提供了全局 env 变量的操作，使用了一个 lua 虚拟机进行管理 skynet_error 处理服务的错误信息，把错误信息发送给之前设置过的 logger 服务 skynet_handle 提供了服务名字到 handle 和从 handle 到服务结构的两个映射关系，要通过名字查服务的话都要过这里 skynet_harbor harbor 功能的实现 skynet_imp 一些乱七八糟的东西扔在这里，估计是没地方放的东西 skynet_log 日志功能的实现 skynet_main 进程入口文件，定义了 main 函数，处理了配置 skynet_malloc 声明了内存操作的接口，实际调用了 malloc_hook 中的定义 skynet_module C 模块的管理部分，提供了 C 模块的全局注册，查询等功能 skynet_monitor 每个 work 线程有一个 skynet_monitor 结构，用来给 monitor 线程检查用的 skynet_mq 消息队列，包括了全局队列和服务队列的实现都在这里 skynet_server 服务的管理模块，包含了全局管理结构和各种服务相关的操作接口 skynet_socket socket 相关的功能实现，包括了全局 socket 管理器和 socket 的操作接口实现 skynet_start 框架启动函数，包括了各种线程的启动，各类全局管理模块的初始化 skynet_timer 时间相关功能的实现，除了全局时间管理模块，还有定时器的实现，时间轮 socket_buffer 给 socket 定义了一个发送缓冲结构 socket_epoll IO 复用的 epoll 实现 socket_info 定义了 socket 的连接信息结构 socket_server socket 线程的核心操作，定义了 socket 相关的几乎全部操作 spinlock 自旋锁，也是用原子操作来实现的 ","date":"2022-02-10T15:33:55+08:00","permalink":"https://wmf.im/p/skynet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%80%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/","title":"skynet源码分析（一）目录结构"},{"content":"　是的，在经历了这些年多次的开坑和弃坑之后，终于又又又一次决定要写博客了。作为再开坑的第一作，首先来写一写这些年写博客折腾的历史以及为什么又要写博客。\n折腾的历史 WordPress 时代 学生时代的时候，查资料看到别人的博客，觉得挺好的，想给自己也整一个。那时候感觉有个自己的网站挺牛逼的，大概算是个可以吹的点了，就兴冲冲的开始查资料怎么建站。从一开始完全不懂，开始看视频学，买了一个喜欢的域名，又买了一个云服务器。事后证明那视频巨坑，就是商家搞出来忽悠人的，域名托管在了一个不知名的服务商那里，不能转出，操作界面突出一个烂，还经常上不去。这还不是最惨的，最惨的是云服务器也是他家的，一百来块钱一年，配置忘了，反正就是各种卡，各种慢。不过折腾了半天，终于是搞定了，在一个烂服务器上搭了一套 LAMP，可以想象有多卡了，再加上服务器的网络也不行，导致访问完全随缘，写东西更是痛苦。在这种情况下，断断续续写了一点东西，后来就完全懒得上了。写的内容都完全忘记了，只记得当时用的那个小猫主题挺好的。\nCSDN 时代 毕业以后，在我还在家里摸鱼没找工作，还在准备做一个 Cocos2d-x 前端打工仔的时候，看了不少大佬的博客，学到了挺多有用的知识。那一年的 CSDN 还不是现在的 CSDN，所以就有样学样在 CSDN 开了个坑记一些学习中碰到的问题。写了应该有几十篇，以 Cocos2d-x 有关的内容为主，可能也有几篇 C++ 的。后来工作了，各种阴差阳错，转来做后端了，再加上 Cocos 式微，Cocos 基本再也没碰过了。那时候工作轻松，下了班天天忙着打游戏，博客就弃坑了。\nHEXO 时代 工作了一段时候以后，有次上网发现了 hexo，感觉挺好的，直接把 Markdown 编译成静态页面访问，做博客完全够用了。那时候正好因为要科学上网，手里有一个搬瓦工的服务器，只用来跑 SS Server 也浪费了点，就开始折腾 hexo 了。hexo 确实比之前那套 LAMP 的方便多了，不需要数据库了，搞一个 Nginx 就能跑。挑了半天还是选了那个用的人最多的 next 主题。这次学聪明了，从一个大服务商那里买了个域名。凭心而论 hexo 还是不错的，部署方便，主题多，使用也简单，除了文章多了以后生成一次很慢以外别的没什么问题。这次开坑只写了几篇，然后再次沉迷打游戏，慢慢就弃坑了。\nHUGO 旧时代 又有一次上网的时候，看到有人在讨论 hugo，go 语言写的，速度比 hexo 不知快到哪里去了，而且从 hexo 迁移过去很方便。看到这个的时候，我又想起了自己弃坑好久的博客，想重新捡起来，于是又投入到了折腾 hugo 中。hugo 确实快，不过那时候主题还很少，挑来挑去才找到一个没什么 bug 的主题。但是这时候我科学上网已经不自己部署了，改用机场了，所以就去学了学，把博客部署在了 Github Pages 上。这次又写了几篇，然后再次沉迷游戏弃坑了博客。\n笔记时代 有次跟一个朋友讨论问题，他说这个东西他记过笔记，然后给我截了个图。我感觉挺方便的，完全没有了部署的问题，随想随写，而且反正是完全给自己写的，写起来很随意，就感觉挺喜欢的。而且笔记软件支持的功能比 Markdown 要丰富的多，链接，块编辑等等。我就花了一些时间，把市面上常见的笔记软件都试了试，估计试了十来款，最后挑了一个最喜欢的，把之前博客的 Markdown 文件都导入了进去。这次写了很久，一直写到了前几天。结果碰到了一个软件 bug，把我辛辛苦苦写的几百字给搞没了。当时就有一种游戏没存档结果闪退了的感觉。想来想去，还是继续把博客搞起来吧，这样跟别人讨论问题的时候，可以直接发链接了，而且之前买了一个自己名字的域名，正好也可以用起来。就又把文章都导出成了 Markdown，不过那些时间、标签之类的标记都丢失了，有一些文档的格式也出了点问题。\nHUGO 新时代 因为之前就喜欢 hugo，所以这次还是选的 hugo 来搞。轻车熟路部署好，剩下的就是整理笔记了。笔记写的很随意，而且导出的时候有一些格式出了问题，所以就不准备原样导入成博文了。准备花点时间把笔记整理一下，感觉已经没用或者写的不好的就不再贴上来了，只把感觉有价值的文章整理成博文发出来。\n为什么要写博客 回到标题，之所以写博客，对我来说主要是出于记录这个需求，其次才是分享，再次是为了用上一直闲置的域名。在学习过程中总结还是很重要的，不然看了的东西总是看两眼就以为自己会了，但是其实不会。总结的过程会让自己变成一个讲述人，一个东西能有条理的讲清楚的时候，大概就可以算得上是真正掌握了吧。\n总之，希望这次开坑可以一直写下去。\n","date":"2022-02-10T03:19:19+08:00","permalink":"https://wmf.im/p/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%88%E8%A6%81%E5%86%99%E5%8D%9A%E5%AE%A2/","title":"为什么又要写博客"}]